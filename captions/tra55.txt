value vector is always s times K siteswhere K is the horizon this much Horacenow my question is you figure out how tocompute the value of one policy I cameinto this war trying to compute optimalpulse so I jump I pick up again I do thesame idea which is somehow this policywas given by somebody I kept twotechniques value I can imagine I justokay to go I'm use a one so I justdecided some actions for each s and Kcombination tacticals now is thisoptimal our map the way you could figurethis out is dream of another policythankfully there are no types ofpolicies because there is s power Ktimes a number of policies right so youcan dream up under cows come home andfor each policy you do this computationand compare the two vectors you see whatI'm saying and you will say one vectoris better than the other vector isacross the boardit's either greater value or equal valuelambda right Steven is a good way todefine is what I'm saying now I'm givingyou a second-order route force approachfor computing the value computing theoptimal policy essentially so phosphitegave you a brute-force approach forcomputing the value of a policy and Ithink this structure now I am saying Idon't want to go from here to theoptimal policy the brute-force approachwould be if every policythe capybara policies check which meansthen pick up next one think up next onewhoever is left in there and that's ourpolicy by definition so it would be acomplexity in fact over here looks easyright this is the optimal value of thisnice connection in fact whatever we didearlier about these 0s v1 minus xn / VKs we can start doing it in terms of thisstar which is the optimal value and youcan directly compute optimal valuefunction if we little harder thandiscovering in the value of the policyas you saw it will model maxing overmultiple actions that you can takeownership but if you do the taxi thenyou get the value optimal value of otherstates once you get back to molarity ofother states you know the policy okayand to do that essentially now firstlet's actually see the following thingssuppose I am at s1 and I have computedoptimal VK minus 1 star DK minus 1 starokay it's not quite in the starshipthen if X 1/10 buoy you know multipleactions XA 1 2 3 then I will pick theaction a I that gives the best expectedbackupso that's how you can work optimal valueto the action so you know that now thequestion then is how did I get back fromthem and basically USADA I'm just goingto walk you through this again which isthis is the set of equations and theseare kind of element equations it'scalled a dynamic programming and whatthey basically sitting is and we know ifyou go to the PI then we need optimal soI can also put this stuff you knowdifferent rotations for optimal ok so inthis part is optimal value with zerosteps to go if I stay still it's the onethat's a good thing revenue powering apolicy I'm not it's your time right nowthen what you can't in this state is theRamsey of this table okay right sothat's V 0 s is RS and then DPS is outof S Plus remember previously for anotification what you did was you needsomething like this but the action thatyou are supposed to do in that state nowyou do this calculation for each of theactions you can do take the max over ityou see the difference so in fact if youlook back at the value cavitationraishin it looks almost the same exceptthere is no max over a so it's just abunch of linear equations in fact it'sthe linear equations of the form that weall lovewhen linear equations is like after 5 Xplus y is this match and X minus y isMarch 1 is x and y the linear equationssimultaneously nearly fits on the formthat we really love the time Y is equalto this match and X plus y is equal tothis much what is Xyou already know given Y in this matchthen we put that value into X plus y andthen you know that span and that's whatis happening in computing the value ofthis particular possible right okay soand then this here except now what we'redoing is that linear computation is nolonger available you have to be maximumor always maximum other is so if you maxover is this is the advantage of sittingin the front and then you want to goso it's hard to come out like that soclearly the reward is higher for yourwebsite so you should do that anyway maxand once you get this we get the optimalvalue for the entire space and then youcan put into the policy as I said whichis the policy for s with K stages to gois the action in it that gives the bestback of them in one step you can justone step okay okay this is basically theformal definition of finite horizonvalue computation but you understandthis you can see this and then you everto think this then you're in good shapealready you don't seewe'll just see that this is how it lookslike essentially so this is not abellman back well the way we compute perpacket this particular case is thatagain you have s1 to s4 and I have twoactions that one under yellow okay so Ican see what the red one will get me butit will get me past 8s that I could willget me 0.7 times VT s 1 plus 4 and 3times 3 plus 4 I am computing the VGplus 1 horizon T plus 1 in terms of allrising the red moon will get with thisthe yellow one will get minutes they'retwo different actions which one will Ipick I take the one is maximum right soI take the maximum that and then I throwin my own immediate reward for being inthis state and that would be Nene plus1s that's it now if this thing lookskind of confusinglook at thatit's just expected max I just made hugeamount of additional terminology andthrough will bellman also into theequation too late I'm different you seewhat I'm saying it's the same thing yourgift is an expected max okay and sobasically what we're saying is thelowest Norfolk tested about values andthen you are doing expectationmaximization expectation maximizationexpectation maximization and if you findthe value of a stream and you do it forevery state since checks not surprisingit's the same thing is that one is nextto right one is you know resonancessometimes perspectives helps oh okayokay so then how does the valuation lookyou start with V 0 I have 0 values usingthat I get that V 1 and then laughterwith you over that meeting except I justdrone on and on in that YouTube videofor like 15 minutes because if I don'tdo this they say you minutes weren'tthat okay you sound less important ifyou understand how the problem issupposed to be done you should be ableto reuse right any questions about thisstuffit's just simple right just you doingmax it in the middle the only differenceis that if it is values computation fora given policy it would have been salaryoffice simultaneous equations of acertain format what account the diagonalformat where you will basically you have70 degrees of type X equal to this muchY for this one is equal to this much andyou know x1 plus X equal to this much x2plus y equal to this much and so onand since the first X values are knownthe second X values can just be computedby substitution you do have a new matrixin worship but this particular valueaccommodation policy that but to computethe optimal policy and to compute a fewmore value you will have to remove themaxi[Music]okay so of course this is give you thatI use every way and then you have acamera into the pharmacy and as I saidif you are in this particular place andif you're in this particular state thisactual of this better little actionyou pick the action that is bringingback the best expected value so in factyou will didn't have to do it as aseparate level as you are reading onepass through this you can remember thepolicy right because because whathappened was you took these max let meput this max to do each one gave the maxif you just remember that as the actionfor that state that's some pure sciencethat's just data structures we won't sayit normally you know if you do this onething and do it all over againokay so here is a bunch of facts aboutvalidation what we mean by the way thisidea is called the dynamic programminghow many of y'all taken other thansources I'm programming in fact thehundred programming essentially says theintermediate results because you don'thave to recompute them again and itcould be useful when the optimalsolution has what's called optimalsubstructureif the optimal solution of the optimalsubstructure then you can use any ofthem it is the idea so suppose you justwent to the doctor after doctor said Ijust did the calculations and you haveseven years to that's it okay so youknow finite horizon MVP so he deeplyMeister come up your life and figuredout the optimal policy for the rest ofthe same is what to do in each state inthe first year the second year the thirdyear before their fifth year at six pmseven then you went for a check-uptraffic to the doctor you only do forsix years it's fun I know what to do itmight be for seven years I take the partof the policy for six years that wouldbe the optimal once everything is comingup millhouse those are few rollerokay then I'm going to do a gamesomebody help your friend ghost was inthe same situation goes to the doctor hesays oh I've been a mistake but it'sgood for youin fact we do this for eight years ifyou have to recompute anti policy youknow the optimal value after seventyyears you compute the optimal value forthe eighth year we compute what to do inhere if you can do this then youroptimal solution has worse than theoptimal substructure policy optimalsubstructure property in fact everythingwe have seen till now have optimalsubstructure power property in StarSearch have optimal substructureproperty in fact a star search doesvacuuming program okay I'm this guybalamani basically came up with a namedynamic programming and it really justlike uniform passage which has noconnection to reality in terms of theword and the meaning not only programagnosticesata programming here it's like why ami Sabra why is this Saldana program isits optimal substructure so in March ofthis stop actually this all of this isyou are including the policy up frontand you just use the policy to deliverthe version of real facts online is thatexpected max were shown that I showedyou at the end game please not inessence you see that one principal isabsent it just starts from wherever youare and goes a few stages known as againis actually taking it from exactlywhatever is it okay so yes actionsupposed to do in state changes based onhow many steps you have left okay andthe operational complexity is the waterspeeds of course the bursting idea thatwe had the Oh s power k-8right number of states power K timesnumber of actions okay instead it'sactually only Ohthe horizon k2 times a times a squarebut it so these two guys do stop beingin the super spirited are just friendsnext desk we're huge improvement incomplexity huge improve[Music]the structure looks exactly like thisthe difference is in the finite reversalthey still mean it preserves ourphysical significance they are thevalues for K skinniest rapport in theinfinite horizon is there just like ahydration valueswhy because two issues first of all youknow that in the case of infantilism asI already discussed last class policyhas to be stationary it has to bebecause even if you live for you know 25years you still have infinite amount oftime to go you're out having that amountof that you memorize which means thepolicy has to be stationary if notpossibly differentiate between you knowV 1 V 2 V 3 etcetera right so then whatthat means is these substitutions of theintermediate values they are justprograms if not no approximation of whatthe real value is there's only V s 1 s 2s 3 etc escape and this is your firstapproximation where are your Pistons thefast across machineswhat was that about what mission is thisthe third approximation is mr. fortapproximation is the fifth atransmission it is where do I stop notonly if this is actually true that theseare approximations of the same thing andas I keep going the difference betweenthese two vectors will become zerobecause after some number of iterationsthis computation has to reach a fixedpointwhat's our fixed one that means VJ + 1 +VK a would be extremely close to eachother it should be zero but there is nosuch thing as zero if they are Marquezyou never check for zero instead you sayVK plus one minus VK that will give youa vector of residuals take the max ofthat not value of that and say thatshould be less than and that should beclose to be less than equal to epsilonif you like I said you take two vectorswe can be a plus one and we get take the- you know how to go back to subtractionyou get another vector possibly havesmall negatives and positive numbers youtake the max of those okay basicallyactual you have to take the magnitudemax side doesn't matter and thatmagnitude should be less than epsilonwhere epsilon is the threshold you setas a placeholder for serial numbers isabout to come zero did I see this so ifyou have a finite authorized that valuefunction that you already written youcan just say it is also the same thingthat works for infinite horizonwhoever is askingit will be useful for tourism just throwin an epsilon and stop terminate when wecan plus 1 minus eta is less than itlooks goodeverything seems all done once again wehave seemed to be that halfway house howmany of you know the difference okay sofor the incredible repeat the good newsis that the policy must be stationaryit cannot be non stationary it justcannot be a statement it has to bestationary which means they're only apower s policies and a power K times Fof T times responses by the way just soyou won't get confused you know in howthe light is used to say and also Tia sa s - the transition walking so that'swhy when I'm seeing it I'm going tocount this game anyway so that's just aminor thing so the idea of a little teston the final turns out that they're juston it except now ejaculation is just anitration no connection to Eliza ifthere were to be unique optimal valuefunction if their warranty is requiresan actual theorem then you get to thatnext class there's a guy called overprove that in fact that will be expungedand in fact what is happening in each ofthese pages is with a third value vectoryou called this operation at Telmanoperation and you get a new value sobasically this is the name for thisentire operation that's calldeterministic and the beauty of thebagman update is that it has thisproperty called the contraction propertyand you can show that if you have twovectors v1 and v2 you apply bellmenupdate to the reference I'm sorry youhave two vectors U and V duthen even minus v2 and you compare thatto V V 1 minus V V - okay this would besmaller that means the vectors afterBelmonte come closerthat's the contraction property now ifone of these values is V star which isthe optimal value function then vr vstar should be Vista because it needs tobe a fixed point then you apply updateat the fixed point then you should get afix point back so if you do the samethingto be a star then you see thatessentially water that when objects aredoing is each of these additionalvectors are coming closer and closer andcloser to be stuff which is what we hopewill happen I will go through this againbut that's the general idea what happenshas to by bellman update actuallyreaches a fixed point you need this kindof a proof or finite horizonbecause every finite number of stages inthe Infinity is and your destiny whywould you believe that any of this pathif you have infinite horizon you haveinfinite sequence of states that's whichthe community reward on an infinitesequence of states is the foundation ofour country at each of those states butsalvation at each of the state's whyshould it infinite summation beingfinite most likely be infinite if it'sinfinite and everything is infiniteeverything is infinite there is nobetter versus one of us so the problembecomes degenerate you see this ok thisis a problem why should value iterationcan watch at all some of the words as aninfinitely long sequence is likely to bedifferent you need to stop it from beingis remember the last time you thought ofa sum of infinite series of numbers andtry to sum them up and found that afinite anybody remember geometric seriesthe calculus right if you have ageometric series with a common ratio Rstrictly less than one then you areguaranteed that a definite series withsome to a finite number it's aconvergent series okay so I have thefollowing situationah s 0 plus RS 1 these are real numbersregister your number I want this somesomehow to become violent what can I doI said it be done less than one meterand then just throw this year with a 4-0this with a 4-1 this DataPower to thisis what I'm saying if you do this amongall these are values let's say our maxis the maximum reward that you can everget in any other states in the world sothat means you don't know all thesenumbers but all of them have to be lessthan or equal to our max then what I cando is I can get our max out and subtracteverything back we divide everything byour max at this point you should be ableto see this series is actually less thanjust the series beta plus beta squareplus beta cos beta beta power 0 which is1 plus beta plus beta squared plus beta2 etcetera beta power in which how manypeople remember what is the sum of thatwhat's the sum of the infinite series 1plus beta plus beta square meter cubeper cetera by 1 minus beta never thoughtmachines againyour life what's become of this okay sothen you know that it's whatever thissum is it's less than if some of this isleft and our max by one - magic I dowatch it such that that has to add up toa finite number at this point I'm donebecause I already have a working board Ijust have to make it instead of insteadof just basically taking this analogy tors4 I just put in beta time so I justneed to take this and say rs4 plus betatimes do this you guaranteed that stuffwill work stuck with work and my bosswill come out that's all anybody everwanted question is why is thisreasonable