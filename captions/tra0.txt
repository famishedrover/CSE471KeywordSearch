okay so last part my boss and I thinkthe things there bunch of things thatare you in particular and after day youwill have to have your cross reflectionsso that we can dance like them I try toon the Q beat how probably one of thenicest deepest mind-bending ideas okayso before I go forward are there anyquestions okay so we are going tobasically start learning again exceptthis time from a Bayesian statisticallearning point of view last time aroundwe looked at learning as a loss functionminimization optimization etc and one ofthe crazy things we try to explain toyou is for appreciate perspective whichis sort of considered the costprinciples idea that's the mind-blowingthing except you turns out remember whatI said about Bayesian when the visionscome here okay the fork is the set ofhypotheses that in explain the data mamalearners will try to see which of thesehypothesis best explains the data indifferent ways for example they mightlike different kinds of loss functionsare try to optimize that matter which isthe reason why you put into thinklearning has optimization but actuallyhostile force perspective learning isjust usually you are reasoning over allthe hypothesesand in fact one of the facilities to dois start thinking a hypothesis itself asa random video and it can take manydifferent values of the same 15 valuesvideo and you have a hypothesis andwhenever you see a data point and it'slabeled let's say you get to update thehigher the posterior you see basementokay and so as the examples pass throughyou get a nice posterior distributionover hypothesis it is not posterior withjustifies there's nobodyokay and then how do patients use thehypothesis they will essentially sit inthat all these hypotheses are possiblewith different likelihoods and then eachof them gets to have a vote on the testpoint for a test guy that's it you willsay I put this one says this guy withmaster class but it has only probabilitypoint one hypothesis to series in faderclass whatever point probably partlythat's it so you take the weighted sumvalue of the prediction actually whatyou do is you compute the distributionafter label itself given thedistribution of hypotheses completely ofa and heard everything I said youbasically got first one-third of more ofthe concept I just try to walk youthrough them with examples but the mostimportant to understand nearly is thisis a beautiful would you be fighting toask until you kept thinking learning thedifferent is being a differentokay the reason you decide to move isbecause you don't talk pleasewhich also tells you that all the greatideas in machine learning such as popoff such as random forest somebody saidcan you please talk about an apologythey are all essentially going from themaximum likelihood approximation basedlearning to words based on I mentionedthis at the time of drop out so there isin learning is nobody actually class itbecause it's too hard correctly but itgives you the basis for understanding itgives you the semantics forunderstanding it explains to you allthis nonsense about what is overfeedingwhat is regularizationyou can say I shall compute I shallbasically find the minimum of some lastfunction etcetera but this knowledgeonce you get it once you let it sink inallows you to understand where the ideascome from this is our small we doingthis and but for a computer small brainsthey would also be innovating in factwhen you talk about whether or not aparticular learning problem is tractablethey talk about base rate that is howwell would a policeman classify a newone which you can actually compute whichyou can actually easily enough personalpreference but what is what will it beand everything else will be a fair shareis that[Music]the department at least in the old daysthey used to be religious wars betweenthe easiest buses we paid this how manyof you heard riri-san versus big babieswhat is the difference this is thedifferenceyes the hypothesis is a random dailymove just like every other variable okayso very CMS are like the engine theywill put a priorityreallyokay so various unions basically willput a primer on behalf of this is trueand then they can compute the posteriorarm hypothesis but that basically meansthey are considering hypothesis to beunavailable frequentist statisticsdevelop in a different way the assumedhypotheses are not of the same world asthe other vendors so in particular thelesions are happy to write this ethgiven d RP d given h HT will behypothesis given the data distributionof hypothesis given the data ourdistribution of data given bibles theyare happy to writers frequentist willhave connections it's because you cannotput a niche on either side of the givenso instead what you say is they write itthey can go to sleep happy thinking theydid do some sort of miscegenation thelongest find treatment is basicallyruled the waves in statisticsdepartments and the basis with me is asmall car now in that same karma youwill find there are frequencies patientson a slightly bigger problem and thenthere is a deep learning CarlaDepartment of Allah okay but that's thepoint okayI'll try to understand these jokes youknow if after the classic please if youdon't understand them you didn'tunderstand what you're trying to say andthe most of these great jokes aboutracism you know it's a guess as wellthis how many of your world and how muchof this I mean of your home the reduceShakespeare Company it's a great thingwhich basically makes parody of all theShakespearean links and it stays thebetter you know the original thefamiliar diskettes that's the same thingokay the better you know what islearning is all of this makes sensethey're from our perspective essentiallyneural networks do maximum likelihoodwhich is just a second levelapproximation of okay Silicon Valleysee you can tell your mom when you'rewatching Silicon Valley Mountain big I'mpreparing for my exam so radium decisionI don't need to use guilfoyle CI inorder to improve in love I need to usemiddle appropriated you have asymbolically I that it's not teachingitself so tell me when you get to havethis kind of a crossover between yourdatabase class or your softwareengineering class about how the heck ofa class you're taking and then you knowhow crazy high right now this okayanywhere so how do we learn BritishNelson is basically where we are gettingstarted from that's like a simple ideaso basically we have base next that weare talking about giving manuallyspecify business okay and then whenyou're specifying base next or thespecified topology your displaced byconditional probability tables okay oneidea is either you know you could eitherstart given topology can either theconditional probability given matican I learn both Recology unconsciousboth of us are essentially learningthese labels the second one learningwork topology and CDG's is obviously themore industries and in fact then you getto I don't know how much time we'll havewhen we get to actually learning thequality of various networks it becomespretty clear that maximum likelihoodlearning is for the birdsit doesn't work okay you really have toconsider base learning ideas back inotherwise you back okay just keep thatin the back of your mind um so you'regoing to learn Bayes net so basicallyyou want to learn from the data and youwant to use to predict the other data soyou can predict other data in betweenyou have hypothesis by amethyst ithappens to be the percentage in our casehas business which is okay so the basicidea that you are probably if you areworried about this about those of youheard about this you would sort of dothis in some sense of loss optimizationinstead of loss minimization you dolikely would maximization but those ofyou who know something about if youdon't that's what I try to doessentially is this really is a reallypoor man's version a poor brains versionof Bayesian I if you have a big brainyou do basically then you won't have anyof if you need a chapter evolving youdon't need a chapter on regularizationall of that stuff doesn't have this whatI'm saying it happens because youapproximately placement okay so imaginehow much I'm building a basement and ithas only down to go now because at thispoint of time you think of this as likethe greatest ideacouple of small things before we goforward which is first of all the orderremember in fact we are trying to learnin from a statistical learningperspective mineral distributions okayso imagine you have you know exerciseand my eyes and then you basically areregularly the Joint Distribution ofexcise and buyers in a classificationsense you typically want to predict Ygiven X okay 35 minutes but some typesof the basically sometimes a motioninterested in just this one anything Ygiven X but he can be done if you havethis if you have the joint you can'tpredict but if you just happens youdon't have the joint this is what I'msayingso there are two things you can do inlearning one is you can directly downthe toilet or you can lower theconditional okay and you can actuallysee that these are connected this isnothing but y 1 etc by n given X 1 X 2 Xn times right so the things that thisguy has that this guy doesn't have isthis so what it's not actually is therelation between the input variables ifyou learn and be sure distributions ifall you have to do is solve the problemof given X that we want this is the Chiparticular and here you may be able todo classification without being able togenerating the data so for example manyof you can hear a song that's a good mystomach sorry this is a bad song butmost of us are extremely bad atgenerating songs especially except inbathrooms that somehow all of us areactuallyokay so the point basically is you maybe able to discriminate between farmswithout having the ability to generatesongs Taylor Swift's I will assume thatKeys discriminate between pop songsbecause she actually writes pop songs sogeneration will give you discriminationdiscrimination doesn't necessarily giveyou generation so there are like twotypes of learning that people wind upfocusing on be the definitive learningare there regular and those areessentially joint distribution versusconditional distributionwhat will you run across thoseconditional distribution of Y given Xwhich is why if you clamp something at 1at the output input doesn't nicelyarranged itself like for example if youknow I have generated to predict yourgrade for a student then I said I wantedit can you make me a student today itdoesn't know do you see what I'm sayingand so that's the generate you want tobe able to actually even might get the Xgiven X get the Y given some of the Xsome otherwise get the rest of the XSotherwise all of those can't we have tolearn the Joint Distribution the pointof course is by learning this youactually wind up running less number ofparameters less amount of informationand so exactly most people will go withdiscriminative learning they reallyshould be doing a variety of energy andwill complain saying sorry these areexactly so there are two different wayspeople keep on complainingwhat is this should be doing basicallythey don't know how to do it so they canplay with maximum likelihood doesn'twork the other way they should be doinggenerative part of it instead they dodiscriminative model and it won't workand they complain saying in learning andthe reason you don't do something isbecause those are hardbut it's important to know that thatselect okay okay so that's about that'sabout this discriminative alsogenerative models and this againpresented here in this slide by therecites another lying I have to talkokay this makes this point that justbecause you're excuse me who damn placeunless you want to have a smallerseparate actually so basically if youare able to generate classifying thatdoesn't mean that you can generategenerative models can do both okay andso all you need for generationally isgiant model that's okay I noticed thatnetworks can represent either jointdistributions are conditionaldistributions they can determineanything you want okay okay this Iactually like that any of you have seenit there is sort of a No Child LeftBehind policy so everybody must you knowfast exam as in the examples the Armeniais this show up every class the easy wayis the exam is testing something justlearn how to pass the exam so this is abeautiful thingshe says the test sense that makes youlearn these people when they're fineyour platesyou know which is my other point Ibasically become this fine you need totell me which one did I think that'sright so you need to tell me which findelectrical burns possible and inparticular you're doing this really isif you're just interested in the burningfire given s basically never tell mewhich one I picked up but the realquestion is I want you to be able topredict next point outokay and so if you have learned time forthe since you can predict out how we goabout doing thisokay so here are my that's what happenednow tell me which is most likelyproduced but basically you want to you ageneral idea would be to think in termsof data likelihood think of thehypothesis that would have produced thisdata with higher probability and saythat cipher this actually was right thisis the maximum active life right soessentially what you are basically goingto do is you compute this data how muchis that X is point two five times fiveis 0.75 times 0.75 that quantificationof number would be a really smallfloating point number which by the wayis another reason people take lots ofprobabilities because after you mustapply a huge number of less than onenumbers you will start getting underflowerrors and so you take log ofprobabilities and since basically ingeneral if you try to optimize something[Music][Music][Music]yes so this is what do you think greenbecause tinea possibilities of boththese wines and give it the posteriorpound is that's how you will say I stillfeel I still won't jump to theconclusion that you see that wiener[Music][Music][Music]don't jump to conclusions you can't bewrong that's the notion of Bayesianlearning which also means that Bayesianlearning is the conclusion so the ideais happening most of you don't believethat points is your hand okay pick acoin it would be so bad that it willcreate a point two five X so if I didn'ttell you that if I have a point in myyou will assume the prior probabilitythat the coin is paid is extremely highthe probability that the coin is notfail is low that means the probabilitysolution Peaksthis means you have you have additionalknowledge about how the world workswhich is fine you are uncomfortable andsay in this case if you do this candyband and you're making the bagsessentially so you think of that in thisparticular case you could be seeing awhole bunch of these kinds of Acts andyou're as if one and then you have to beable to predict what was the percentagewhat was the version of percentage ofline works for us jelly candies businessdo you see what I'm saying now if I didtell you anything you can just take thebag and keepthe candies and then eventually camlhonestly now like people have like thequestion is how quickly can you get tothat so I'll tell you just like in thecase of