okay our horizons so we will do mostlyreflects we want to mostly do mark ofopposition processes today so before weget there actually so that thing that Ihave to keep in mind is it's such a deeptopic that is extremely easy to go itwill rabbit holes almost at every bulletand every slide okay because of weekendaren't things connected to modulationprocesses and so this part of trying toget you to put enough idea but probablynot at that depth that you would need tounderstand pretty much everything abouteverything is but they have such ageneral idea that in fact one of theideas that grants and disciplinaryboundaries we will see this in factpeople in control theory do mathvalidation process they speak researchremark rotation processes people inoperations research through operationalprocesses and in fact the solutionsalgorithms finally is came from alldifferent directions some of the thegreatest idea actually came from peoplewho condemn the operations researchforce our control theory force but nowthat's all done because Tommy I saw itbut it's actually it's kind of a deepertopic and we will spend a little moretime than just today because then wetalked more process on this but it's atricky thing not to get into rabbitholes we try okay so the other thing ofcourse is I getthrough one more of these things and Imight as well have this chopping this upthere because this is the previous onethis is the current one it seems likeit's been approximately 75% of thepeople are engaged much more interestingways about one third of that peopleactually would so I assume that the restof the two thirds are definitely engagedit just saw engage that they don't evenhave time to actually take I'm certainlynot going to assume that they've lostbecause that makes me unhappyso I guess this is what you give a polland you don't take part in the polldon't complain right that's a scene itis so I am assuming that 75% of theentire class is understanding in factmaybe more than ever for some because asI said the two thirds of the class whichdid answer the poll completelyunderstand everything okay so I'mbasically going to the discussion on letme finish before I continueare there any questions okay so it'ssort of the tip that you want to keep inmind in most of the discussion on anypieces how it is connected to a starsearch which I've done already and howit is connected to game research whichhave done already I already gave yousome of a an idea because I went from astar search to get release which thenext one agent and then I said justassume that the other agent Maggie'snature when it became a TPS and wesolved it using something very similarto give me ideas which is this onlinesearchidea except we will mostly right nowwhen we talk about MVPs but onlinesearch algorithms are still as importantin fact most MVP solvers use onlinesearch in real-world problems but you dowant to understand how to compute thefull policy okay and what does it needto compute the optimal policy and youknow that when you do onlinenot guarantee any optimality but as Ikeep mentioning and if I haven't alreadymentioned here this will be the firsttime you've market to the deaf time thedifference between computer scientistsand hackers is very simple computerscientists understand what does it taketo get optimal solution I'm going to sayto heck with it and satisfy Sigmahackers don't know don't care okaythat's the big difference after eighttimes keeping optimal solutions might betoo costly that you may decide to go forthings like online computation which nolonger give you any guarantees but thereis a difference between knowing what youcould have achieved and decided not togo there versus being ignorant and proudof it okay and so that's the cleardifference in computer science versushacking okay so we will in the couple ofclasses of MVPs right now we'll focus onoptimal how compute optimal policyknowing fully well that reallylarge-scale machination processes whenis a large a large number of sphereslarge number of actions and everythingthe only thing that you can do in theapproximations and there are two Z'sMVPs are approximated one we alreadydescribed which is online computationthe other is this upper password all ofyou probably have order reinforcementlearningokay so MVP sits smack dab in the middleand you understand how to do itoptimally and then you think of offlineapproximation winds up beingreinforcement learning onlineapproximation winds up being what wediscussed last class which is thefavorite time dynamic programming and soon but let's keep that in mind okay sowe already looked at this picture lasttime as an example for MVP I'm justgoing to go 8-8 and notice that there isa transition function which tells you ifyou do an action a in stay I what is theprobability that you get to state chipokay here in the textbook I think inthis particular example you usewith the letter M in many of my slideson use the letter T because that's thetransition function but that doesn'tmake any difference intersects basicallytransition function tells you theprobability of the aging when it is ingoal state I and the reaction age what'sthe probability go to state J so in facttransition function can be seen as anice little matrix okay if you have tenstates as one as a club where stem thenfor each action that's a a one then thematrix like this okay and then basicallythis was in this lesson satisfy the rowfor s5 helps you if I were to do a onein s5 what's the probability that oneget to s wanna maybe it's point onewhat's the property that I go to s finemaybe point five and what's the propertythat I'll get to extend I just they saypoint four so everything else is zerookay because these are the only idiotsthese are all these days and when youbuild reaction the probabilities have toadd up such that you will go into one ofthe states if after the release actionsaction that it will disappear sort ofyour state space that's magic that youcan't captured life okay that should bealso be in your states this a quicksanity question if in fact one of thethings that I wanted you to keepthinking about keep thinking about iswhen how I said something about MVP whatabout a star search that's how youconnect things if you wind up learningMVP as a completely different thing fromgame please like a very different thingfrom yesterdaydifferent thing you just one more baselless than appropriate who think of themas disconnected ideas the beauty of anysubject is that we things are connectedokay so you ask yourself if i was doingand in fact if there's our MVP is moregeneral than a staff model that problemsare we are search one okay so if theysearch actions will be deterministicright so what would that mean in termsof this most one of the states only oneof the states will have probability oneeverything else in our probability iszerodo people understand is you reallyunderstand this that's ultimately thedifference between the S box and thedeterministic action matrix and isfantastic action matrix if thedeterministic action matrix is thematrix of zeros and ones such that ineach row in fact in what cases the rowshave add up to one right in both casesthe rows have been added up to one okaybut in the case of stochastic searchit's any number between zero and one butthe row has to add up to 1 in the caseof deterministic searchit's only either zeros are ones and ithas to add up to one basically meansbecause it means exactly one one yes nono this is one one one action a1 so ifthere are 10 action that then we wantfor each other their fantasies one ofthe actions so think of each action as astate by state so an action a1 is n s bys transition matrix we talked aboutatomic was a sadistic models relationaletc is this atomic model karma do youhave structure for the states now statesare black boxes just like before so thisisthey're still in atomic models just likein Star Search the only thing we reallyneed allowed for actions that havestochastic transition is you're insanebut there is a very interesting questionso these rules are after one should thecolumns also add up to one should theyhave to say anything at all if you areunderstanding what you what you'rehearing either you're saying it yourselfare you understanding what other peoplehave muttering then you're in good shapeokay because in fact you put haven'tseen a video you would have seen in yourquestion I'll ask again what would bethe largest value of the column can takethe number of states did you guys seethat so this is the huge big six statein this because there you are whatactually if you do this action you willget it a distinct this is called thebomb being action but you bomb the classit doesn't matter how many people areevading how many people are asleep whatare people on get the lecture versusNASA at the end of the bar being actioneverybody will be in the same statethe class will be in the same state I'mnot trying to bomb the class okay butthat's the idea which is basicallyactions contain all states into a seniorstate yeah interesting so actually Iknow why I got into this this is thekind of this is the kind of that onceyou have that what he is a joke therewere two jokes there were ideas in yeahthere was a competition for jokes atsome kind of time and BBC has arrangedOhI'm very disappointedno jokes okay okay so that's that andthen we also talked about rewards okayand the remarks essentially you knowwhen you go to that stage you can talkas a reward and it's an immediate rewardas soon as you get to that state you getthat the one in fact we talked about itas if there is something called alloffense which is rewarding the moregeneral way of thinking about reward isthe reward is associated with atransition it's not update associatedwith escape it is associated with thetransition so what does that mean it'stypically in the most general here is itis like this ah as a as - if youactually a please state s and get tostate s - then there is a number thatyou are the one okay you already kind ofstart seeing that you look at one singleMVP model but there are like a millionother MVP models all of which have thesame theory they just intact it anydifferent okay and some homeworkproblems for example will ask you tothink in terms are for you know a modelwhere essentially the reward would bestate action state transition that's themost general and reward can ever getokayand so I give you a number for that andthis specifically this case I'm justsaying that it only when I say RS thenI'm just saying RSA - for all s and aRSA - equal to others - that's what Imeant when I said this is just a rewardfor the stateirrespective of which you started fromwhat action you did as soon as you getinto the state you get the same rewardthat's a special case of a more generalcase maybe we do care about where youstarted fromwhat action you took the resisted andthe reason in fact you willthink in terms of these motion graphicsin a minute we will start talking aboutthis distinction between a star searchwhere you always whined about costseverything is a cost and MD piece isthis happy place where everything is inwhat so he's like a MD peacefulactivists and histories for pessimistsactually turns out there is an MVP forpessimists - you can essentially talkabout all of these instead of thinkingin terms of rewards you can think interms of cost and if you start thinkingabout CSAs - model if you jump out andstart thinking if you ever had aboutCSAs - model then you can see that watchnow and DP would be very close to a starsearch because a star search also had aJacob H past you see what I'm sayingif you do get in terms of rewards itlooks like a star is about us and maybethe water-powered such as a friend's momin fact in the textbook for reasons thathave to do with the fact that MVP issuch a subject full of rabbit holes theydecided to stick to one specific MVPmodel they went with infinite horizonreward first state and we wantthere's over bunch of other models andat least a couple of which I don'tmatterok ok so that's your reward part and youalready looked at this last class and sothe in general we think in terms areWard's so what is the objective for thisproblem is the case of a star search theobjective is clear the organized stateyou want the shortest path to the goalStatethere's one open up whatever state allthose states are connected you considerto be equal and what to get to one ofthese four steps do you see what I'msaying ok now what is the what is thegoal hereso cool he essentially is come up withsomething basically so you should be aquality that I mentioned to you lastclasssome solution would be a policy not aplan which might say things like you notdo this put in your policy to put apolicy solution is a state to actionmapping in this particular case okayand the question of course is there aredifferent different solutions in factlet's ask ourselves how many solutionsare there if there are ten if there area number of actions per state and thereare s number of states each policy isessentially putting an action in eachstatehow many qualities of it you can have soyou can do family law economics so ifyou have transact a policy you are putan action here there are no differentactions we can take any of them so eightpossibilities here then we get herethere are eight eight a differentactions so now a times a times a timesit how many times are we doing this stimes right so in fact there are a powers these types of policies one of them issome of them are better than others justlike a path is a solution in a starsearch the optimal solution is the onewhich has the optimal cost right hereyou're interested in a policy that isoptimal and to do that you have to kindof video Egyptian and of course wedefine it mathematically the intuitionessentially about what is a betterpolicy has to do with the fact that ifyou want to follow that policylet's say if you are mistaken at zeroand you have policy PI 1 upon C Phi 2Phi 1 is better with respect to state aszero compared to PI 2 is the expectedcumulative reward are following thispolicy by one starting from s 0 is againR equal to the one with Phi T this is avery complex idea if you haven't thoughtabout it before because you are notasking so we are thinking about it isthis is essentially the value of a stateyou're talking about how good is thestate even this policy involving yourknowledge is good as is knowledge goodthere is immediately what you know thereis drinking this party coming to myclass for that right but is that reallythe reason you came to college unlessyou want to die after 4 years that's notthe reason you came to college you cometo keep the college because this stateever halves you transitions to othermore interesting states with even betterrewards and so college is betterbasically means college versus nocollege it turns out going to collegepolicy tends to open up hi Claire rewardoptions now of course the world isstochastic it doesn't mean that forevery one of you this will happen butone of you probably right after thecollege as you're walking on the imitateabove some guys device you can heats youon your father and your opportunity saysthe guy tried but that doesn't meancollege is badit means college was bad for youon top of all the people if you can livethis remember don't have me how many ofyou remember one day if you were to liveyour life many many many many timesusing going to college as the policy wasnot going to Valley as the policy thanthe average value you achieve by goingto the college is rather than theaverage value issued by it's not worriedabout this is what we need by say doingsomething is better when you havespecific actions and more because thesestochastic actions you don't necessarilyget the exact outcome we're expecting doyou see what I'm saying these are thesame thing everything but people tellyou honesty is the best policy tap intothe guy who actually say sorry I copiedin this exam and then say sorry and haveto fade you right now right and you feelvery bad saying now I'm a French teachersame thing they can tell and they areobviously passing this exam idea is theywill be Jeffrey Eckstein one of thesedays I mean the guy was a success storyand you nobody will remember that in thesuccessful like ever after this so youare optimizing for the longer run you'reoptimizing for the short turns not happybut again this is the point you want tothink about in talking about value so apolicy is better if the expectedsuperlative rewards following thatpolicy from state s0 is higher thanfollowing some other policy peoplepolicy will give you the optimal suchcumulative or cumulative reward for thatstate and in fact an optimal policy willgive you optimal stability reward forall the states that's the definition ofoptimallyI didn't write a formula and get to theformula but keep this in the back ofyour head that's what you mean byoptimal policies okay it's a prettycomplex idea if you haven't thoughtabout it before okay so that's yoursolutionand so basicallythis is the policy and after manydifferent policies you're looking forthe policy that has this property andthen it's what researching for policiesso in fact if you have some idea forcomputing the value of the policyespecially for discreet is there only apower s policies what a finite numberand you have Google cluster so you canbasically enumerate each policy computeits value vector and see which one hasthe best value vector now you know fornumber how to say one is bigger than theotherfor vectors you say one is better thanthe other if in every place in thevector this vector has that EconomicForum annual average did you get thatokay and that's how you actually wind updefining optimal policy but we'll get tothat you know who steps okay so that'sthe policy whatever I said is writtenhere okay now since we were talkingabout a deterministic search before no Iam as I said think in terms of combiningcannot no comparing this to a starsearch whatever we talked about MVP to astar search so basically they're verysimilar if they stop searching deep interms of a single solution and they tendto call it plan and when people think ofpolicy which tells you what to do inevery state and the reason you need apolicy is because you can't be sure evenif you know the optimal policy when youdo the action you only have aprobability distribution as to what willhappen to you so you know that that likethat classes to take to get the Googlejob is for example a high class so youtook it you have some idea about whatyour grade goodthat's a distribution you don'tnecessarily get the grade that youexpected you will get it depends on howsorts of uncertainties between now onthe end right and so you're taking thatuncertainty so based on that insideback in the I then maybe our dignitymaybe you have to do something else youknow you mean a different state fromwhich she gave you up against or whatdocument actually eats so you will needto go when you say a politic so in factremember when you have a plan you sayI'm going to take this particular flightto go to that one so I can take thisother flight to go to Singapore and thentake that flight to move in India andtheir friend says we have contingencyplan suppose you go all the way to SanFrancisco and suddenly did some strikeI suppose that for when you go to HongKong and you realize like several otherflight from there what's yourcontingency because you expect it to bein a certain stage but the point wasactually dynamic even though us you needto be static and so I kind of starteddoing strike so you actually something adifferent state and contingency planmeans you also know what to do in servethis other stateyou start from plans you typically tendto say this is what I think will happenfor a few things that would blow us Iknow what right policy is the activatecontinued signal it tells you what youdo wherever you find yourself and in theworst case that's what you need to knowbecause anything can happen you can findus or anyway so for example what's yourcontingency if in fact you go to SanFrancisco and find that asteroid heatSan Francisco yeah the next big asteroidyou do nobody expects asteroid easynobody expects fantastic position ithappens okay so the question then wouldbe policy would have to consider andwhat to do in other states again it'snot necessary but this is a very definedpolicies in fact the better way ofefficiently solving large scale dutiesis to never compute the entire policy upfront but just figure out what's thebestyou will what that's what I mightsuggest and what I'm saying that's whatI made the search - in fact I kind ofone more interesting to me that we aremaking it sound as if MTP can be solvedeither exactly with a food policy ah byonline just figure out what to do nowand then figure out what to do next timewhen your turn comes whereas what giveswe made it sound as if the only thingyou can ever do is online is that trueso I solve the game there is no suchthing as a solid game unless it'ssignificant again first all the gamesyou will have full strategy which is notthe full policy really for other we weretalking about that because nobodyexpects any reasonable game to be solvedin fact hips also sometimes can be fullysolved in which case they will have fullpolicy and in fact just as we're talkingabout Markov decision processes if I cantalk about a slide which says the wordmotto has many many endings in computerscience in general and they are inparticular and so just as we haveoccupation processes we have Markovgames which is a generalization of magmato person games then you talk about fullpolicies the reason I'm saying all ofthis is everything can be done foreverything we need to understand theassumptions for the model ok ok sothat's the execution time in a stocksearch you didn't have to look at thestate you could look at the state butreally we don't have a look at the statebecause they store suggestionsdeterministic actions and we know theinitial state and the roller action dueto you just say that the other stateoccurred without having Newtonbecause it's in fact your water iscorrect that's exactly what will happendo you people understand what I justsaid if you if you really are in adeterministic world and you know theinitial state then you will never haveto open your eyes in any wcq our eyesare overrated just decide where you areand then this part that's the thing youcan do it in fact if you do exactly howto walk and you know the entire map youcan you know start where you alreadyclose your eyes and walk so that's wherethey start doesn't look what it in factcan be made to look to in fact itthere's no difference between say a starlooks at the state and a star doesn'tlook at the state because when it looksit finds exactly what it expectedbecause of stochastic City MDP has tolook because it says I toss the coin andthen depending on what happens dependingon if it comes heads or tailsI can't just assume I passed that one isto retain heads under design that makesno sense so MVP models this is going tolook at the world so that's why so anexecution time you observe the currentstate do the action a policy recommendsfor us I notice when I talked about astar search with twenty gypsiesif I do a static contingencies thatmeans I've done only a stupid matterwhat is deterministic but it may notactually be nearly deterministic inwhich case you do help open your eye alittle bitmost of the time martingale for worksmost of the timeasteroids moon heat San Francisco SanAndreas Fault is a different motor butyou know asteroids don't treat surfacesokaybut if you want to allow for thepossibility for a contingency you openthat's what is happeningI have money recognizing that so that'sexactly what happens we haveconveniences it's the same this is oneother thing which makes a start-up evencloser to MVP model but for stochasticthink about a watch enough problems ofresearch remember what we did in a starsearch once there was only one morestage basically our goals are you putour core stakes are you pause so yougive a state these galaxies goal is thisin the goal state that calls for testpoints to the disabled state and thenyou declare success so the only thingthat mattered is that part of totalcaspo exists that you do to reach thegoal state right that's what you doimagine I just expanded the problem alittle bit which is more like they whatyou live in you have multiple books youwant to come to this class you want tohave fun you want to put a movie youwant to do homework etc etc lots of lotsof goals for each goal there's a contextbut really it's not a single goalbut then for every action you do thereis a cost so in this case because thereare many goals each book should begiving you some medicine and then youmake energy that is you do both 1 & 4 2then you get the benefit 1 + very fittogether part of itimagine we are a total capitalisticslightly everything is in dollars so thebenefits are dollars and then you'redoing actions like make stay awake andstudy versus you know buy a book by thecheat sheet that somebody has figuredout it's a larger class and you came upwith a plan which basically