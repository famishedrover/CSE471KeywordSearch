okay guys let's get startedstudent of mine basically put up thesepumpkins yesterday night in Boston weekon the AI you can give people more rightso I'm rising I am wearing my a It-shirt because that's the best way toscare people especially you okay sotoday we will try to wrap up thediscussion about multi-layer neuralnetworks which have been called deeplearning but as you saw from last classyou can't understand you knowunderstanding fundamental fact machinelearning okayotherwise you basically only have somesort of idiots guide the dummies guideto neural networks so again today too wewill actually have a discussionregarding a variety of things that youcan do with respect to machine runningmachine orderscorner to achieve justice connectivitynetworks and then in fact one particularPapa biasing there you know that whatturns out to be a convolutional netsthat will also discuss okay okay you canpull the Caddy like something okay so solast class in the extra session we lookat this stuff basically about the factthat in particular there are really twokinds of learning regimes that you canthink about some of the traditionallearning tasks characterized by Safamanually extracted which is okay and andthen basically this one comes from theclassical statistics and also the ADImachine learning to learn is not to highdifficulty people try to learn hundredsof parameters okay there are differentaspects going on hereone is the dimensionality the other isthe number of parameters okaythe dimensionality is how many inputfeatures are there how many Greeks inthe case of neural networks how manyweeks does the neural network havetypically they're connected but they'revery different aspects okaythis original difficulty tended to warwith low dimensional data much smallerlearning tasks whether you couldactually have high dimensional data butthen that's we converted into a set offeatures that somebody extracted becausethey know how the world works like forexample building you guys is the highdimensional task but we converted itinto a row dimensional learning problemwhich is basically the vector of youmust goes in your homeworks projectokay once I do that then I can considerhow many parameters I want to use tolearn you know the distribution of yourgrades okay in the training data willgive somebody you know previous yearslet's let's say I'm in the test I'mtrying to predict techniques given thesame vector so this is no reversal datatypically you can tell them underparameter is network path to manyparameters so in some sense you addparameters as needed because of whichit's like if you are given a bunch ofpoints you've got to first look forstraight lines which is basically whatyou get in two dimensions only requirestwo parameters in X plus V then youparabolas then you look for cubicsetcetera etcetera so this is how you gotypically this is what much of machinelearning basically went and then theother side is high dimensional taskswhere the whole thing you know you putitself is high dimensional and nobody isextracting tasks for you from the dataso you just have to deal with the datain the bar form you see what I'm sayingand in those cases that basicallyhappens for example speed in a signalprocessing etcetera interestingly inspinach in Ã¤j--processing it's a much of the linearpart speech processing much of the areacalled image processing etc they triedto extract hand coded features fromimages hand coded features from speechsignals see what I'm saying so that youcan reduce the signal into a lowdimensional input unfortunately whathappened was these tasks are speechimage etc these are classic tacitknowledge tasks remember from thebeginning of the semester so reallynobody really knows how we see the worldso this part of trying to make somesense I would say maybe you will belooking for it ages maybe we are lookingfor textures maybe we're looking forpitch in the sound etcetera etcetera sothese are the features that they willtry to extract but there are many otherserious ways we do have prettyreasonable theories and so you knowgetting the hand coded features would beeasier in those scenariosokay in the case of speech animated etcas I said followed by those who arebeing used as low dimensional learningtasks with hand coated and codedfeatures in fact when I was doing myundergraduate study we before were youand your parents were born I was doingspeech recognition isolated world speechrecognition and those who'd essentiallybe using some specific features thatsignal processing people have figuredout as useful like cash in my case I usewhat the context of of ships okay whathas been happening now is in fact youput essentially say I don't know how tolearn them I don't have any IDwhat to make of this data so let thelearning strategy learn the featuresalso when that happens you don't knowwhat we just wondering it's just you canonly see whether the final task issabudanaperformance and I finally feel good atthatthis has been known as end-to-end speechrecognition and to end image processingetc even at the start just before likeAlex nets are sometimes in 2013 beforethat two people would still be using apre-processing step get hand codedfeatures out of their images are speechrecognition systems then apply learnthis other one essentially you have veryhigh dimensional data and you just letthe learner figure out the features thereason that wasn't being done before isbecause nobody thought that was scaleand two of the reasons it actually woundup scaling partly is because if we havea lot more data from which to trainlearners they more data you can'tunderstand being young guys right nowyou don't understand that people willnot people used to have only a fewphotographs we could only take a picturelike I'm a special occasionsfor example maybe for a wedding orsomething here when you take a picturefor almost every making minute of yourlife sometimes seconds of your life andthen what you do you put them upstairsto put them at the end so they're allavailable as data if we read the systemswe will talk about this for example thefact that the entire area of imageprocessing used to have this one infiction more or less if you read animage processing paper you would seethis woman called lemma who was a modelfrom Sweden and they basicallyfor pictures such that it's only theface and you know it's about PG h PG 13and then basically use that to doeverything with such as edge detectionwhatever any any or immaterial villagecrossing welcome on that now theeconomic benchmark called image net has14 million images and the types ofthings that Facebook and Google etc useto actually train their systems havebillions of images and where are theygetting phantom you have a facebook howmany of you guys we get down you'regiving it to them are you surprised howcan Mark Zuckerberg knows where I amwell you're building something okayanyway so that's useful to know sothat's that the Italian as well ofcourse the compute part which is as Isaid the GPU aspect warmed up helpingquite a bit in terms of speeding up thegradient descent a boxokay if you understand that thesescenarios typically we call thistypically the over parameter energyscenarios because nobody starts withlet's say look for a line in orsomething okay they essentially givethem more parameters the Andy is dataokay so first of all just make sure thatyou understand that they're talkingabout three different things number ofdata samples touch one dimensionality ofdata that's a second one the number ofparameters that's the third one theseare all different and UDP wouldunderstand which is which and in whatmay you know given a problem you haveexamples you can have also do thesamples with no dimensional data you canhave high dimensional data with many fewdata samples you can do either up therewith fewer what size largerwhen you wind up having number ofparameters much much larger than theamount of data you have in classicalstatistics that's a dumb idea becauseit's basically like saying I have fivepoints can you go ahead and seat it250th degree polynomial do it you havetwo points you already have a line theseare the five points will get you up tofourth order polynomial and you'retrying to figure258th out a polynomial to the data so inthe in this basically you try to go upin the expressiveness thinking in termsunder you know bias versus VD instead ofin terms of how is the training datatraining error falling and whether ornot we should increase the capacity ofthe learner capacity meaning for exampleincrease the degree of the polynomialhere you just say to heck with it we'respark with huge number of parameters sothat nobody can come in and then seewhat happens and a few things happen youdon't really know that things areworking sort of and is that do theyactually make any sense and that's wherewe spend time talking about it in theexternal session last time in particularessentially if you continue the trainingdata becomes zero by the way some of youwere actually awake enough to ask andthen another external session last timeas to if the training error becomes zerohow are you changing the mates well itturns out that we will talk about ittoday that the loss function that youminimize for learners is not just theerror it's ever plus what we'll talkabout as linearization term plus onedoes make you feel like adding and infact this is a cottage industry of lossfunction engineering okay you want thisstuff is not working aroundand one more factor to the last classfunction is last function is theobjective function for your optimizeryou say what I'm saying so you computethe error with respect to that okay andin fact what happens is as we see whenwe talk about regularization today it iswhat is happening in this part is thetraining error has become 0 but theregularization factor continues tochange in particular we will see thatregularization is connected to kind ofkeeping the weights smaller and thereason that is needed we will talk aboutin a minute but in general if you havevery high beliefs and you have let's sayyour parameter so let us say okay I'mdoing it to kind of you're trying totrain yourself on 1 million data points1 million data points but parameters Tsee what I'm sayingyou can make each of these parametersstand for one of these data points thisis what's called in neurology you knowscience they talk about this grandmothercells like if they like a cell just thatrecognizes your grandmother or nothingelse you see what I'm saying and sosimilarly you can have a neural networkwithin each particular parameter or avery small subset of parameters are justtraining themselves on one particulardata point if they do that they will doa good job in reducing their trainingdata but they will be compressing therewon't be learning anythingremember this compression was ageneralization is a classic trade-off inmachinery okay to stop weights fromspecializing so much with top parametersfrom specializing talk so much you needto say nobody gets to haveI made it see what I'm saying so thatessentially means you take that verytractor and put some restrictions on thesize of the weight vectorI mean magnitude of the weight vectorremember the number of weights would beof course the amount of data and amountof the number of features but themagnets you can be minimized for exampleif you take al to nom the Euclidean normthen you're saying W R square plus Wsquare check for the whole square rootof that that needs to be minimized ifyou take L one arm you are basicallysaying the magnitude of the absolutevalues of the weights added up should beminimized these kinds of regularizationterms are used to essentially have wideover fitting to over freedom is anotherway of saying intestinal rememberingthat data I'm not learning anythingabout the future okay so that's what ishappening here so basically as you goforward you see that they allow a smoothinterpolation in terms of the smoothnessof the vector in particular there's somany features that different sets offeatures might be used slowly to figureout which ones will actually fit thetraining data best why did you say themain vector excites this is what washappening and then we also pointed outthat when this happens there are alsosome interesting issues such as andwe'll see real examples that can begenerated we talked about how ostrichesand so on I also I also realize that asof this morning 21 people watched therecording video so that basically meansI think more than 70 people left when Isay you know later on but they're only21 people even by pigeonholing argumentswhole bunch of people have not watchedand they were just hoping that they willget thingslet's see what happens okay so one otherthing that one eventually is thisdistinction that I'm teaching you is notvery understood by the overwhelmingmajority of people who think theyunderstand machine running not talkingabout your finger okaybut Jeffery that is not the only machinelearning specialist everybody you yourfriends your friends friends their dogsthat kept our machine learningspecialists and most of them don'tunderstand this distinction so much sothat in fact the company is using youpeople misuse it they say I am doingdeep learning I told you all otherpeople can say that we are deep learningwith one layer okay and one unit it'sreally deep accent you know anymoreother things you think that that's likea silly thing but let me show you this Idon't know how many of you have lookthis is about two years back Googlebasically there was a huge amounts ofarticles I mean Google uses deep my DTIto cut the cut data center energy billsto put this in context deep mind doesn'tdo anything useful for Google okay deepmind basically figures out how to solveone more video game like yesterday therewas a nature paper about Starcraftpeople who depend on that result SaudiPoint has major revenue profit problemsthere are always at the edge of beingthrown out of youhoover like if you keep track of what'sgoing on like everyone said well thensay google people are it's true there inUK they don't get paid as much sothey're cheap but good science butthey're not making anything for thebottom line so every once in a while ifthey actually do anything that is at allpossibly saleable they'll make a hugedeal about it so what i'm somebodybasically said this thing Googlemy di technology of course Google sayswe sneeze the New York Times you're likein the front page right so this this isLondon of course and all that stuff andit looks like a great idearead this tell me which regime is thisin it has 5 million 5 million 50 unitsper mayor okay and it has selectedteachers this is stuff that's not doneby Larry and Sergey this is done bypeople who work in air conditioning aHilda coolingyeah beside it these features make adifference in figuring out whether ornot buildings can be good you see whatI'm saying this is hand selectedfeatures how many are there is a highdimensional learning problem with 19directions if you can tell thedifference and you know basically goingfrom raw data image data then you arejust part of the masses of people whothink they understand the distinctionsand and the trade-offs and I hope youwon't do one of them I definitely testthese things and will feel very sad atthe end of the final exam but like myability ok ok so keep that in mindso again as I said that became successorfor the high dimensional regime can youeven the tacit knowledge areas therethere was no other thing the so far thehuman extracted features basically fellby the wayside because all the greathuman extracted features for our visionand speech etc you could do much betterthan them from learn features ok a fewthings that we want to get through thisbasically we saw he lead us to thefamily's neural networks one is thatthere is a bunch of pre-processing stepsthat you have to do for any big networkbefore you start actually doing thedatacenterok some of which we want to just walkthroughthey're kind of important each of themcan be like you know actual lecture youknow do that first thing of course isyou want to start by scaling the inputsif the inputs basically a very highvariance in terms of the magnitudes okaywhat you want to do is you sort ofnormalize them it's like how many ofyour heart of z-score like what is itz-score you take you can actuallycompute your z-score for example formidterm you can take your score minusthe beam for some of you that V negativedivided by the standard deviationthat's the disco okay if you do thez-score then in essence in the databasically is being converted into 0 meanunit mediansdo you see what I'm saying okay that'sone very simple thing that we would doand he Papa also winds up avoiding anycovariate shift in the input featuresavoiding any covariate shift in theinput features okay what that means isif the training data is in one scalethat test data is another scale but thedata to Y is actually the same the datato the classifying the label is therelation is the same you can make yourlearner building more robust byessentially upfront changing the inputlet me scale such types of the zero meanequation if I think about it this way ifI have two midterms and say in one weekterm people dothey're people very badly and thensomebody who gets like a you know 50points in the second midterm he is notequal to somebody who got 50 points inthe past meter because first midterm isactually harder if on the other hand ifyou computed their z-scores it actuallyallows you to compare that's what we aredoing okay now having said this thispart is just pretty much classicalstatistics does this the one problemfatherneural networks is actually yes which isevery the first layer of the neuralnetwork that the third yes layer of aneural network next to the input sidehe's basically getting the input thesecond layer is getting the outputsurface layer the third layer is gettingthe outputs of the second layer youscared the input numbers what happens tothe number the scale of the numbers thatare coming in this minute TC what I'msaying for the fourth day a guy theinput is the third layer activations andhe too can have covariate shift thatmeans forever in fact in fact becausethe way the training is being done rightevery weight is changing as you arelooking at the examples from a popularbook and they were four they might startseeing very different scaledistributions of the activations arebecoming limited is what I'm saying okaywe need to deal with that and in factfor the longest one equal T's know howto deal with that and the big thing thatchanged in 2016basically these batch nom which is oneof the patented ideas from Google that Italked a little about you know ash toysso I don't care anyway okay but we'llget to that but this one more justbefore that which is very initializationremember just like this is like you knowwhen you started the value iteration youhave to put some values and then startiterating similarly when you're doinggradient descent in the weight space youto start from somewhere so to put someweights everywhere so you want to alsohave some weight initialization methodsonce again the idea for weightinitialization is that along with thescaled inputs the initialized weightsshould try to keep that assured unitsnot saturate do you understand what I'msaying because for every unitessentially imagine you are talkingabout sigmoid units you look like thatright and up actually a let's stop okaythey look like this and so only in thisregion do you have nonzero gradient itreally allows you have zero gradientdo you see this okayso if in fact every how the gradientsare zero there is nothing beingpropagated the theory is correct butthere is nothing being propagated it'sexactly like how many of you have doneany actual engineering courses you knowsomething about opera op-amps the wayyou know we have all thought oh my godhave you got to like you know anyfunction with the audio sourcesand they'll start trying to materialstuff what is happening there isessentially the the amplifier is equalto amplify only them if the signal is inthe working region and it amplifies bymultiplying the signal by a nice valuechange so that's how amplifies if it isoutside of the working region itessentially sort of swishes the entiresignal because all of them are beingamplified all of them are beingessentially floor or ceiling to aparticular level which is that crazysound you hear so much of electricalengineering much of goods that youdesign is to make sure that you stay inthe operating regions of the amplifierswhich is why there is something calledsound engineering still left because ifyou try to do this basically you'll behearing my father died and that's whatwas happening in the neural networks ifyou do not carefully initialize theweights under you could scale the inputssee what I'm sayingin the beginning people did people thiswas happening but they wasn't talkingmuch of a real understanding so let mesee what happened okay so for examplefor the initialization idea is verysimple all you will do is you basicallypick weights to be in you know like it'sa normal distribution that is zerocentered zero mean normal distributionthe only question is what about thevariance and the variance it turns outpeople used to set it also to be theunit variance just like the most obviousthing which are okay but people havefound by engineering and thensome reasons explanations as to why thisworks but one of the idea that you cando is if you are made is going alongwith other waves and not the other wayto this particular node that means thisnode has an input Fanning factor of Mthen for these weights you initializethese weights with 0 being and 1 by Mnot millions but the general idea thatreally many many variations in fact thisone is called the Xavier initializationand that actually works for the sigmoidunits and then there is a slew unitsremember we talked about any units towhich look more like these and there isa slightly different way a clinician isimporting it a static initialization ofthe wheels which makes better sense andthat one is actually called it ainitialization suffice it to say thatbasically you are trying to kind ofstandardize the mean and the variancethe mean will always be 0 the variancerather than always being unit it's beingmade proportional to the Fannyhalf-an-hour combinations that's theweight initialization aspect thatfinally gets us to this internalcovariate shift which was the last thingthat basically got figured out in 2015and it's a huge improvement in multipleways and is actually how it looks andthe reason this was written up in handis I was going to teach these threeclasses back and as I was running to theclass I quickly wrote it down and Inever changed it but then we finallycame to it three classes later but it'sbasically the idea first idea would bethat as I said what we are talking aboutis we need to just as we scale theinputs and they put layerthe inputs for the internal layersinputs for the internal layers are theactivations of the layer before it thatmeans we did some they showed it by theG whatever is the number that's what isthe activation if people understand isright so you get a bunch of numbersthere now of course these numbers keepchanging as the weights change duringthe gradient descent so what washappening is that the internal layerswere essentially going after a movingdistribution of inputs the originalinput layer is just the stabledistribution we can thought of all thetraining graders distribution but theinput layers the intermediate layers theinputs are coming from otherintermediate in a years of the networksthat one quietly them and those guysoutputs depend of course on the mageconnected to them and the thresholdfunction parameters as well as theinputs to those weights and so on and soforth it's a mess trying to makeeverybody behave just so