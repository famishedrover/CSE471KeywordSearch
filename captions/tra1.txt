in the case of claims you have priorknowledge you start most pointsessentially this you're dancing so youput either probability for the pointfive heads and over for anything elseright that's what you need so thequestion is I want to do that first thend so obviously there's nothing naturalabout Ken if you don't quite know so Iam Telling You the already suppose hereare actually five times of backs ofcanvas I mean each one each onebasically is 10% of the other the candybacks and all of them are jerrycansfully Chilean and there is a 20% of thebags which is h2 type they have 75%jelly candies 25% blankets okay and then50% 50/50 and 3%5575 candy bag distribution given thisyou would assume that the candy bagsthat most likely that's where you stopyou are insane we do have priorprobabilities by the way there is thisidea called silesia prior[Music]but you just because sometimes peopledon't know what is thatso right basically the point here islearning about this diagnosis so anyminute should have this situation thisis prior and I picked up a noveloperatory candy bag and I start pickingwith candies from the back these are thecandies that are coming out from theeducation my question is after the firstchipwhat is the posterior probabilitiesafter the second chain what's theposterior probability so what kind of ifthat is it is one question you have andwhat they were with the deaf communityis the other question veena remove allyour life you take a statistics onproperty of poverty and statisticspursesyou may create a fabulous dynamicallyfrom statistical perspective they'regiving out what kind of bracket is instatistics knowing the bag figuring outwhat the probability of the next and sofull-face already vessels reasons I willassume that they are hypothesis whenequals h1 to GM for now I think in termsof discipline hypothesis h1 to HR andthere is a fire on themwe are chip would apply on them become Hokay and as the observations come theytype observations mu1 data's coming buteach data you can compute the posteriorprobability of H given D 1 H given 3 1 32 HT 1 D 1 D 2 D 3 HT when you say V Hgiven D is nothing but V given H timesBH right and this divided by P of Pprobability of datanobody wants problemso probability of H even D is nothingbut probability of D given H line fromproper generation I assume I know theprior probability bitch and I shouldknow given the hypothesis what's thedata properlythat should not happen all right becauseif I were they say that I'm in a twentya hundred and twenty five percent changein seventy five percent live band whichis basically hypothesis for thenprobability that I will see a a laggingnext this myself I write hypothesisgives you so from the disease symptomsperspective what were the symptomshypotheses are diseases symptoms aredata which is a great analogyso to basically we're yet now VP youenergized other likelihood that is JacobI came up given the hypothesis now thisis to basically compute is how youcompute the posteriordistributed by the way you guys[Music]okay so this is to compute the flag orsteal the property to the progeny havingcomputed is an IO for still powerinstitution over my purposes I give themcompute if you get the point givenprevious data is nothing but new datapoint given reduce it and the hypothesistimes property if I put this event dataam over aware this is essentially youare assuming hypotheses remainintermediate variable between the firedata and the currently you're insane thevariety that is causing hypothesis whichis causing the next table and therewould be multiple hypotheses areexplained the trial data and each ofthem will give different weights to thenext data point use it from continuouspower distributions you don't have theintegration that's what Bayesianinference would be here with Bayesianlearning would be so if you have justonethere are worse labeling here you haveto consider what every hypothesis saysabout this data point multiplied by theprobability of the hypothesis using thebasement that's what this guy's so thisis the matrix emission rate ain'tworking so either in many of you some ofyou but you decided to do only sync andthen you started having bias that's likerealizing okay everything okay so nextthis is how the distributions look sothese are showing individual lines areshowing the probability posteriorprobability of each of the hypothesesnotice that all the posterior alreadystarted the prior probability is herethat's exactly like points to 525[Music]in fact this[Music][Music][Music][Music][Music]give us zero probability give a zero ifyou do that then how much evidence byassuming when I said persons notthinking about a man so how the datalooksso this is an oxymoronthis is no no it's just you soperspective learning is actually victoryslowly changing the posteriordistribution of the hypothesis as theevidence processing in terms of datawhich is what is positive inference itis math learning equal to optimizationeverywhere else you hear learning enoughmanagement that's because you alreadygive upyou have to understand why these thingsokay oh just to give you this idea thisis how robots will organize themselvesokay if there are suddenly put in and itknows the map of the room but you neednot be part of the rheumatism look atwhat happened in the beginning and Iopen it you see in the beginning ropethere are red packs every bit that meansthe distance I have hired how prettybasically I could be dead anywhere inthe building then it starts walkingaround throwing its owner here and thereand the reflections come back to it theyare nicely distractions the rest of theevidence he computes the posteriordistribution of its position variableafter some amount of walking itsunderstanding of its position becomesmuch better then you say that this isthose red dots as hypothesis and thesonar us that think the points comingyetyes so I put you randomly you know intothis building you know the map of thebuilding it's not slam a tree to thisglobalization slam is simultaneouslocalization and manthis is the case where you know thathalf of them will flow and then is abattalion and you are walking and one ofthe interesting questions by the way isif you don't know so this is actually avery good question but this is also thereason why visions and frequentistsused to fight the frequentist guy likeyou would ask did you get this in factstatisticians it by mouth you want youto act as if you're just elevator okayyou don't know anything about the worldonly thing you know about is the data ifyou know anything about the world isthat you are hey you get along with theworld because were prejudiced about howthe world works otherwise you feel likethat kid who said I won't say it's a busokay so the question of course is thenthey'll ask it is the fires coming frontand the idea basically we should knowthe fire PK player that has the leastamount of knowledgeso so how do I get a distribution thatmakes the least amount assumptions wehave our infinity develop an epsilon 1epsilon uniform distribution will bediverging because the whole point ofdistributions is the area under a caphas to be 1most of you just like your feet uniformhere what would you have think for a -simply declassified you do many manythings without having any idea why you Ido mama you why did you it turns outthat if you set the problem of finally aprobability distribution that makes theleast knowledge assumptions which can beformalized in terms of the notion ofentropy distribution yeah just agreewhat you are trying to do you are tryingto minimize maximize by the entropy isknowledge if it's low entropy that meansthere is virtually no work if you don'twant to assume anything your su maxmoment so you're trying to maximizeentropypick me a distribution that maximizesthe tantrum this looks like a simplecalculus maximization image problemright because I'm asking for a functionnot a value of an X function thatminimizes or maximizesso using the bestbefore going and say that people whoremember them and us basically I'm happywith how the maximum likelihood learningis going to come up with ideas allowingyou to keep different neural networkconfigurations because by updating onlysome neural networks in someaffectations basically by assuming thatsubtractions don't exist and on meyou're acting as if this is a random setof hypotheses whose posterior zoning ofunity and the influence teams requiremaximum you know the costly integrationbecause them the inference time came yousaid everybody is in now with whateverrains you have so it becomes like just alittle network doing the inferenceexcept that single network was built asdifferent pieces of this becauseso clapper was coming out because Jeffrythan you really venturing into the basinthe other idea is random forest which wewould not afford a very simple idea sothe like simple idea we talked about itbut some of you may have heard of andthere is an idea called random forestmysteries when another heart rate but anidea of grandma father says essentiallytree a huge number of dictionaries byrandomly picking the features which youconsider introducing into the and thenwe get a forest of trees and you do itokay and similarly ideas like theglobalization will also be already canalso be cut back to Beijing so the lastthing I want to say is that taken apartand then in the extended part actuallywe'll do some baggage check in terms ofdo is I can do some benefit of basicallyabstract with this right this is myproblem if I have too many hypothesesnot only to have to compute the questionof whether it how did he create overthose hypotheses to figure out the labelafter intervening town some nationalintegration of hard to simple ideas forminimal approximating a big some marketintegration some is nothing butintegration in his vehicles is refinedby approximately big sound right we lookfor the maximal element in that sound[Music]so if you are essentially looking atthis like 2xp[Music]these summations is being used tocompute the table challenge okay to dothis instead of looking at all thehypotheses that is H 1 H 2 s H anotherway this particular part because thenetwork looks H RSP