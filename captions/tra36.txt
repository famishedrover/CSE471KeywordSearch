[Music]so there are a bunch of things that wewill do regarding multi-layer networksbut depending on how soon the patternsthat Google and Facebook are findingcome true it may have much less to dobecause I can teach patented stuffit's a batch along which I would like totalk about so this will be for theexpert did know that teapot has in factstuff out there like curing suddenlydidn't know that he could sell itmachines everywhere and so now anotherhand Larry and Sergey and have figuredout we have selling anything interestingstuff that means basically there's a lotof researchers in these companies nowand they get your patent the supplytable you can do it too but in generalthat's not a particularly nice thing forscience we will later party before we goforwardare there any questions yes so you notallowed me to discuss it expanding andsomewhat if I will get to that noobviously we can you can do whatever theheck you want you are the universitynobodybecause they realize that you have nomoney and you won't make any money soit's okayit's only when you try to do a spotterand you try to you know spatula you mayhave to set a couple of hours to thisday I don't know if you guys know aboutexample Java or what stage and patentedby Sun Microsystems and it's not thatthey were after view because you useCour they just what after Google becauseit you job because that's like apocketed Anthony so this is trade war Iguess most of the Facebook algorithmscannot any longer reduce a packet isgenerated on the news background andthen of course probably yonder Khan willsay I sold my soul as well as I manageto Facebook and so you can't use itanywhere else and so put in contextcontinents on that big one but yesexactlythis stuff we can talk about it is truehowever that they have been patented andyou have been talking much aboutpatented stuff in your class if I amassuming okay other questions okay soquick review perceptrons can be used forlinear regression binary classificationwhich basically using logisticregression if the classes are linearlyseparable this will work and thelogistic regression is nothing butperceptronit's avoid activation of the shortfunction by the way the word I've beenusing the word the short function butpeople also use activation functionsessentially the inputs are coming youweighted sum of the inputs you pass tothe G which is your nonlinear functionand that is either car thresholdingfunction from the old days of gettingthe activation function okay and andthen so if in fact there are processesare linear it's a cuckoo you canseparate themthis recognition and basically gives menothing what was apparently sigmoidfunction as at the shoulder activationfunction and constant will be lost asthe last function there are manydifferent loss functions you can use oneof which happens to be cross entropyloss the other happens to be the meansquared loss that we talked about mostof the time I actually will show you thegradient update equations for the Casaintegral as I just showed you how thelast looks like probably on at somepoint of time I will send you a workedout example of how you write updates forthe cross entropy loss function thereare even other kinds of processes forexample support vector machines for heatloss functions and you know basicallyyou can click you find every which wayyou want and then you try to minimizethat so anyway that's about the clampsand the sinking layer perceptrons can beused to learn multi class transects wetalked about remember you can start outin the same layer you can have multipleperceptrons one for each class so what'sthat when one tries to say is this classone or not what's that wants to test totell me is it wants to or not this ispastor Harmaand their 50 classes so you have 50 perseconds okay and so in fact that's avery useful problem so for example Imight want to develop such a perceptronto decide whether she be great a grade Bgrade C great dignity for the people inthe class in suggesting pass or failright so it's a multi-classclassification problem okay and then wetalked about the facts cannot and ofcourse this is not surprising because wejust look at the equation of theperceptron all is saying is you knowin some weighted sum of the inputsgreater are less than somewhere and sothat would be avery quickly so if the points are on oneside of the hyperplane then you'll sayyes on the side of the hyperplane youcan say you know okay so that's what wasa conservation region so they cannot runclasses are not linearly separable sothe question of course then is whatcauses our body maybe an inseparablegiven an arbitrary training datacan you tell right away whether or notit is almost linearly separable it turnsout that the easiest way is to run aperceptron on that data and check if itgives you zero error on the Cassadagayou understand what I'm sayingso in fact the simplest way to checkwhether data is linearly separable is torun the perceptron and if it's lossfunction training loss in particulargoes to zero okay ah dess class alsogoes to zero then basically theperceptron has converged and so that'sactually the me need to compute theminute separated because you don't getto see multi-dimensional data in a nicelittle picture in front of you exceptthat one guy used to be there I said youcould see forever chuckles most of yourtime and so you actually run perceptronsand that's basically to see whether youconverge at not and in fact if you workfor small examples that thing that willattest phosphorus was the booleanfunction examples the gooniac want toinput boolean functions that we havelooked itso the two input boolean functionexamples that we looked at last class wesaw the XR function basically y sub havea degree in it in two dimensions excelfunction looks like this includingmentions this is Plus this in each classthis is minus disappointed and there isno line that separates the process for -so this is a simple example that tellsyou if in fact one of the nice things ingeneral improving anything prove thatthe theorem is true is much harder thenprove that the theorem is false becauseto say that the arab is false you needto provide one counterexample but youcan come up with one counter example wehave proved that we shown that that canbe sparse right and so so if the theoryis participants can learn all functionswhich is what Frank Rosenblatt in effectwas thinking which scales back or justhas to show here a guy here is X I andyour town ok so that's basically what welooked at last time and and then ofcourse you can learn concepts are notlinearly separable by using either oftwo ideas one is blurring of thedimensionality first so if you have twodimensional data like X 1 X 2 then youknow basically increase it's a muchvalley to infinite dimensions of toinfinity motions there are many manydifferent ways of growing up thedimensionality one might for example belet's say X 1 X 2 X 1 square X 2 squareX 1 X 2 X 1 cube X 2 etcetera how theseare diversity so now the only point datapoint has now become an many coordinatedata point okay and then if you do thatthen you can see in this bigger space isit linearly separable and actually turnsout that any data easilyseparable if you increase thedimensionality is sufficiently do yousee what I'm saying okay this isactually connected to the idea that wejust talked about at the end of lastclass which is that any function can beapproximated with just one materiallayer in fact those are two you maythink that were two different ideas butthey're basically the same idea in factthey can be passed off as telling youthe blown-up dimensions mr. Lamson it'sthe same idea in two different ways andas I said in fact you know historicallythese ideas of an interview first wasthe neural networks there came theblurring of dimensionality stuff well itwas actually done over 60s too but itonly became widely available widelyknown in 90s onwards and then you havesupport vector machines yes and then youcame to then you came back to neuralnetworks yes what yesbeyond that there is no point so you canjust basically impact the best you cando in classifying a binaryclassification whenever we talk aboutlearning - most of the time the simplestproblem will be thinking in terms of aprimary class classification problemeven though the multi-classclassification problem can also be donebut anything that can do binary classokay so phosphine classifications areokay for that all I need is you know puta line between the passes and fails andabove it the line is passes below theline face aesthetic okaythe interesting thing is they exist somewe are moving up the dimensionality suchthat in this new bigger space data isalways linearly separable okay thattheorem happens to be current muscleshere just as useless and useful as theother theorem that I talked about at theend of last class which I'm gonna takeagain which is for NVIDIA of a neuralnetwork which one is in thereso if given any continuous function anyarbitrary continuous function thereexist some neural network with a singlehidden layer that will be able toapproximate this function to any degreeof accuracy you want these are connectedand the more you realize that they areconnected the back love you are comparedto people who think returning is alreadyyou see what I'm saying we're allconnected ideasokay so then multi-layer neural networksand the other idea of course is themulti-layer networks and as we pointedout multi-layer neural networksbasicallybut for plants being stacked in twolayers yes I'm being stacked in two daysit's extremely important to understandthat the only way you say my team a yearis if one person's output becomes theinput to the other person when you havethe multi class parser plant they wereall getting the inputs from the inputlayer and there are collisions going outas the output of the overall problem doyou see what I'm sayingthere was no internal nodes nodes whichbasically whose outputs go to somebodyelse as inputs that's the importantthingmulti-layer basically means there existsa yes okay and if you just have onehidden layer you already have fullexpressiveness in terms of continuousfunctions and if you have more than oneunit if you have to eat in layers thenyou have full expressiveness in terms ofdiscontinuous functions tooand are there any other functions in theworld there are continuous that iscontinuous that's it we say what I'msaying and by the way these theorems arevery similar to the kind of for you knowactivities neighbor theorem which isthey say there exists such a networkfinding such a network may not be easywhich is kind of the connection betweenthe kinds of questions people are askinghow the Piazza asked you if you can dofine with just one in the air are twolayers at the maximum what the heck isthe point in having 250 years what whatno they are hiding the dimensions sobasically they are computing features interms of other features every layer canbe thought of as newnew dimensions okay yeah basically theyare computing dimensions in terms ofother dimensions and it turns out wewill talk about his deep that propertymultiple times but it turns out thatthere are some training advantages interms of computing deeper networks butit is also possible that actually peoplewho very quickly might find out that infact you will be able to find the rightnetwork the right thing in their networkthe problem with this part of thelectures is this is the partage of mostinterested in the area because you arehearing about deep learning everywherebut this is the part that's not reallyan undergraduate level subject becauseit hasn't stabilized I basically did notgo to my lab today I sat at homechanging things that I thought two yearsback okay because I already even at that4:71 level things that I said last timeare no longer necessarily true becausepeople have figured out that they arenot random say so it's fun times to methis is like you are at the front hereokay it could be good into today theoriginal after massive job and your bossalso knows nothingno but I know front it also means youknow basically maybe I'll be doingsomething wrong you know because yetwhat you're using yesterday's results asagainst Chris reasons okay now inparticular I actually mention a Piazzathat is a significant amount of workthat's going on right now aboutinfinitely wide networks infinitely minebasically is essentially single middlelayer but it's infinitely wide if youjust wrap this and this is actuallythey're coming up right now this what ismeantime I think actually gave any looseantivirus paper who is doing very goodwork in theoretical directions on ournetworks right now and it turns out thatyou have a way of understanding notfully but at least you have an entry tothat direction because infinitely wideis essentially like that meddling infact if you read the paper you basicallyeven at the abstract level they say ohwe will use tangent cabinets the kernelsare can you actually blow of thedimensionality so you can have dualinterpretations of the neural networkyou can see it as doing the kernels asyou can see it as being analyzed and ifthis is not enough little by later wewill talk about probability and Bayesiannetworks and we will start talking aboutanother interpretation of neuralnetworks which is really the mostimportant one which is probabilityinterpretation and in fact if you take anovel machine learning class you shouldbe taught about probabilistic reasoningand Bayes networks fast before you touchneural networks okay anyway so that'sone other part so one thing that I wantyou to understand is the market actuallythe sigmoid Network the perceptronnetworks essentially can have anactivation function you would have forsoftware networks have linear activationfunction stupid but if you haveperceptrons having linear activationand you have stacked them into layerswhat would happen it would still be alinear function did you see what I'msaying a function of if you have importsthat if they can be a function of theinputs then you take another give me afunction of those inputs and that thoseresults on the linear part of thosezeros what you get in the end is stillbe your linear function it's stillaligned so expressiveness can't be thereif you only consider linear functionsthere should be some non-linearity whichis why we always talked about thingslike sigmoid function step function thehyper tangent function any of thesevalues Hypertime function etcetera okayso that's kind of an obvious thing butyou should remember that you must usenonlinear thresholds otherwise you knowit makes more sense so my talk on yetthere's a function tests are players ofcourse a class is just equal into alinear perceptron later and then theseare the two theorems we just talkedabout in one continuous function can beapproximated by some neural network withone hidden layer there may be arbitrarywide this is the infinitely widethat people talk about infinities is notthat data ask the computer mlo infiniteamount of space right it is just thatthe theory does not care about how wideand how big for this okay so in thisparticular case the know the infinitelywide networks are connected to the finaltrick which we won't talk too much aboutbut it is useful to make that connectionthe power theorem is a discontinuousfunction can youby some neural network with just twohidden layers okay I know we talkedabout this part two that essentially youhave these kinds of my qualities of oursupplier Network this is the master anetwork where essentially two perceptronunits they outputs are going into thisgame that's why it's smart awayokay now currently when you've got forneural networks you essentially havemany such layers maybe 250 such layersokay and so that's the part about itbeing deep but the depth actuallydoesn't forgive anything in terms ofexpressiveness it currently feels as ifit is buying you something in terms oflearning efficiency but this is a movingtarget right now in the exam if somebodyasks you in your interviews somebodyasked you you can say that that seems tohave in them and in fact some ideas butthere's batch novel that we will betalking about then connected that thesenetworks make it easier somewhat totrain them okay but part of it is reallynobody like to find completely whitethat's also true so this is the only waythat's working so for example this iswhy this part of is real engineeringokay when you bridge to be you don't getthose romantic gesture funny you knowthey look like nice things that you didmake posters of that then you just don'tlook at all like thatthe much better bridges they look verydifferent sheets and very different waysof holding the bridge to believe thatwhich etc the first pictures that werebuilt at least we have a bridge let's behappy you should I'm sayingmind there are some engineers in somesome some you know Theory people whositting in some group say let's look forthe optimal bridge before building onewhich is how these will be falling intothe water okay so this is where we upthis stuff works but that doesn't meanthis is the only thing that works and asmuch as you want to get your job I wantyou to understand that things willchange in your lifetime probably nextweek we won't talk about but it's whatyou watch when mentioning at this pointis both these types of networks as Isaid the only thing that's commonbetween the neural networks and brain isthe word neural right we can't haveneurons at any Andy you are is the onlything that's wrong because rain has newlungs and your neck was everything oneof the things is the connections in thebrain why you are so connected to eachother in the brain those connections arebi-directional they're notunidirectionalwhich is why which is why if I were togive you let's say you heard like awhole song right and you remembered thename which is basically know what thesong is saying is the name okay so ifnext time I give you the song youremember the nameif you create a neural network to takethe charm for example and then take thison tell me the name will tell you butnow you put the name of the song aboutput it this you're insane everythingflows one direction if you wanted me togive some the song names I should be thesong there should be something else Ican do it but your brain does both thisis basically what we call associativememory if I first of all I learned andbad things okay I defied them basicallysometimes I learned usas well as the name of the sound as wellas who was a handing out the first timeI heard the song as well as how good wasthe bread that I gained on thatparticular day all these things areconnected and how many of you have ithave done and I'm sure many of you havebasically heard a song and suddenlyremembered something serene from the youknow back at the busyou are not neural networks you are notfeed-forward neural networks so if younever did them will you probably ask mefor your networks okay yes yes that'swhat these are and a large 1960s and itis why they couldn't do anything withthese they were doing also what can bydirectional networks in fact there aregreat names for these like Hockfieldthat word Boltzmann Network andrestricted goes when Network it's a tryand most people who got into who gotinto your efforts and a long enough timereally wanted to find the in practiceand I don't know if I'm I probablymentioned here but they pass a course ondeep neural networks that just hit thetaught and Coursera in 2030 it's nolonger available on Coursera becausethey removed it because it's completelydated in the sense what people are usingis not what wasn't that course okay butif you but it's actually working on iswebpage and that course essentiallystill talks about what he was hopingwith what what he was hoping with workwould be some combination offeed-forward networks and bi-directionalnetworks he would have this idea ofstacked balls when machines withinand the interesting reason why he waseven considering it is when we get tothings like Bayes networks whichhopefully when you get to variousnetworks are essentially industryrepresentation of probabilisticdistribution and connected to othernodes connected to other nodes and thereare arrows except their semantics arecompletely different there is it'sbasically a probability distribution wewill talk about it more later but thepoint is there if you tell me what istrue here I'll be able to tell you whatis the distribution of any of thesenodes if I tell you what is true hereand here I can tell you the distributionof these nodes probabilistic networksare essentially bi-directional okay thetest distribution has no distinctdistribution has no direction and so infact his idea at that time was to cleanfeed-forward networks by viewing themlarge feed-forward network for exampleas a Bayes network and use prognosticnetwork learning techniques oneparticular one we will talk about isexpectation maximization and thenrightfully bit and then do sweep forwardNetwork Starthe reason was not moving forwardnetwork wasn't working even as he wasteaching this course engineeringdevelopments made it so that in fact notmore feed-forward networks already areeasy to train so you don't need thesethat doesn't mean these are never neededthey are not needed for this if you wantto understand actually have brain wordsyou'd better understand this stuff andin general these are what are calledgenerative models these are alldiscriminative modelsso these guys only learn the connectionbetween if you give the import I getlockers because in essence theprobability terms they learn just thispeople given a conditional property okaywhere are these guysLerman oh i joint distribution and thoseof you might still remember theprobability I remember that soessentially if you if somebody waslearning this and somebody else is onlylearning that middle part so somebody isnot EPA B and you only not be a given Byou not less that those other guys areinsanethe guy will not be happy and do whatyou are doingplus more because you only be able tofigure out the distribution of a given Byou see what behavior where I can domaybe that's what generative models arebasically probability solutions aregenerative models and you can think ofwhat neural networks are trying to do inessence as basically learning thisdiscriminative model P output giveninputs in the best case I've got on theprobability first and then done thisthen I won't have you will say my godthe end of the channel is coming it isno deep learning I'm sick of it I'mtired of it'll bore some errors so giveme the deep networks and keep youhopefully and then I tell youactually is the background that you needto understand to understand why heburning okay okay so that's thatbasically bi-directional what else andand that's about keeping up but mostlythere's not too much work going on andthen your network side on this one okayokay so and then you are talking aboutmulti-layer networks last time and wetalked about format Alain networks toyou can essentially compute the weightupdate routes just the same way use thesame children and in particular Ipointed out that this is moreadvantageously in fact you do thatbasically one height one question ofcourse is when cycle is for doing isthey were worried about okay an outputlayer I know what the error isI know what is supposed to be the outputI know what the network said is theoutput and so I know the ever so I canuse the ever to change the Ringswell how am I supposed to know what isthe ever and period notes I don't knowwhat they are supposed to be this isstuff you've made up in general inneural networks there's only twoobjective tools the inputs and theoutputs everything in betweenyou just made up and so you don't knowwhat their values are supposed to be butyou want to be able to make them uponright way such that given you inputs youwould be able to correctly get the rightoutput you see what I'm sayingright okay so basically people are veryworried about you touching this problemfor long is fine because they didn'tknow what the heck is going to happenabout the error here and you knowactually for a while in fact that way topacify psychologyto say well you can think of this error100 and then you can spread it around togive me one of these guys because thesefour guys are connected to the top -with the power I have over 100 theseguys have somehow contributed to haveanything ever 100 so we will essentiallykind of a portion the ever to each ofthese in proportion to the meet withwhich they are currently connecting tothe target it makes sense of intuitivesense we do not know whether any mainarea here at the CapSense are not whatmake introduces right and in particularone of the makes is 0 then this guy hasmy error because whatever this guy isdoing it's not contributing the errorthat you saw right but it turns out thatyou don't need to do that psychology isof that sort of a pre mathematicalreasoning in fact you could just do youcan just do the the chain rule basedcomputation of errors everything's oklayer better okay and that's basicallywhat