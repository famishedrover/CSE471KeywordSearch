103 last was what's the main differencebetween model based on water peeyou don't compute T you don't estimateokay instead you directly go to thevalue 1 policy so you note thattransition and as I mentioned last timeis you do not have P then you can'tmentally simulate the world in your headbut you can still be able to reflexivelydo the right action and I told you thereare cases in the world such as swimmingwhere you don't already know how to doalready don't know how to simulatementally how what will happen in new toyto make certain various you know actionsin the world you actually just have todo it anytime you have to do it thatmeans such model is free you don't havetransitions whereas you suppose you'retrying to make a plan to go to likeBeijing okay you have a model Iunderstand sometimes planes don't boreon timeunderstand that sometimes Hong Kong gatecode might be full of protesters butit's not that it's completely 1:03 youactually makes some kind of a planbecause have a model you know that'swhat are free way of traveling iseveryone saying which makes no senseobviously for us because in fact we dohave reasonable model so my point is forsome problems there are models and whenthere are models we want to use them forsome problems which be very hard to getmodels you know it's not that there aresuch they're very intractable ok I'm soI'll try to your process it's better notto think about models so this issue ofmodels of data versus not dead actuallythe problem becomes a philosophical onefor example people will take the viewthat there is no such thing asstochastic city in the world except atthe quantum level some people will saythatso you for example they say I toss acoin and some guy with incidentsimulation capabilities might say youknow so it's like it's all well knownwhat the equations of viscosity are whatthe equations of gravity are so if youhave the figure if you have someinterest you can exactly compute howmany times it will world how many timesyou will get pushed by various Eddie'sin that particular area and exactly howstochasticin fact a lot of times by the way peoplewould like to assume that the world isstochastic where they can't be botheredto do the computation you see what I'msaying like for example our ancestors isit sometimes seems to be innovativesometimes some since we graded who knowsno obviously we'll make this daikon Godwho seems to know everything and we justsay our ignorance means that and ascivilization progress God has becomesmaller and smaller and smaller okaybecause we actually know exactly whenevery particular eclipse will occur wedon't need God for this anymore forthose of you who still want to believein God for eclipses you are basically[Music]you should be okay okay so then give mea that's the quantity of you you cansome of the heels Personal Transporteridentity questioner is the model for theproblem tractable water for the problemthat's kind of an existential questionand in fact as I said quantum physicspeople will say everything as a modeland there is no really not even there isno such thing as an acceptor and this isa question of do you want to act in theworld in a model see we are modernbasically so we are talking about thesecond part an idiot can live in thebeaching you know travel back to Beijingworld in a model three mate so forexample it might have compiled astrategy if it goes to every day doingit all the time can you basically saidif I am on eleven you then I turn rightthen I sit in this you know how thatmagically appearsthen I get down at this door for youknow Terminal four then I take thiselevator etcetera etcetera so all of theactions are this big deep and then upwhich will be really okay but most of usdon't solve the emerging problem that webasically have the model and then we usethat to plan you know okay which is muchcheaper than actually going you know I'mtrying and page okay so if the case ofmodel free in stuff you know learningthe transition T we just want to learnthe value of a policy of the best policywithout bothering to learn t model freeis most reasonable when in fact learningT is too costly it's unreasonable if infact learning is easy but you are toolazy to learn it okay but it doesn'tmatter you know you could use thealgorithm either region and that's allthe like motivation is thatis to be the action done from a statecan potentially take you there put ahuge number of states sometimes thespace of actions is infinite so we youknow as much as we are looking atreinforcement learning in this nicediscreet big words like that's the onewe're looking at where each cell hasexactly four actions okay and they canonly take you to four different steps ifyou start considering in basically factif you are spaces such as swimming rightso you can put your hands in anyarbitrary fashion some a number of themmake you sink like a stone some numberof them make you flow some number ofthem also let you go forward having anentire model operate is much fasterbecause they say at every point in acontinuous space in essence you haveinfinite branching factor that in yourspace with continuous action setswimming is one of those it's both acontinuous space because their positionin the pool can go from you know it'snot going discreetly to be and theaction space is infinitely in thosepieces for sure model free is betterokay so rather than wait for T toconverge in those cases if you try tocompute T it actually computing he canbe harder than computing the estimatingT can be harder than estimating so yougive up on estimating T and then godirectly to estimate me and of courseonce might be nearbyokay so that's one of free nowhave you seen water free or until nowhave you seen one other than where wedid not estimate T so I'm talking tomyself mostlythe very fast algorithm for RL which isa dumb beware them the Montecarlo didnot compute e right so Monte Carlo isyour first model three of them youalready know what what oh forget thatI'll give you a better one you guys gotit anybody having trouble understandingthat Marta cargo is a model for youngeryou if you don't estimate T then yourmodel free that's it that's a definitionof Model T here but it's modeling okayso you saw the moniker of already what Iwant to do is a better idea that MonteCarlo in the case of massacre or what wewant up doing is we got huge episodesfull of episodes at the end and you takethem and evaluate the you over eachother a resource what I would like to dois somehow for every transition can Iupdate my values for every transitioncan I update my values that's what Iwant so imagine the following thing youare basically initialized the values ofall the states to be 0s now we made atransition from let's say s to s - thefact that you then you try to do anaction s you went to s - is valuableinformation because it tells you thatyou will value of v-0 of s1 shouldsomehow be closer to s2 value p0 s Tmodulo this action cost you agree youdidn't know before but now we knowremember the reason we thought MonteCarlo is a bad ideabecause it doesn't take into account thecombination between states whatever Ijust sayinstant information with mistakesintercom so clearly just like ATP thisour method somehow making your value becloser to your neighbors value everytime you see that neighbor that's awinning idea only thing of course is Ilet say b0 s is one and this one is onev-0 and then I did an action a in s1according to the policy file and I gottaguess true and the V 0 of s 2 happens tobe currently 59 I know that one is toolow I need to go towards 59 how fast doit has become 59 our phosphor and becomemy neighbor I could say well really if Itook the action whose cost is like say 4units and then I am going into the placewhich has 59 units value then 59 - 455must be my which is a reasonable thingto say but for the fact that the actionyou do is a stochastic action you don'tknow how often then you do this actionin state s1 when you go to s cube maybein fact Antinous to you the propertiesare that when you do this action a in s1which probably 10 power minus 25 you gointerest you just happen to be the luckyguy and you know and then people areteach about if you just change your V 0s to be 55 you make too big a commitmentyou guys see this what I'm saying so youwant to throw in a little learning rateyou want to say I want to put the wordsmy neighborhood but stop of what I wantto say is I take some is alpha which isa number betweenyou know zero I take 1 minus alpha mydamn you plus alpha of my neighbor'svalue and I said that's meweighted average aren't you giving thisanother sinking feeling as to why did Itake this course we're after lower 11process we find the weighted average andthat's like the greatest idea this iswhat I told you reinforced learningalgorithms are extremely simple theinteresting part is why do they makesense and under what condition do theyhave what properties ok so that'sbasically what so what we did is I wentfrom s1 to s2 and because I saw thattransition happen I change the value ofs1 a little bit so that it is consistentwith the capacity and this happen tobunged right basically it's a nice one Ididn't action asked in when you doaction causes Chagas flows in theforward direction so after some timebasically the time is very forward andthere was a difference in your valuebecause you took action a and when youquestion and you're taking the scapulardifference in the values and you'repushing yourselves in that directionthat's why it's called average theconcern and the thing about this is heis completely missing in action there'sno T if in fact this particulartransition getting you to state s2 ishigh degree you don't have to worrybecause it will never again happen andyou will keep seeing that you areactually surrounded by manypoor neighbors and so he ritually youwent let's say from one to four maybebecause of something alpha learning ratebecause we had a fifty nine neighbor andthen for two you go down down down downdown too many - the word tells youwhatever does that's what one of threedoes basically wonderful earners look atthe experience and then update theirvalues extremely simple idea okay so inthis particular case what we are doingis you are taking you know you I'messentially taking me by Affairs and I'mdating the value of cash with respect toPhi to be the current V PI of s plussome learning rate alpha times thedifference the difference is my valueand the value the transition plus valueof the next state because the samplewhen I got us - the sample basically isthe transition cost rsas - which is whatI was telling you for units cost that Iincurred in this case it's a reward plusthe s dash itself has a current valuethat's me bias - and then if you aredoing discounting you also have to throwin a gamma because there is - value willbe yours only after one step and the badnews one step away is gamma less thegamma times less damn value now so inessence the sample then you will be sand when you dig is which is actually ato the policy unhuman us - then you gotthis number which is basically a samplevalue you subtracted that from what youcurrently thought is your value that'sthe difference that should have beenzero if things are consistentbecause it is if in fact if I can workthis differential becomes you know zeroso that baby's basically making you goto words conversion looking at this ifyou want to read exactly is you canessentially say one minus alpha times Vby s plus alpha times sample is that noone - up four times before he is yeahlet's just rewriting that basicallyweighted averages so here's what youthink even though I really understandingthis you should have already and youhave any questions as to what isovercommitted you have to convinceyourself that it's in fact s - is one ofthe lucky things that only happens oncein a blue moon and because of which youincreased we fight too much even withrespect to the learning rate alpha itwill slowly compound because anydedicated s that won't come because theprobabilities will worked out in theneed all of these are only supposed towork in the limit our properties aresupposed to work on in okay well you canbuy us because when a recurrence is bydefinition real it is your time same aslong as you have you're not braindamaged and think that the one time youwon the lottery must mean all the timesyou win the lottery without having triedbuying tickets more and more times youwould get if you kept on buying ticketsin the lottery you would realize whateverybody hasis already that lottery buying buyingtickets or luckily it's in fact dollarsis utility dollars in value then buyinglottery tickets is at access to it youwill realize that if you just took theneck the word the word do just stopafter one with meaning and sfor whichwas in one and everything else or thatjust a mistakeI'm sure again I'm continuing to be achosen one and then you know how manypeople by the way this is trueapparently how many people who wonlotteries have died poppers move it up alot because they tend to think ithappened once I'm sure it happens a lotsometimes so the next fifty ninethousand times I news the lottery andstill six wave I'm supposed to be that'show you become Papa's okay this isactually a cute way of remembering Imean it's like understanding in factwhat temporal difference learning isdoing he essentially computing movingaverage x1 x2 etc you know exam andyou're computing their average that's Xhat of n which is the average of thefirst n numbers so now if I get the nextnumber n plus 1 xn plus one plus onewould be this right now what X out of nplus 1 which is X 1 all the way up to xnplus 1 divided by n plus 1 can also bewritten as andlet's have em this is our support I toldyou there are 50 people in the class andtheir average mark is 10 and it is 50first person comes and they remark is 25what is the new average unless you havefailed a safety and GRE multiple timesyou know that it's not 10 plus 50 by 2it's 3 times 10 plus 25 divided by 51remember I told you how to journeyeconomic tracks through this extremelyuseful this is like a random questionkeeps coming up in ers because theyrealize that most people don'tunderstand the concept of moving averageokay so we can write that as plus 1times X out of em plus xn plus 1 minus Xveteran because I basically added oneextra one and I subtracted it now I canremove this part English come out andexception plus 1 by n plus 1 times X ofn plus 1 minus X hat I so we arethinking about it is the new outer edgeis old average plus 1 by n plus 1 whichyou can think of alpha times thedifference between the new sample whichis xn plus 1 minus the old average whichis X happen I would argue that'sbasically more or less the structure ofthe temple difference equation of coursehere there is no notion of one sectionsection I'm just saying the math worksthe same way okay so if you must have anexample here is like a simple word ABCDin other states you are here and then Iobserve remembertemporal difference you must observe thetransition when you observe thetransition you update the value inMontecarlo you treat the same thingexcept you have to observe all sequenceof transition which ends in to someterminal node or something and then youupdate okay here for every transitionyou're updating so you are consider youstarted with zero zero zero eight okayand then I like was basically instead Vand I went east and then I apparentlydid get to see and in doing this I lostminus 2 that's their cost of transitionso because of bridge in essence this 0should be pushed towards minus 2 rightbecause the total cost is minus 2 plus 0and it has to be pushed towards thatwith the learning rate the learning thatI use is off I put a half and then it'sgone factor gamma equal to 1 so itshould be pushed towards minus 2 at rate1/2 so it should be minus 1 okay andthat's what I wanna show this basicallyis the actual number is being crushedwhat you understood this is so you won'tthe mom I learned how to do there weredifferent learning mom asks they show meand so I had numbers then if I can get anature paper soonokay so that particularly you continuehere the people to see from then you goto be and that point isn't true toyourself that update for this is easyyou see what I'm sayingokay so this is sample of differencelearning for every time the transitionactually occurs it updates and if youdoes it an often of times you knowbasically converge to the same value ofthe value of the policy because we arefollowing the policy okay now the nextthing is now that I have the temporaldifference can I defer the classicalmodel free idea can I do and it's amodel fleet passing because I'm using apolicy tax event you have to get allthese technologies into your headpassing means you mean given a policyacting means you get to do actions modelthree means you don't have T whateverbase means you have T discounted meansthere is a gamma all this off you knowthis on this terminal you are believingfine but anyway if I just went with TDand I basically remember the way I wentfrom ADP passive to NDP a pitiful almostand quite a friend but I said I happyI'm supposed to be just using it tocompute V PI but you looked outside Ican put in a greedy policy all right Iwant rightcan I do something like that for atemporal difference learning where Ihave the value turns out loop becauseimagine here's a very interesting thingto understand this imagine you be and weactually sparked about this once beforebut I'm you know this time it's actuallymore immediate imagine I give you V stopI get a wavy stuff and we know that ifyou give me star you can get PI star canyou get it PI star given me star in themodel 3 C namein the bottle failure you know daddy youdid kill about D before what you haveyou don't count when it's missing yourelbow my god what do I do now because ifyou don't have T the way you can wouldbe starting to buy stock you start atthe state s and then you consider allactions that you can take from that andcompute their expected valuation thatwill be in terms of transition watch onthe values of the neighbors and thenpick the action that is maximum in thatexpected some since you have TVC there'sa far side cartoon by the vectorphilosophy the far side cartoon far sideis full of amazing no scientificconcepts there is this one particularcartoon which I should send you with thecows you know how many of you have heardof far side cartoons before okay goodany of you people who raise your handsyou know and all I ask these referencesyou guys can get very high grades if youjust talk to me because you have theright cultural background but that'sstuff you need to figure out anyway sofar side basically so laughs and GaryLarson is in the pouch as we can tellbecause of diversity and then thiscaptain which I will send you that twocows are sitting in their living roomsand then on the form of all phone isringing and the cow what now isutilizing they put the phone again andhere we see lacking opposable thumbscome on guyswhat did you do with the corner you canpoint it if you have you never part thatopposable thumb was so useful for yourlife because you had it if you had itmissing then try to thicken phonewithout a possible pound then yourealize what comes back as long as theTR function was did nobody thought hewas important to convert V star into PIit's like - if you know plus and minusis there but it's missingoh my god how would we start into mystuff this is moving its opposable thumbokay that looks like me nicely paintedalso clearly back partner but thankfullywe did tell you last time that insteadof tracking V values you can track Qvalues you can track few values okay andthe Q basically would be saying you knowif you want to do actually even state swhat is the expected reward you will getwhat's the maximum expected reward itagain and then of course if I give youthe Q values V value is nothing but maxof the two values for any state thisyou're insanethat's where Q energy becomesinteresting because otherwise it's apower goes out you know faster what youwent away and you didn't know what to dookay so instead of learning whichbasically was just like you know relearnyou basically matchingQ values to be consistent with theirneighbors few hours instead of not Gvalues to be consistent with theirneighbors and if you do that then youcan do active learning so you can thatyou managed directly by the way this Iand I can't imagine it only probably butthese are the equations would look likeif you are keeping track up valuesversus Q values okay values the max willoccur because the bellman says the valueof a state is the value that it's thebest action will bring it is Q valueseach action what's the maximum you canbelieve me and then a maximum will bedown as and when I need you and I needto figure out what actions are doingthis act in the state I think might bethe highest um okay so that's basicallyit you learning so these are all the Q'swill be accessing it from the point ofview of numbers essentially instead oftheir big s mumbles but you know svalues in their states now that we atimes s numbers where is the number ofactions per state if you are wonderingsince I put in this Authority your headthat things like see me will haveinfinite number of actions right that'lllook good infinite times five still init so those cases you will essentiallyhave to learn USA as a analytic functionand maximize that then you do actuallyuse calculus to figure out the gradientwhere the gradient vanishes and saidthat's the a form HQs phase phase thisis just a little probity to continuousspace direction we won't do anycontinuous laughing okay so so far interms of Q learning essentially we doalmost the same sample now will be RSA sdashed plus gamma x max the value ofthat state the value of that state ismax a - us - hey - right this is theonly thing that's differentpreviouslywe just about you it would be max us -qsa that's - you - so that's your sampleso then Q of X n now will be much to beQ of si plus alpha times sample minus Qs the same temporal difference exceptnow for cuter okay when you are doingthis that's basically equal in you cantake us a one-man sub problems we haveas a plus alpha times the sample so wehave weighted average and so one of thenice things then doing humor do you meanyou can nicely now of course you'reactively acting in the world so rememberin the UDP we said that any given pointof time you have a very deep policy