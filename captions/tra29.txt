I think that is coming to you as theinput will be that's externalized evenresume nemedians people understand thisis anybody having any questions aboutwhat I just said if you you know this isa people keep saying in background isimportant this is basically what it doesit's because you're trying to figure outfor the intermediate layer just beforethe period near I'm sorry trying tofigure out so you have an it immediatelyin here of nodes right and then theremakes coming yet they have inputs comingfrom the layer before it and they havethese activations for these inputs andyou want to essentially make sure thatthey are well behaved there are alwayswithin the same sort of a range and ifyou the right way to do it is to workedon the entire network before such thateverybody was just so in tandem thatthese values here will always be zeromean and some required medians that'simpossible to do because there could beabout a million gates before you thereare changing at the same time you seethis so the wrong way to do it is whatwe try because the right way doesn'twork okay what we'll see is with it weforget about the inputsforget about how these guys found ofhaving the values they had I take theactivation values at this particularpoint of time for any particular minibatch that I am passing throughnormalize them to have 0 mean unitvariance you see what I'm sayingokay so I'm basically just going toforget about these gets one of havingsome values based on the weights and theunder threshold some of the previouslayers such a track these guys had someactivations I just basically take thosenumbers forcefully convert them it wasthis course essentiallythis what I'm saying if I do this I'mguaranteed that my inputs would be wellbehaved but it turns out it wouldn'tlead to far an idea is that it ispossible that my just as in the case Ihave found in the case of severeinitialization we notice that we are notputting the weights to have a differentvariance zero means but one hour andmillions weight and is the panellingparameter so different layers might windup having different mean and variancewhat you will need to do is I don't careabout your parents what I'm going tostart say from now onwards you shallhave variance beta and the mean beta andvariance gamma that's what I want to sayokay how can I do this I do that a stepthen I got this course which has 0 meanunit variance and then I basically Iwould pass the z-scores to the nextlevelinstead I pass gamma times D plus betaif Z is unit it is these zero mean whatdo you think is the average of thisexpressionit's just beta and the variance would begamma rightthis is what I want all I need is somerecharter to tell me what beta and gammaokay that sounds like if I do this theneverything seems to evoke a mean lookthat we all might need to do is make theentire network we have just so that bythis layer things are already scaled butwe are beginning that you are okay thevery easy thing to do is to just atevery level convert the activations into0 mean unit variance that's a littlegoing too far in terms of simplifyingthe problem instead you say you guys canhave different means and variances gammabeing the variance and beta being themovement is what I'm saying and then itputs out there 30 to the next layer andthen then you're like what Musa the onlyquestion is would usually get mamitafirst idea might be that oh I'll justfigure out and randomly generate somegammas and betas and further the beautyof batch nom idea is your reach uncle isthis really slow or a chunk of gradientdescent okay about modern neural networksees do it that's it so you will let thegradient descent go gamma and beta thisis kind of mine Dannybut because usually fog there may bereason over able to look at the weightsnow saying it is not only looking at therates it will also try to learn how tomemorize itself at the baby is not howthe network should Nava is except at thevarious layers just so just so what justso what what does need a decent do whatdoes it optimize then just so that thelast function is minimizedthat's it it will find gammas and betasfor each of your layerseach of your players you find m.asamwith different builders are differentgamers because these will become weightsjust like you're learning me it stilllearning that mustn't lead us to youwe'll start by setting them some valuesand then they will slowly change andit's the same line the gradient descentnow this is why it's very important tounderstand remember what I said when youfunction of composing functions becauseof reach the back propagation isautomated step by step by step right youagree with them have changed that whichthat point is now the neural network nota composition of functionsit still is because after linear sum ofwaves function on those values then youdid this transformation on a bunch ofthose values with gamma beta a parameterstill alert and you will just learn thepartial derivative with respect to gammawatch the derivative with respect tobeta and use them to change a man with aunit in the right direction again ifeverybody thinks they completelyunderstand how this stuff is going onyou know in Sabah you're hoping thatwhen you decide which is the right thingokay but really these are little littlechanges for more and more parameters andthe treatments are being if I have aproblem how do I come would it be aproblem that mighty jungle they do whichis very slowly chime convenient I saidthere was a question before yesbecause it turns out that you do thatanything throughout much of what isgoing on right now is engineering thereis actually a beautiful so two yearsback in 2017 at Europe's a littleevening and been wrecked these twoothers was given a price canyon bestpaper award price for the work that theydid Denis back in Europe's that waspublished in olivesokay and so then they get to give a tenminute presentation about it and Ihighly recommend that you watch a LyraAmy's new lips presentation basically hestarts by saying like one of yourcrotchety or the granddad'ssaying you guys you don't have anytheory is true batch Nam has become thebedrock of the modern deep learning wedon't completely understand why it workslittle by little we may be understandingbut not anywhere close so the answer toyou the simple answer is it works betterif you put them on beta if you don't wetalk about as a but there and then nowthat you figured this out then you willmake up new room you ideas saying itturns out that actually just as I meanyou would have asked the interestingquestion is why did you ask that samequestion here why did I do the bignovelization such that the main it isthe same thing it is there is a realreason when I normalize things theyshould always have minute various okayagain Batman is so huge in terms of inGmailthat what used to take a cover thenumber the first time in the 2015 paperby jaggedy distance record and somebodyelse came up they basically said whatused to take weeks now takes hours totree because the number of books thatyou read to get the wage to somereasonable place has reduced quitesignificantly which is why you start offyou know if you want interview is goingnot all that good you should say I'llthen use batch now and then we say whatin the batch not because they know mostpeople don't understand then you throwthis stuff up among it up tonight okayokay so that's about that job so that'sour first okay the mind-bending thing isgamma and beta are learned by thatfoundation the same old vehicle whichwas 11 even is okay which is actually ofcourse what we're trying to do is we gotsome input and we're trying to figureout basically different networkconfigurations even if you have a suitefully dense network if you make some ofif the network points are learning someof those weights to be 0 in some senseit's learning a smaller Network okay sothis network is basically giving you afamily of functions and you're kind oflearning like every function from therethat minima is a star training error tozero training error I may be too closeto zero okay so in general this kind ofbrings us to this question machinelearning which network configurationsthat means each of the weights from thisnetwork that can be thought of as ahypothesis it's basically yourhypothesis about what the function isthat is underlying the input data rightmaybe maybe many functions mightactually be consistent with the inputdata we have seen this if I give youfour numbers I'd remember the time whenI said two four six eight and you guyssaid ten man haha that's not true it'swhat you do okay because they're like amillion functions that I can fit suchthat they will be two four six eight atthe first X 1 means X equal to 1 2 3 4and you get any random other valueafterwards and the entire point ofmachine learning is not sitting to thedata that you have in front of you butdirectly predict me the data that youhaven't even seen to do this you need tohave some metaphysical biases about howthe world works we can all of thesefunctions are just equally good in termsof capturing the training error so whyare you picking one over the otherthis is what I'm sayingso bunch of much of machine learning isto realize that not all hypotheses arecreated equalof all the zero error hypotheses youmight like some better than the other ifyou use knowledge other than thetraining data to figure out whichhypothesis to pick that's externalknowledge from outside of the input dataright you can think of these essentiallyin machine or in terms of how this biasI understand outside of this room if Isay bias people will say or whatever sowe should not be biased but then youshould say if I'm not bias you won't getany nothing that we are all biased okbecause any one one can anybody remembera good example of bias that I gave herein this class about how people use biasno no the other onelots of it yes no that's those no I'mtalking what simpler one yeah I'm gonnakill youyou took the kid I said son this is thebus right find the fun think the Empiredamn give me the bus why did thefighting this kid say maybe that Bibleis the bus maybe the steering wheel isthe busnotice that kids don't move these wordsthey don't know their words and theirmeanings they're trying to learn thewords and their meanings at the sametime why did the kid basically say youknow any of these things could becausemost babies and kids are under the busbut it's important to understand it'simportant to understand that the rightway to do machine learning is to beeasier in fact when we get the properestate reasoning and when we talk aboutsuch learning foundations we will saythat the real foundations of machinelearning are be easy and learning whenyou never make up your mind you seebefore I came to before I saw the data Ihad a prior over which hypotheses aremore likely which are less likely aftersee see that beta I pushed in Ian aboutthe likelihood have the difference Ifocuses I will pick one and keep all ofthem to predict any test data you giveme and basically I take the weightedprediction cavity vote if you do that itturns out the easy one is the record youlearn much of the stuff that we talkabout as a problem such as overfittingis because we are not good enoughBayesian learning fast the human Secuniamatch our machines are also not able toright now to be easy and learningefficiently if the dude is there but inpractice they're extremely slowso just like the Bayesian keep movementunder the bus the Ovation computer alsobasically won't go anywhere because it'skeeping our life for this at all butonce you know what is the right peopleto do it actually winds up helping andin fact many of the things that wetalked about here you can see them astrying to get to these what else did wedo by the way maybe try to get to theright idea of different ways thegradient descent should really be doneusing newton-raphson methodnewton-raphson method would require youto use second derivatives secondderivative is Sen Sen means M Squarednumbersso instead much of the so-calledfostered optimization methods such asthe momentum method and the atom and allthese things not doing newton-raphsonbut somehow getting an efficiency whichis between just pure reading decentversion of the same thing can be thoughtof for the entire machine learning muchof machine learning problems likeregularization and randomization that wetalked about which is connected to dropout which you want you to hear aboutthey're all essentially because we arenot good enough to do it militialearning but we know that we should haveinvasive learning so every once in awhile we'll do some fixes to ouralgorithms it's a very useful thing toremember because it's not what the ideais it is where the ideas come fromthat's important in life I understand aswell but this is exam for this questionis in this particular courses concernjust want to know what the idea is sothat you can prevention but after thisthe real you would ever use any of thesestuff inas I give us just being downstream fromthe place where somebody else isproducing ideas okay now so thecommunity perspective is anomaly fromprior or hypothesis the trap in generalthe bias basically is sort of making youone of these hypotheses over the otherhypothesis in some just keeping all ofthem this is what we're essentiallynormally by us basically if a superviseddarkness in general typically it comesfrom the background knowledge whicheither you have learned over your periodof your life are you came into thisworld indeed so for example the kidbusts hypothec the kid busts exampletouch what's called whole object viruskids just seem to come into this worldhaving this virus they'll assume untilproven otherwiseif you saw a show like this and say theclass I am NOT saying that there is theclass I said the whole thing is that wasokay that allows for learning actuallyto take this more efficiently okay sofor subbu is learners which don't haveany background knowledge such as neuralnetworks you need to smuggle backgroundknowledge in some strange ways okayand the strange ways include for exampleso basically you need to make themprefer specific process or functions tokeep more functions around to reduce theproblems involved in overfitting okaythe three idea that we'll talk abouttoday would be one is the simpleregularization which is basically whatis called the last function engineeringwe I already kind of explained to youthat in addition to that the error overthe training data you had somethat place to do something in thefunctions farm itself for neuralnetworks the functions farm is just thewaves and you try to minimize the nahmastery vector that's a regularizationokay most networks do this so whenthere's a lot there easily trading it upplus a bunch of things that we have noback we throw that meanand this is all added to the last andthen the advantage is that everythingonce you tell you are back you know thenetwork specifier the last function itknows how to automatically do the backpropagation because it concludes them incomposes they are family pieces that'swhy it was very important to understandthat it would actually compose you knowget the gradient for the sigmoidfunction by just getting there lookingat pieces validator that's what givesyou the confidence that this is actuallydoable now is not just making it rightokay the one is necrolyzationthe other idea is that to berandomization during learning whichessentially says hey i just keep aroundall the hypotheses typically keepingaround this is in the right is extremelycostly so you kinda do it a little moreefficiently that's the idea drop out sotop bound is basically trying to do baseyou learning over the neural networksokay and if you wind up doing if youmight have no english and please we willalso talk about this idea of randomforests which is another idea that'sactually a useful and random policiesagain try to do very similar theimmigration trees not just keep onetension tree how much ablation trees andthat they've all worked on every newexample and take that empty wall if youdo that overfitting is not wrong theseare and say you heard lap out and youthought some of you probably heard aboutand this is something that only trueneural networks it is just an idea thatis connected to fundamental that machinelearning and a particular version of itas a clinician has been ok but you knowI can show you what record is in thenext slide but the important thing is tounderstand why do you need Java which isbecause really if somebody you know whois this guy I think I can think of thisthe baseball game you said when you cometo the fork take it okaythat's what we see in student when mostof us come to a fork in the roadwe neither take the left other right theECMC let's take the fork okay whichmeans keep our hypothesis with theirlikelihoodsit's extremely hard we don't do thatwell in fact humans basically jump toconclusions all the time that's whythey're bad learners okay but it turnsout that you can do that if you do thatif you do the Bayesian learningquestions then you reduce the voltageokayso change in the mass function as Ialready mentioned you wind up basicallyone idea that many very different ideasfor regularization one idea is loss isequal to everyone training data plusalpha times the norm of the vector alphais a true hyper parametersomebody it's not you know gradientdescent is not give me to be interestedit and you are minimizing this wholething is what I'm sayingand you have to understand that don'tmake any specific feature have to honeywait that's approximately what you aredoing by minimizing the norm of thevector we are ensuring that features donot start specializing a single datapoints that way lies partition that wayliesmemorizing the data rather thanthe patterns in the data okay so that'sthe that's the basic idea of did Namregularization there are other kinds ofloss function engineering can be done infact if I have time I point out to youthat remember in the extra part of thelast class I said adversarial examplesyou know a city bus so I guess it was toall of you but is 100% an ostrich foryour network and these and I was actinghaving conversation with other peopleafter the class examples don't justhappen by happenstance somebody designsthem and I was using example again mystar Star Wars example you know like pifighters don't just randomly come andheated in the only vulnerable part ofthe Death Star you need an cell phoneand his deliberation for the first threehours of the mood to do that adversarialexample this somebody else'sintelligence are trying to catch it sothese are musical examples for made andyou know how to make them so I just sendit up and mention it to you that here isthe way that we made you have taken somenetwork let's say that's being used forimage recognition and then you put theyou know the school bus picture to itsinputs and it's the school bus but youwant to say atomic standards say sayostrich say ostrich say ostrich sayostrich but you should do this withoutchanging the weights the weights arealready there which have been learnedduring the training instead what you'resaying is remember in the Mohammedtraining input is constant rates of thevariables how many of you numberimplicit differentiation from the olddaysso if the input applied to the functiongives the output you can either considerthat dates to be the variables are thepixel intensities to be the variables soimagine I keep the weights the same Istart fiddling with the pixelintensities until the network basicallyagrees with me that this is an ostrichthat's how a person examples are madethe only problem is you can fiddle withthe pixels so much that it actually isan ostrich everybody in which case it'snot a Brazil example you understand whatI'm sayingso what you need to do is menu fiddly sothe original image is a vector the fieldimage is a new vector you take thedifference you want to minimize the normof that ficif you do that nobody else can tell thatthis is not a school bus say everybodyelse thinks this is looks like a schoolbus so that noise it was Harrison Fordnoise it was carefully constructed toyour eyes it looks like noise your eyeseverything looks like mice this imagethis image which looks exactly like aschool bus and it's high certainty anostrich for your network now thequestion is how did you compute thisbasically you have to learn how tochange the pixel intensities you haveone old regional radio digituse the gradient descent to learn tochange the pixel values and you makesure that the two images are stilllooking like school buses to everybodyelse but insuring by adding a lastfunction and pop up this and about justagreeing with me that this should be anostrichyou will also ensure that the lastfunction is Kate syndrome the differencein images and minimizes the differencethis should be properly bending yourmind even more right because we saidgradient descent three classes back westarted doing expressions of chain rulewhich itself is actually far inaneabstraction from FX plus h minus FX by Hwhich tends to 0 and top of it we buildmulti-layer networks on top of it withlittle hex with all of this we just usesome you know science and just activatethe compose functions gradientcomputation the movie if we did that nowwe don't have to worry about what we areadding to the mass function because itwill get taken care of and I don't needto worry about where these parameterslike gamma and beta in the batch normare coming from because they too will belearnedokay so okay soso other types of mass engineering ispossible to an example is a no saleexample that I just shown youokay the other one is just do Bayesianlearning already they D be easyokay maybe not fully variance areplanning more Bayesian than whatokay and so I just keep more off type orthis is around an idea would then be tosay instead of learning a single networklet's say you learn an exponentialnumber of networks at the same time onthe data they all predict the data onthe test data they all try to minimizetheir tester training ever but they'renot in you know and different times andso basically they are learned aredifferent lecture parts of their inputsthat means if you have let's say tenmillion exampleshe put for example you some of theexamples will just train one networksome other examples okay the othernetwork that's an important now we havetwo networks and then you can make thembe a County this is the way you wind upgetting be using dropout does this andthat does this in a very clever way whatit does is what it does is it how do notwork which is this is a network that Ihave every time basically when gradientdescent is happening you will haveprompted some ways other weeks ingeneral you update all the waves butthat's of readiness animals what reportsays is that every epoch you randomlyact as if some of these weights don'texistthose connections not exist so itbecomes like this randomly every a poopif you get random network why am I justbasically say you toss a coin and justdecide whether or not this actuallyexists if the coin says yes theconnection exists you keep thebackground if it says no oh you don'texist you don't exist I don't haveupdate you if I did not take you thatmeans I need to learn your value on thispart of the circuit if you that isstill answers