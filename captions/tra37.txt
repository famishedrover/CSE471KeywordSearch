last time I mentioned here in particularyou notice that if you are computingjust is particularly fast you have everhere using that ever you need to changethese babies okayand in doing so basically actually thatthe way you wind up doing is that thesummaries or the no basically if you'retalking about the ever here again I'mtaking not our Center villas but ratherthe squared loss currently if you keepthinking okay the T V is given in fivesquare glass and you're askingeverything is you saying you think thatworks is really cross-entropy last theone after this I will tell you thatnobody actually does the differentiationthis is just to show that it can be doneonce okay will show that that's thepoint about how to deal it turns outthat everything essentially is doneautomatically in the backward pass inthe current systems like answer for thatyou have okay they essentially do thisgradient propagation themselves and soyou can put pretty much any analyticallyis fine function for which you know thedirected the different shapes and thenthey will can compose the differentialscorrect I'll get to that in a minute sowhat now just see that if you arethinking only in terms of the meansquared error you will still see that ifyou go through the steps the weightupdate is the input size for the weightupdate for the j23 is the input LJnotice that the AJ is input for itbecause this guy is only getting inputsfrom the interior nodes the nice thingis for this it but the example that youare looking at you know what the rangesare because given the reach you know thewhat the edges are the people see what Ijust said right because I will put somenumbers here and there are alwaysbecause of these weights I know thenumbers in the next layer because ofand because of those weights and get thelast number say everything you actuallyhumorous even though we're talking interms of you know minions but thesenumerals are changing this particularvalues are changing step-by-step effectthat's the thing that you want to getyour mind around and the real change isthe change in proportional to the inputand the gradient which is pretty muchwhat we were talking about earlier wherethe gradient here WI is actually justwe've said that this part has been saidto be never so you threw in the G - Iinnings and you call that Delta and thenwe should go through the steps for thenext one you continue doing the samething in terms of chain rule applicationthen after some more change allapplications you will get to a pointwhere you see that the way to change wkjthe beach here is going to be in termsof using the input for it is a kidthanks againdiscreetly IntelliJ which is coming fromthe which is coming fromthese parts it's essentially by thistime notice that they are specificallyasking yourself what is the errorsupposed to be in the resilient layerthe way psychologists would askthemselves okay the interesting thingalso is that essentially this Delta Iwas the original error in the firstlevel approximately and so what you didis you're multiplying it with the weightwhich is essentially apportioning theheaven proportional to range which isexactly what you say exciton is removingokay now so essentially the ingredientsbasicallylook like the input times that the errorterm summers neglecting four times Iclick OK and the gradients are beingcomputed step by step and two things nowwhich I want to just reduce let'stesticles to get people to kind of havethese I would start as you guys look atthis particular page because this goesthrough excruciating detail doing theforward and backward steps for onenetwork in particular I'll just walk youthrough this so that you will again gothrough this I'm not going to say thenumbers are loud but so they take thisnetwork okay and they will initializethe ways to something because you haveto start with some additional weightsthat's what happens on the time okay andthen they will also start with someinput this particular case that input is0.5 1.10.10 the true outputs aresupposed to be so if you put this point5.1 I believe it should work throughthese and in fact these networks thatthe makes the UNIX always are all havingsigmoid functions and so in the forwardpass they will tell you how theycomputed the activation value coming inand then they will throw that into thesigmoid function to get the activationcoming out okay and then they do allthese steps this is basically whatcomputers are good at and you areextremely married but you want to do itonce just to know that there is no magicand after that you just depend on typesof flow okay okay so then you continueand then you are computing the outputfor 101 which turned out to be 0.75 butremember itsupposed to be point over so there's anerror okay and similarly you come to infact this is a case where this is amulti-layer multi output Network you cando any of this item you can do whateverand there are multiple outputs that youtry to minimize the error over alloutputs at the same okay so you computethe value for this guy oh to do what whois supposed to be point line after he hegoes through all these numbers he findsthat it actually is coming into twentyseven seven and then he computes theerror and that takes that and thenessentially passes through that throughusing the chain rule and using thedifferentials step by step by step okayand and then if you have to go throughthe whole thing and so basically a forexample which is putting together onevideo in terms of the multiplication ofthe chain of the ratings this stuff youwill understand because you know okayand then you continue you know and atsome point of time at the end of sobasically you go through all the layersokay at some point of time you updatedall the waves and so when you havefigured out the waves then you will findthat the ever which used to be point twonine eight three seven one one zero nineis now only point nine one zero twoseven nine two four is your hex a easy Imissed in my whole day you're notsupposed to do this computers arementally this is what I'm saying youshould do it once in your life just toknow that you see what is done but youare not supposed to be doing this doingthis for neural networks is like doingdifferentialactually computing the limit H tends to0 10 X plus h minus F H by H you shouldknow that's that if everything elsefails that's a way to do it if you areon desert and you have no that's a flawand you should recognize a cat you canvery quickly do this by hand okaythat's basically what this is for onesmooth okay now thing so I'm sorry thatyou go through this walking you throughthis step by step basically photographerthat stuff us to sleep how it is somepeople ask me there are some othersectors but let's not do thatso I reduces so detail this with onlyone two three four five six seven eightnine ten weeks just ten beams it's a 10dimensional optimization problem youknow GPT to people may have heard ofthis opening is the potato it has 1billion ways 1 billion ways and allthese 1 billion waves are being updatedlittle little little little little onesand all your hair is that the theoryworks and somehow you will become thiswhat I am saying and because this isbasically gradient descent inmulti-dimensional space okand in general it's kind of hard tobelieve that these things work at alland you know I think I find that it'suseful to sort of understand that reallyreally small steps as long as they'reapproximately the right directionstatistically can't need to prettypressing outcomes how many of you out ofBarrier Reef okay right I mean you haveheard of coral reefs in general thecoral reefs are found so Disney goesslowly builds follow reefs coral reefsare essentially the food of this littleaquatic creature that's like you can'teven see the plaque right and you knowgiven deep time gets you Barrier Reefenough given enough number of updatescan get you a neural network thatactually converges to something is notthe only thing waiting for neuralnetworks as agonist that is is we canspeed up time for neural networks weunfortunately cannot speed up buying forvariable the stuff that we people haveyou know store it is down by destroyingrightso that's really the peak so these aresmall steps as longer than thestatistically light direction and Ithink that's like bothersome aboutanybody who sees they understand theypretty much understand by the gradientdescent works probably don't understandwhat they're talking aboutokay you know because it can only beunderstood as a level of abstractionwith the background in statistics oneparticular you remember we actuallymentioned let me see if I can you knowbut in particular it turns out that youcan compute the thousand input exampleson their labels that's a training data Ican compute the error for all all thethousand examples to this networkcompute the error for each of theexamples somehow those errors minimizethat so in one fell swoop we wereessentially doing fully gradient descentand the full error not believe whenpeople think of gradient descent beforeneural networks that's what you talkabout you do all your training data youthrow them and you find the output youcompute the full training error so whenI was showing you those steps of flowvideos the last that they were showingthat's the full training error and thefield test ever so one way of doing thiswould be to pursue the full trainingerror in each step modify the wavesthrough them you know it puts the entiretraining data through the network againwe update the mix modify the steps weupdate the rates that's one way theother extreme way is for each exampleupdate arrays which is what we shownherethe first one is called batch ourNamibian desert the last one is calledstochastic gradient descent inparticular if you have data that ismislabeled like for example in the endsome of you should be getting it plusmight be getting T - just an error atthis Safari and you know to myselfnothing but but and somehow if youshould have gotten V Plus might get aplus these are errors in the trainingdata this you're insane and in thisthese errors if I give those examplesthose long examples change to change theweeks then at that particular points theweights are actually gone in the wrongdirection do you understand what I justsaidso if I took I mean I don't know me someof the grades were actually long leap init they were that's not what I meant youas the teacher I gave the round rate sothe training examples can be noisy inthere Vegas and if you just take theindividual examples and pass themthrough the change maybe I'll believethat one direction because the network'ssay so this kind of Bozo should begetting a plus that's not what I meantthat's right by mistake that particulargoes I got a class okay but if I tookall the class give me that I still havea job it must mean that mostly I do theright thing in the entire class Ibasically mostly the right people getright rates but there'd be some eyes sothis kind of noisiness would be reducedif you took the entire batch of trainingdata and pushed it to the networkcomputed the error and use that tochange the waves the people see thisright so you would think which one doyou think would be a better idea interms of conversion if you just don'thave you care anything else which roomyou ever thought would be a better idea- knock knock - nah don't say the wordback now it's it that's guaranteeddecentthat's what do you say that word but youknow that's gradient descent weather apretty decent has nothing to do patchnow we have a word - come on just likecome on okay so you would think that weshould do that very indecent originallynormal what people did not do batchgradient descent because if you have 10million training examples they neveractually have invented they havethousand training examples with all thiscomputing one pass was taking too longand you feel as if you know it's nicerto make more facets and hope for thebestso they would actually try to moveinstead the thousand will be split intosmall batches and another hundred andthen through each hundred through thenetwork compute the full error for thatbatch that milli - and they use that tochange the rapes you see what I'm sayingthat would become really batch gradientdescent and the most laziest we match isevery example I just throw it to thegreen circle that's the stochasticgradient descent now let me ask thefollowing question when we did when wegot into the sordid affairs event fromreinforcement learning right to learnthe value functions then what would havemade best sense the batch gradientdescent we leave as gradient descent ofthe stochastic gradient ascent becauseyou can find online gradient descent andthen it becomes nice you see because ifyou're only getting one example fordinner and you still have to learn youcan't say I'm very taken of those of yousay let me say it until PhD is over andthen I'll start answering a champs it isa good thing to do because after the endof PhD you'll get some idea how toanswer any exam so then you quickly gothrough like first grade second gradethird grade fourbut we are forcing you to take thephosphate exam in fasting you see whatI'm saying this because we are sayingit's an online what we basically willget you data one at a time and then youjust have learned from that data so thisis so the stochastic gradient descentwhen seen as online gradient descent isa great idea but you also know thatstochastic gradient descent will bemaking moves in arbitrary directionseven though it's supposed to go in aparticular direction so if in fact theright direction towards the minimum islet's say this stochastic gradientdescent will be going all over the placebut even channel towards the rightdirection statistically speaking towardsthe right direction so if anybody says Iunderstand this is what it meant theyhave because in particularimagine that particular example that Ishowed you if in fact this example wasprobably never then you just change theways that one direction so the ever maybe good for this example but on theentire batch it would be in height buton the whole if you do this stochasticgradient descent to the Stillwater okayyesagain it's a question offthough there is no in general and thereis no others we are an engineering worldfrom now onwards like you can understandthat want ages of each other sometimesyou have to do online sometimes you maywant to that because of which you can dosarcastic very decent but in generalactually there are some interestingtheories the results people thought thatthey are doing as GD because they'reforced to do it that the right thingwould have been to do the batch gradientdescent but there is enough work so thatshows that when you can do both doingstochastic gradient is it actually worksfine in fact better for very highdimensional problems is there a theorynot so much ok ok so if you like thatthing then the other thing you can doI mean that was actually a useful thingthis is also a useful maybe for 40minutes of anybody talking videos I meanyou know I hope that we can begin outthat even three blue one wrong guy hasmade them and it's very useful you justwatch it actually have nice animationsand stuff this stuff obviously doesn'thave animations you have to press presswhereas this looks as if all the numbersare being done automatically now if youdo this and you don't even need thisessentially understanding how anythingworks is for the words really when youget into the when you get into thecompany you just have to ask where isthe switch I put on the switch and thenthings willokay I'm so really the propagation isdone most of the time is this art ofdifferentiation example to see whereapparently yeah so the idea is tobasically say that you can compute theaverage numerically that the gradientnumerically just step by step by stepokay and this is what they are good atas computer scientists so I want to showfirst of all I want to basically makethe point that a neural networkunderstand this these are as I pointedout these are those capacities slavesand so this part is managing the neuralnet is just a huge composed functionright of inputs so mm up in force isgiven minus 1 is minus 2xf one youremember the composition of functionschain rule is made for compositionalmarkets this is what I'm sayingsitting there and trying to think up howto deal with an oppositional function ispainful programs are extremely good atdoing that stuff it is our Authority andif you are just done in particular ifthe subroutines have very regularstructure in fact this is the only placecomputer science is actually have briefessentially the programming team werehappy because what happens is that youcan compute any arbitrary differentialgroup for the automatically okin particular let's restate thatbasically is taking one function whichitself is a composition of manyfunctions and then you see how it worksthat you realize that that's howbasically that propagation path works inneural networks so they have one forwardpath and then in fact you don't evenhappen but you give the backward passthey will know what whothe backward pass and so now Matthewactually this is what I was saying thereason I school students can do neuralnetworks even before even knowing whatis calculusokay so anyway in this particular caseright so I'm considering let me just gothrough this quickly and to see and youcan go through this for me in particularthis is available also a written versionof this is available in this set ofnotes that unless a body has and itbasically goes through the example thatI have okay so I will go through it oncejust to get you bendings and then youcan spend more time to be a rate so allwe are say is this big function thisfunction which looks like a big functionamateur circus malfunction to us canreally be retained in terms of this flowgraph this is called a computation overthat right so you basically ting thisfunction what you're doing is you'restarting with the waves and then thereare the inputs in green and then theyare being multiplied so that the inputsand weights are being multiplied andthen when you get something here addedso here what's happening is the insideof this is being computed by this 5 ee 0X 0 plus everyone X 1 plus W ok and thenat that point essentially you thereforeyou're adding a 1 to that and thenyou're multiplying you have multiplythis whole thing by minus 1 then you areputting an exponent so what is this isyou send me any good I will take the epower of it and then send it out andwhat this guy says is you give meanything I put a minus 1 multiplied andset it up right these are computerfunctions is how you would actuallywrite the function if youlike it as a program step by step bystep right and I am so then theyexplained and then then you add a plusone and then you put the whole thing ortwo on OLX so this one to this you sendme anything I think one of our Xoperator has anything else the nicething about this writing is it as Ialready down the chain room for youright because what is the X here the Xhere is this whole thing so the X hereis this whole thing it's not X 0 X onceright and so basically that's youunderstand this this this is you knowthese are stuff that maybe people modelin programming would help understandingbut this is what you have done now yourlife is the computational graph and tocompute the function what you rapidlygrow is if you through this thing andpush these things through and you getthe ready that value is fine 7/3that's an agreed that's the forward passthrough this graph to compute thegradient you go through the same graphin the backward direction now whenyou're going in the backward directionthe thing that you need to do is youneed to take for every place you need totake the local gradient multiply andthen combine it with somehow with thegradient coming down coming back so theinputs go forward direction thegradients go backward direction rightnow the only toe situation of course ishere when I come to one over X I need toknow what is the gradient for 1 over Xthat will attack you know and that's theminimum math you need to know even thatyou don't need to know actually thereare lookup tables right so how manypeople remember that if f of X is 1 overXis minus 1 over x squared gosh you guysstill the moment so here's the thing soyou start here in the beginning the everbasically be after the error is e andthe differentiate itself with respectitself that you need is 1 DF by D F is 1then you pass that through 1 over Xwhere X is 170 and then you get fromsorry then you get minus 1 over xsquared here this number this numberhere let's actually go through thisdivision so you have minus 1 over xsquared is what you have here that willbecome 1.37 so one point we serve is theinput that's coming through this one xminus 1 over X 3 will become minus 0.53okay and that is that the next stop isthe gravy and the differentials that aremaking their way back ok and ok and thenI'm gonna come here I have thepropagated period is minus 0.5 3 andthen I'm adding a plus 1 to something sothat's like basically saying thepungency plus X constant plus X and ifyou differentiate you get just 1 whichmeans you multiply whatever is here byone and put it there right that's minus1 by 3 then I have E power X for E powerX we know that E power X differential inthe power X ok so then I will basicallytake minus 0.5 3 I multiplied by E powerX and that will be e power minus 1but the X is the minus one okay anypower minus 1 values this - point 5 3that's minus 1 is that ok I'm just goingstep by step here and I continue hereI'm multiplying it by minus 1 if you Iwill multiply anything by minus 1 thepower D power yeah minus X by DX isminus 1 so this minus 2 point 0 shouldbecome plus twice it here the gradientis be computed without you touch itbecause it's just going through the twolittle steps and computed but it's notbe computed in the analytical fashion itis be computed in the numerical fashionyou just need so in the example that Ishowed earlier that they actually hadthe analytical expression for thegradient and he substituted the valueshere you are just basically getting thevalues by composing the dividualdifferentials so insane and the onlyneutral differentially needed are theones of the small analytical functionsthat you are using pretty much the verybig thing by the way the reason I'mtaking these particular function isbecause there is sigmoid function it'slike one of the biggest things that isactually there in the neural networkthere's nothing really going on in yournetworks other than very experimentapplied by the imports adding them upand throwing them through the sigmoidfunctions except you do them like abillion times and so it looks like apretty complicated expression that Swaleprogramming is good that's exactly whereprogramming is good you see what I'msaying ok so you go through this map isreading is here is a blastand these are actually fat faces whenour differentiation was an area lookingfor an application until your networkscame along because people have taken ouroperative order differentiation is thenumerical analysis they used to be thisarea our numerical analysis people neverused to care that much about it now theyare extremely popular becauseessentially meaning of these pizza thoseare the guys who used to care aboutcontinuous optimization those are theguys used to care aboutafter differentiation so in general oneof the things that happens is you cameup to here up until now the gradient isjust flowing through because there is alinear path here you have a fork in theroadso what do you do it is I who is thecassette when you capture the fourthtake it from baseball guy anyway so inthis particular case it turns out thatwhat you do depends on the operationbeing done the kinds of operations thatwill be done in neural networks acrossmultiple Asian Maxine if you are doingmax fully we have yet one there but someof you heard of max those are the kindsof operations and he turns out