and whether it's going to work yeah it'snot making people so how many peoplethink this proofhow many yeah you're actually saying towrite about this okay how do we go thenit's falseokay somebody who says it's false tellme why they think it's falseyes yeah any of this question by the waythat said these will be are doing bigthings but any of these questions that Iasked you can answer without formulasI'm not important but ask what isactually happening is just as importantokay so your answer is that there'ssomething about 1 by 1 minus beta andthat has nothing to do with number ofiterations so did you think it was avacationso think of the two Cosmo cases whereare these kinds of things happen pick upthe two extreme cases it's not factorone it's not factor zero the to do youthink we need more iterations before themessage comes to you remember there is asolution of message the value iterationis all about the message from the faraway relative whose reach coming to youthis you're insaneyou can't stop before the messagereaches because then you will get longanswer so if it's not not equal to 1what happens in essence you have to letthe thing play out because even if thereis somebody like 2,000 actions away fromyou who is rich you have to wait for twothousand of days and then that value thecompany right yes yeah it basicallydelegate about neighbors because knowthat I want to die yeah Mike what isbeta equal to 0 means you're payingright now so right so this will now bemessage yes but they're being attenuatedbecause there is future so if you'regoing to dietoday 25 hours tomorrow in what 0 foryou doing this you're insaneso if the discount factor is low that isequal to 0 the value iteration will havehow many iterations one equationdecision linear equations if we justupdated if you said it to me the rewardvalue itself then it doesn't actuallyavoid any further because if you do onemore iteration finds that the valuehasn't changed and it stops okay here isthe question so having discount factorclose to zero is being extremelypessimistic about future you agree wetalked about a sim ism and how it makescomputation is he in Star Search set theheuristic to infinity we have theheuristic to be infinity then you'll bedone by criterion it is a go solution nosolution you see what I'm saying theysee me some out it makes life shockedthe computation start so same thing heregamma equal to 0 is - ism there is nofuture there is no future you won't needto think much about value iterationsnumber of iterations is our thinkingcost you can just see the rectangle andball because there's no future if noteatingthis so playing with gamma is actuallyin fact one of the interesting things isthat I kind of said this yeah remember Iactually did mention this I also I putit in the words that this term factorcan be actually interpreted in twodifferent ways and I use bothinterpretations in giving you examplesone of course is that are in steps awayis what beta I forty times our other wayof interpreting beta is it's theprobability it is related to theprobability that the game will end rightnow gave max names like X that meanshorizonit could be step in that point so unlessit doesn't have to be uniformly so foryou you are rising over your time is upso if beta is 0 there's a probabilitythat your 5 is ending is 1 minus 0probability 1 that means basically youlike the game is over so beta is 0 isbeing extremely pessimistic about futureokokay anything else on these sodiscovered MVPs we basically talkedabout how to do that this comes in factthat some of the infinitely worse willbecome finite and one of the interestingquestions of course is why the valuefunction converges in the applet as yousaw the telephone is converging butreally the reason that we know that whena function converges determine if wewith even saying the Belmont update is acontraction operator I'd like to giveyou a sketch here and I'll give youlonger so basically it takes any twovectors if you have quite a marketoperator then they come closer and ifyou think one of think of one of thosevectors may be star then in a senseevery other factor is being made closerand closer to this time because if youtake this star and apply them on updatewe start being the optimal valuefunction then by definition when anupdate should give you the star backthat's the optimal value so as you keepapplying applying governor update 2million B star this website remains andme star this our guy is coming closerand closer and closer to be stopped andin fact the interesting thing is itcomes closer by a factor gamma beta bythe way this other thing I actually tendto think of this contract was gamma inthis slice it's released as beta butpeople use either up there okay so it'sactually related to that is compatibleso in particular you can actually showthat VV minus- is is less than or equal to beta timesV - V - and whenever I say - for avector it's all about mounds we areessentially taking the maximum betweenthe two vectors the maximum differencebetween the providers and if you drivedown the maximum difference to zero thenthey become the same and every time youare especially reducing the distance bydata factor and since beta is less than1as long as when I strictly less than 1you know that will come - how manyequationsthat's not guaranteed in fact valueiterationthere are no bounds as to how many timeswell relation has been rated before itpurely converges which is why actuallypolicy expression is interesting becausein the case of quasi iteration you guyshave also read up on the information inthe textbook we talked a little bit inthe case of oscillation at least thereonly a power s number of pollen I meanas far a number of policies number ofstates power number of actions so if youjust keep track of duplicates you can atleast know how much to look at the samewants you more than us there are thefunctions of real function air vectorsreal valued vectors the incident numberof them ok ok soso then they're in just like so inaddition to let's let's get back to thepositive evaluation okay first let'stalk about the policy evaluation so eventhough we're actually interested incomputing the optimal policy which isthe value iteration you also want tounderstand that giving a policy PI youcan evaluate it remember that's exactlywhat we need for finite or is it firstwe evaluate the policy then we didoptimal policy okaynow the interesting thing here is in theand I said that in the finite horizoncase evaluating the policy essentiallyinvolves having a set of simultaneousequations a very simple kind when someequations are essentially saying X equalto 1 y equal to X at where it will besaying X plus Z equal to 3 since youalready know the value of x in the caseour form in the case of federalismessentially you just solve this part tothis particular set of equations sovalue with respect to policy PI of somestate s is of course the reward for thatstate X plus beta times the probabilityif you know the value that the falsifytells you what action to do in s let'sfound that Phi of X makes it a so if youdo action a in state s you will get tostate s - it's our problemand that - has a value with respect tothis policy so there are s number ofstates from there s number of variableseach for the value of each other statesand you get a set of simultaneousequations in in s simultaneous equationsin S announced and it is powerin fact computing you know something ishow many of the equations essentially ismatrix inversion even though that's notthe main ingredient when you read lasttime around minerals which are yoghurthousings Hamilton's equation is justmass matrix inversion matrix inversionit is all momentum equation okay sothat's computing the value as a policyso computing optimal policy youbasically don't get this nice thing hereyou essentially had a simple linearequation it's very important tounderstand that underneath that if youactually plug in the number of therejustice a community relations with thevariables will be attraction right okaynow if you're doing it that optimalvalue function then what you need to dois basically this is the true bellmanequationit just says the son of s is our affairsand this part is the same except foreach action in you have to compute thissong but each action is that you do whatis the expected value from the neighborsthat you are getting and you take an axeall our actions we didn't happen to themax before because the policy told youwhich action to know and that's the onlyaction you do here the policy is inthere so you have to consider everyaction that you can take and you thenwind up saying I take the one that thebest it's easy enough to say except allof these are unknowns we start up and hestays so now you have s unknowns ssimultaneous equations except is theequation of a linear nonlinear becausethere's a max operation sitting rightokay and the value iteration essentiallyis just thinking of this and it's toughseeing equal its nest basically lookingit as an assignment and makingintegrations so let's start byinitializing all the values to therewards that would mean zero that's thefirst iteration and in the seconddepressioneverybody essentially uses thiscomplication to update their balance insome use here as an equality you justuse it as an update okay and you keepdoing this every time you do this youhave changed your value once more andafter some number of times of doing itwith your value hasn't changed betweentwo iterations then your value hasconverged and of course then the valuefunction is a vector overs s States andyou would say that the value functionitself has converged if they need twoiterations the maximum differencebetween that two value functions in thetwo iterations is less than Epsilon okaythat's what basically mountainexpedition okay so this max operatormakes the system a linear so the problemis more difficult ever call this policyevaluation and then the optimal valuefunction is a fixed point of this Delmonbackup which is this particular thing weused as an update okay very topicessentially is this bellman update of avalue function V essentially does thisby looking at its neighbors language sothat's one iteration of alice onvaluation so this will be a new valuefunction and you're essentially sayingthe value function that you get by doingBwe would be closer to restart than theoriginal value function and one of theinteresting things is this is to aboutsix point computations really doesn'tmatter what you initialize the valuefunction game I kind of said let's do itthe way we need for a finite horizonvalue iteration which is what functionvalues right but I didn't know zeros andif you think for three seconds yourealize that if you start with all zerosafter one hydration they've all becomethe water you see this right because thesecond part is zero the first part isour offense so our politicians becomeadvocates this is also the place whereif your rich uncle says but I havesomething for you here are four numbersto visit and see if you can reduce thenumber of equations so I thought is if Ican give you an approximation if I cangive you an approximation to B starthat's better than zero RRSheuristic that's exactly what if youwant to understand a star search as amicronation process the heuristic is theinitialization of the value functionokay so that's another way you canimprove the speed of the number ofiterations that you have to do one lastthing is if your include jumbo computerfinds and if I asked you is the valuecomputation value equation an optimalquasi computation is it exponential orpolynomial is it tractable AMA form ourcorrelation processes that's areasonable question to ask you know thatsomething is practical it's n log nalgorithms are there so you valuecomputation tractable or not actually Ithink until now whatever I said doesn'ttell me that is tractable a car becausevalue iteration number of iteration ofthat value iteration will go to if youdo it as an object is different youdon't know exactly how long it mean thatin your doing before the value is drivendown less than it's not if you put somebounds but they are not very strongbombs and if you think in terms ofpolicy iteration it's the next volunteerbecause you do that brute-force searchare looking at each policy and compareme to the next policy but which is oneway of doing it right because now I knowhow to evaluate a policy using this andthere only s power a number of policiesI throw in each of the policies it is inequation I will get the D Phi for that Icompare them take them optimal againunderstand the stump way of doing it sothat they can appreciate the better wayof doing what you doing this clearlyit's like in factorial wave form youknow something and it still tells youthat it's unattractive it not only theway you show that something is tractableis what you actually show another andsay look is having them solves it and ithas polynomial FX it is for the wrongsideI'm giving you half an algorithm that isfollowing advice like I'm hoping that Iteach you what mostly I never gave youit turns out that actually you couldtake these and like these directly as anas a linear programming problem I meanhave you heard of the new program okayso linear programming essentially is isactually one of the reasons why we takeon this idea Dominick program is if youhave a set of linear constraint and aset of variables and you have somelinear objective of those variables thenwhat should be the values so for exampleif I have if I have X 1 plus 3 X 2 lessthan or equal to 5 X 2 minus 5 X 3 lessthan equal to 7 etcetera these kinds ofconstraints and I want to say maximize5 X 4 X + 7 X 3 minus 2 X 3 that'slinear programming power have you everseen this in your life in fact I knowfor sure that those of you went toschools here you have seen it at thetime when you saw finite mathematics inyour middle school and our beginningafter high school without having seenthis you haven't gotten into castles okturns out that the linear programmingproblem is actually polynomial timesolvablethat in itself is actually a huge bigsecret for the longest time peopledidn't know whether linear programmingis tractable or not until a guy calledTom okkermore than tomahtosure idea car interior point out earlyand sure that is for problems that thecomplexity that we are solving linearprogramming is actually polynomial sonow we know that linear programming isfor Nambiar context and I just told youthat you can compute optimal value as alinear program and so you have a way ofproving that in the corner data modelnobody actually uses that particularmodel other than ensure that it'spolynomial time because you don't noticethat they can be huge constants and so along time complexity might actually be Ithink my accent was polynomial time butit's worth remembering the optimalpolicy computation is polynomial in thenumber of states in the number of stagefor modulation process that's the lastwe will have that luxuryonce you go to parties partiallyobservable operating policies plasma itbecomes exponential for finite horizonand undecidable foreign languageokay so let's just see if I can tell youwhat this is basically what I was sayingabout the convergence proof I mustmention this multiple times but I wantto show you some intuition as to why itis actually what's out by a big it youknow the bell Munnabhai drives down thedistance between the two vectors and sothis is the minimum you can understandand then maybe I'm you know watchingthrough a lambda proof in both side ofthe class I know one of the videos soessentially if you look at this we areme Deborah update on me but say s isthis we have V - is this subtract onefrom the otherwe have F of s is this yes if you do nothave a polynomial time out there imagenot right and so and now what I do is Isay this is less than this first magicnow that to understand that you onlyneed to understand this general propertyof any function f function if you haveany function f and G the max of F - maxof G or out is that means every pointthat's another point in the range of thefunction of the domain of the functionokay so here is F on genus G this is maxof F over all X's here where is max of Gover all axis here if you subtract thisfrom this you get that muchyes this can be shown to be less than orequal to the maximum difference anygiven point between both departments inthis carefully capturing example you canactually see that right because this isthe maximum difference between so sothis is max max more carefully we needthese two photosystems for the samevalue X this is G of X this is f of X soI'm just saying this is less than orequal to this I use that to essentiallysay this essentially you have functionsin a more complex form and once I dothat from that and the other fact acouple of facts one of which is thiswhich is for any function in fact youcan think of that as functions I thoughtin terms of B is but if ending of anyfunction f and g f of X minus G of X isless than or equal to max of f of Xminus G of X always right so basicallyI'm looking for any X I'm taking thevalue between F and G and the differentwe didn't apparently did will have to beless than a maximum different page I cannever that's pretty obvious by the waymany of these theorems go like this theywill take pretty obvious things likesome subset superset connection Iactually made a lot of comments on thea-star search nodes that subset supersetand how the minima change over them isconnected to the a-star search too andthat's what we're doing is as another ofthose enumerator properties another ofthe simple properties you are using hereto use that and the other fact that younotice is that this is a probabilitydistribution ESS that is a probabilitydistribution so if you but all over forany given a PSAs - over all this - willhave to sum to oneso if I say what is TSN as - times kwhere I'm going over all of Hollister Kis a constant if we just if you see thisright I'm multiplying here by wholebunch of numbers which add up to 1and I'm summing them up fine get K backyou use that those two then you get thenext part which is which basically meansevery time you do a hot day V and V dashare coming closer and closer now if youthink of one of those V - to be D starthen be our B star is just be stoppedthat means every time you do have anupdate on me it's coming closer andcloser to the star and this doesn'tmatter has to make you started with mebeautiful proof this is myinitialization doesn't matter okay sothat's the true state of contractionproperty and then if I want to computeusing value iteration the the optimalvalue function then I need to thenconvert into the policy right how do Ido the same thing that I did formonetarismso if I know V star I can always convertit into the actual I'm supposed to do soif I know if I'm in state has - I'mand a bunch of neighbors around me and Iknow all of their optimal values thenfor each action I can take I can computewhat is the expected backup for thataction and I will take the action thathas the maximum expected back you seewhat I'm saying I said this for finiteOregon and I also said for a finiteilysm that you may as well remember ateach stage which action was it that gaveme the max and that way you don't have aleave of you this is you're insane youcould do that too here in fact there isa different way of doing this year ifyou don't want toafter getting the optimal value if youdon't want to do this for each actionthe property that the action takes - s -times V star occurs - and expected valuefor it and then compare are thosenumbers you don't do that instead ofactually computing the optimal valuefunction you could compute what is Godthat is the optimal q function and it sohappens that V star of s is max over Hof Q star s so instead of justremembering one value number for each thI will remember these from this state Ido action a then what is the bestlong-term outcome I can get this is howthe Q value with respect to this actualgame so instead of tracking one valuefunction one map of our state you arenow tracking a times ss times in numbers but for each state Ineed to know that my defensive actionsand if you do that at the end you don'tneed to do the additional computationyou will right away now max Q valuetells you the action is the maximumvalue is the optimal action for thatstate now this is actually some of youmay have heard of an idea called Qlearning how many have heard of Q okayand the fact that alpha go use a fewnetworks a statue and we will talk aboutyou learning in