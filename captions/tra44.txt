that has been been applied this G whichis the thresholding function that willgive up what the perceptron contains theanswer you subtract that from Team yousquare it that's what you need any helpand you try to minimize this withrespect to W J's the device the wheelsso now the WI is inside of G which isinside of a square right so if you wantto chain 2 now your root three timesfirst you do the differential of thesquare that be 2 times whatever is inthe square right there you do thedifferential of the G which happens tobe your you know simple sigmoid functionand I told you that you can actually doit from first build suppose what ithappens to be G of X G dash of X is G ofx times 1 minus G of X so unless youthat G dash can be done and then Ifinally do the definition of this linearfoundation but only with respect to theway that I am differentiating and justas before only term that will survivewould be the one that WJ is multiplyingwhich would be the feature J after youput item right so after the three stepsessentially you will say the the TD -Cole is coming because it is it is theinside of a square okay and you have Gdash because you have you know Jeep hasto be differentiated and then the insideof this is differentiated you get RJbut you are the truethis looks almost like before it's likealpha is the gradient we accept thegradient is not just the errorit's the error thymosin I meanpreviously for just the error times thatCJ barely remember because G - happenedto beit's an identity function Mashiach isactually has a value util that you haveessentially a dynamically changing alphaso alpha times G - is the new alphawhich actually changes based on whereyou are this is called a perceptronlinear perceptron update rule and thisis a part of this sigmoid perceptron oftable but you notice that the updaterule looks very similar there's almost astructure to ityou have the error you have the featureif it's linear as long as it's linearsummation which is what perceptrons dookay and then this is the learning rateand then make the G - but this is theoriginal reason for craziness aboutneural networks back when I was born in1960s when the guy called FrankRosenblatt figured out that these typesare for supply units can be used tolearn lots and lots of functions nottransferred each second of the bunch ofthings can be done and he was very happyuntil couple of other people basicallythrew cold water came and said look itwould not be enough to learn howfunctions it's only enough actually tolearn only the classifying problemswhere you can draw a line a hyperplanebetween the positives and the negativesit should not be surprising to youbecause what it is doingfinds that the final dates essentiallysaying the wi x i i Sigma greater thansome threshold if it is there isn't achurch onyx yes otherwise known thatdefines the hyperplane in the J damagedin the in the space that is of the samedimensions at the ego okay so this islinear perceptron nee-sama started fromreinforcement learning online learningwe can bring in a perceptron I want toend by pointing out this MIDI danceslide that you get to read butessentially out of most of the thingsare pretty obvious here that if you aretrying to learn function ask and you aretrying to predict F dash F hat okaythen you want to compute the error andthen minimize the error you get todefine the error in any image way youcould have defined in an absolute valueare the squared of 4 etc I am dependingon the way you define the error thedifferential which is the weight updaterule will change the aggregate errorfunction that you define is called aloss function which is the name of ascope the parameters WI are changed inthe direction opposite to the radiuswhen we need is computed from the firstprinciples for now at least fortolerance and the gradient except as wesaw depends on the form of the lastfunction easy J square loss function andthe form of the treshold function theother three was I also said for the DRLthat it also the linear function I don'thave to say that here because parserclass only to Linnaeusnation of their boots each particle unitis a linear summation of the involveslinear matrix sum of the reverse thebeauty of our circumstance if you stackthem such that these guys goes tosomebody else's in program that's whenwe get Martin your multi-layer neuralnetworks are so coming back to this thesimplest loss function that we do pairis f hat minus F squared that's thesquare mass function but there are manyother loss functions in particular inthe case of classification you tend tolook as an artistic regression forexample can be seen a message withalready the probability that this isplus a versus plus B which means you canthink of the perceptron output as aprobability distribution the truedistribution is known because you knowwhether it is actually one of them soyou then have to compare twodistributions so in fact in generalwhenever you're doing errors you aretaking the distance between two vectorsthis is the L 2 norm distance betweenletters the squared error to that thelongest distance absolute error willcome F 1 and exits between directors fordistributions for probabilitydistributions which are also vectors inaddition to these FK norms you can alsostart thinking about entropy functionsand in particular there is thiscrazy-looking function called the crossentropy which is the true value the truedistribution times mod of the value thatyou actually computed and similarlythroughsecond distribution second value interms of the to distribution in the lotof you will go through this again but Ijust want you to understand that this ishow the car center we lost this wouldhave taken place in instead of thesquared error if you did this navigationinstead of this square we put theaverage sum cross-entropy applause andthen you went through the steps and yougot a negative the truth which doesn'tlook that much different then that wouldbe called real logistic regression nowof course notice that in this other guyyou have logs so you have now startdifferentiating logsyou know what is the D by DX of log Xyou never should forget this is reallyuseful lots and lots of time so if youdo that that becomes artistic notationokay we will stop here one full weekafter again