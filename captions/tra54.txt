started so attendance for and so we willbe discussing finally ever have infinitehorizon value iteration and they'reactually for the infinite auras in thewatcher quantization part we probablywon't talk to todayand so before first of all I miss usingthat all of you have looked at thewalkthrough after final terrorism valuecapitation quickly today but since youalready did that you have a sense ofvalue of a stage and you also have asense of reward of estate so my questionthat I often think about is thisstatement true or false the value of s Tis proportional to its reward how manyof you think it is true raise your handsso you think that the value of a stateis proportional to the reward that's ofthe people okayhow many think it's false why are therest of the people actually do that lastright you should have enough convenienceyou know there's no point in showing upif you have no opinion about what'sgoing on herewhen asked again value of a stay isproportional to its the word how manypeople think it's true okay how manypeople think it is falseokay sois that it let's just get this once andfor all out of our minds by takingexample there are two states state onenight hang around outside hydrogen aloneis beautiful rather like the first ofthe beautiful days to come to laughsclassokay the port rewards you can be honestlike I mean I even in my wildest dreamsdo I believe that people think hangingaround outside in the less interestingthan being in my class right so theimmediately walk up maybe these classesfor more negative cannot make you knowespecially on this beautiful stuffthat's going on outside okay that wasinteresting Mises telling a few jokes onthat but honestly okay so if you want tomake up some words the reward for thisseems to be kind of like 100 the rewardfor this is like - point you know maybe- find seven what okay that is what nowwhat's the value of these two escapes inlife I just remember what you're sayingokay okay God you hanging on outsideenjoying rather that's about itokay there's not much major things youcan do because you enjoyed the weatheryou come to this class lots and lots ofopportunities open up your grading boostyour mind opens up your more jobopportunities open up so Allah untilthey were rightyou understand what I'm saying so is thevalue of these two states proportionalto they rewards if we ever have aquestion as to whether Ahmad value isproportional to the reward think abouthanging around outside versus going tomy class and even after many many yearsthe woods will still be fresh rememberthat the immediate reward of being in aclass for the rather rock it wasn't verygood and yet you will be enjoying yourGoogle salaries right you understandwhat I'm saying so in fact this is likethe most important thing about myaccommodation processes but people haveto understand that the reward is notproportional and all it's not actuallyrelated to the final value would youprefer if I give you two different MVPsone in which I just somehow make thereward is proportional to the value theother in which then you watch to bewhich would you rather prefer as anagent the first one right becauseessentially that way you don't have touse your brain you just say oh thisthing looks sweet must be good for meyou see what I'm sayingand the people in our lives who do nothave particularly big brains other thansome of us are little kids okayso you make sure that when you build ahome with a living room you make surethat you don't make your dumb theshiniest object in the homeright because kicks think shiny objectsespecially if you also put some candy onit they'll think wow that's a nice petdetectiveyou understand 1f says okay so ingeneral so this is very deep points thevery deep points in general like thisdown reward value is not proportional toin fact most of the reason people screwup in life is because they go forimmediate good reward have you poweredup delay to the world delayed reward isall life is all about okay the peopleout here out there they are please getthe concept of the native or assaultlike me okay and the point of course isthat in general value is notproportional value the agent agent Martythis is the word that you should use onyour friends and you stop being so myokay okay my hope it means looking onlyin front of you not looking ahead okayin words will rewards are proportionalto the final values local decisionsbased on what is in front of you alsowind up being is globally optimaldecisionsnormally that's not true okay so if infact in the kind of in the kids casewhat you are doing is you can stabilizethe environment by reward shaping so forexample if I want it to be nice to you Irealize that I want you to get yourGoogle jobs I realize that you don'twant to come to the class so I will saythat it can't be every day rightI suppose it's candy you know instead ofoutside candy inside and come here andthen we'll cross that over some teachingin fact I want to make sure that youwill do well in your life then I mightdo it even though people might actuallyput me together stuff that's basicallythey watch it how do you make HomerSimpson go to the library food donorsall the way to the libraryis what I'm saying that's why whatshaking so give me the damn DP if me maynot be the case that the rewarding isproportional to the final value ingeneral this is not at all this is notat all true in good M degrees like veryeasy MVPs where rofl decisions we mighthave being optimal you you know it turnsout that either that it is the case thatrewards are actually proportional to thevalue as somebody took time to make therewards be proportional to the land okayreward is the look think of reward aslocal gradientvalue optimal solutions it's related inwhat is related to tell of convenientvalue is related to the final theoptimal solution you see what I'm sayingand and so if you can in general whenthe body is not about proportion to thevalue that is something that you justhave to understand you can have a highlypositive reward for a state whose finalvalue is highly negative that is maybefor this and I mean negative reward butwhich actually the value is highlypossiblewhy do so this is for example by the waythe word shaping is done now all ourlives because I'll make you just don'tlive in the world that is given to us wechange the world in a minute right youbelieve that medicines are sweet all thebits and say I want to proceed rightwhen you put it in you know but you puta tablet most of the expression thatkeeps tablets it's fun you know they'relike nice and sweet they make it sitjust so that kids will eat it your kidis not going to say yeah I should reallybe taking this really no beta releasethem now because 15 years later I haveto go very high class and then you get ajobsince parents are into making kids dowell and the society is into making itsdo well they want - you are shape theyhave another one okay understoodquestion yes it's not actually that arejust true of that particular soimmediate reward so first of all thefinal state whatever saying final stateis just reward you know immediate rewardof being in that state okay immediatereward of being in state particular timethat is then the end of like being inthat state is not a power connected toits reward in steps before and it isconnected to the case that before moneyin terms of EDA expectation-maximizationokay by the way if you think in terms ofrewards this book surprising if I justask the following question if you haveevaluation functions in the game threeand you could have jumped just basicallylooked at the evaluation function valueat the first level itselfI've gone deeper applied the evaluationfunction okay to set it up if theevaluation functions for actuallyproportional to the final backed upvalue did you understand what I just saythen why should you goody there's noreason it's only because the evaluationfunction side yeah yeah may not go tothe final value must you go deep to makesure whether or not actually thebackground back up value is a trueindication of the balanced reward is notabout indication of the well in the caseof given please without rewardsevaluation functionthese are things that you want to keepin your mind because otherwise they'rejust topic one topic to topic but allconnectedyes it's a characteristic of the Editdecide MVP is an environment where areyou so this is the environment we areliving itsome people came into the class some arestill outsidethat's not a friend about theenvironment it's if you were to startfrom that state and live a long enoughtime remember that value of HT is reallynot directly the property of the statevalue of a state is the complexcombination after you one of the statethe actions that you are available thestates those actions will take you tothe actions you are available at thosestates and the states that will bootthat's what was being done in theanalyzer MDB commutation it's notobvious at all what is the true value ofthe state that's why you search if infact there is any obvious connectionbetween rewarded value we should beresisting our time talking abouthappiness you just do the obvious justtake the reward and I'm also do botheryou here this reward shaking thing a lotand I explained to you what aboutshaping it we watch it with twicethey reward we soccer proportional tothe value okay good so let's go so weare going to talk about finite horizonMVPs to walk to the slides so we arelooking at the value of a policy firstand then we look at how to compute theoptimal policy okay so now to decide howgood a policy is you have to talk aboutthe accumulated reward following thatpolicy start in the pending state so ifyou're in some state yes then using thispolicy you can go you can get onetrajectory you again use the same policymight get a different project you canuse the same policy might get adifferent project right the reason youget different trajectories is becauseyou do the same action sometimes it goesto state one sometimes go to state toits promise and so anytime you have atrajectory you can compute thecumulative reward you can add up theimmediate rewards of all the states onthe trajectory once you are followingthis policy by even though it looks likeevery time you run the quantifier youare getting a different trajectory theyall have more similar to each other thatmore similar to each other than theentire set of all project trees of theworld because you're still doing onlylet's say action a1 at state s1 and hereone will let's say will with 0.7probability P 2 s 2 and point 3probability 2 SP it would take you toany other states right just becausethere's some random a thousand mean it'suniformly random across all projects sowhen you on the stage you basicallyconsider a bunch of treescompute the cumulative or out of therewards you get to make a robot take theaverage of this that is the expectedcumulative rewardI've started from s I am following thispolicy regularly remember when I'mtalking about finite horizon the policyis of this type by asking to the policyPI's it's just a test and you have Kstages to go then you must do action isthat's what the policy about Jordan okaythen I'm saying I'm simulating thepolicy then I'm you know here let's saythis is one of k states then here arebasically taking PI s scale and then atthis level I take whatever the state Kminus 1 PI s - K minus 1 then the nextlevel I came is Double Dash K minus 2etcetera so I'm moving these in the backof my calculations okay please I add upthe cumulative words I take the averageI will get the expected average this isthe family of the state so you have abrute force way of doing itin general in computer science it'simportant to first know whether you cando something in the group for sale andthere will be a computer scientist I'msaying can I do it in a better right soin case you forget about this a bruteforce way of doing something aftersequence is given a set of numbers youfind all these permutations and check ifany of those foundational is sortedsimple just n factorial permutations andchecking whether something is sorted ismuch cheaperonce you're trying to do this thenthey'll throw you out of the job becauseit turns out that they instead of doingthis generate and test you can actuallypull the test part of a general testinto the generator that's what somethingactually does in fact Minskyhas said that someone to findintelligence is putting the test partgenerate test people into very deepstatement this is how you actuallyimprove alternatives you start with adumb idea and you prove algorithm by noteven generating trajectory is that noteven generating solutions that canpossibly be like an expedition thatcan't possibly like in the case ofsomething what you do is if you alreadyhave a subsequence which is unstartedthen you don't try to consider all itsextensions because you're dead in oneplace you're dead allôthat's the truth of heartednessit's not actually true if you're sayingdo as much something as you can so fastinstead of changing if I change thedefinition of something problem to saywhat is the optimal amount of author youcan put in your room before your momcomes home when your dad comes for moneyI'll set you saying why is everythingeverything that's an optimizationproblem then it is not clear that apartially inserted sequence cannot beextended to become a globally optimaltime but now most often a partiallyinserted sequence the reason I'm sayingthis is this idea that I showed you isthe brute force way of computing thevalue of the policy if you have nothingelse to do this is what you will do youbasically simulate the policy get thetrajectories about 3 watt numbers takethe averages put that asbut then what you realize is this isactually a structure to the values afterstage with respect to the horizon thefirst insight you realized is if youhave zero stages to live if you haveescaped us you have zero stages to livethen you are now going anywherewhatever you are is the best your lifeso if you have zero stages to live yourreward is your policy I mean your valueto people understand this if you zerostages flip the reward is your valueokay so then what I write is essentiallywe PI 0 of s equal to R of s that's thebase case so we already know and we PI 0of s and then I just need to opportunity1 of s in terms of V 0 of s we doaffairs in terms of V 1 of s and so onwhy because it turns out that just likesour team has some structure just likesake has watch it you essentially havestructure in the values of states givena boss ok so the example here this is V0 PI of s without a test and then wekeep PI of s that is K stages to govalue of the same state s is all of SPlus lops if I'm using PI basicallythere is something saying fine SK whatis the action is K is an action thataction will tell me the states I canvote this transition function here isbasically saying if I do the actual PIescape in state s what is theprobability that I go into sso if for example this is an actionwhich ones are taking you to state as towhich property point-seven s3 whichproperty point 3 and 0 everywhere elsethen most of this will be zeros 4s -except for a step equal to s us - equalto SP right and so those are theprobabilities and then once you do thatso if you actually want to go to s -then promise you will live with 4 Kminus 1 more steps do you see what I'msaying so you essentially can I take thevalue of the policy Phi with respect toK stages to go upstate else to thevalues of its neighbors as defined bythe policy your neighbor is the one thatyour policy can take you this what I'msayingand these values with K minus 1 stepstable and this computation will be to goout with K minus 1 K minus 2 up to zerothe zero stage you know what that isthat's what values application is now ifI just tell you this but remember if youwere doing this then you act as if everystate is completely disconnected withevery other state you try to compute avalue of state as one you will do hugeamount of empirical simulation take thecumulativerewardin take the average for itsneighbor do the same[Music]you are not realizing that the valuesthis is the real estate all over againDaniel of our house depends on the valueof your neighbor's house which is why inus in particular people are very mean totheir neighbors if you don't give me afront yardyou know in fact let me say is that youas people are generally happy to havetheir keys truck is as neighbors but notpeople who don't believe in their frontyardthey don't leave their front yardbecause the house value is going downyour house where is good enough rightthat's the same thing is happy to hearthat you are value satisfying in termsof your neighbors vendors neighborhoodin terms of the policy and if I don'tuse that then I'm trying to compute eachvalue of each state independently so Iwish a lot more timeso you just as you really appreciatesake in after you think of first theidea let's just take a permutation andthen test whether it is a factoredsequence or not and that can factorialand they and if I give you even and yousay man you're smart right so thesearmies are much smarter idea and what isit what is it able to do this it is ableto do this because the value of s Tdepends on value of its neighbors okaythat's what we did okay so how can wecompute V T 1 plus s given V T withrespect to some policy Phi with respectto policy PI is and then finally youknow the computation goes right to leftthe rightmost case will be you know s1s2at zero play but so be zero offense onexcited class so you have their backthat is already because afterward andfor these guys you know yes I will sayyes and happy plus one stages to go andthe quality tells me that if I have aplus one stages to go and I'm supposedto do actually that's what my policytoward this is the policy I'm trying toevaluate how good these quality meatsright okayand so then I take it don't tell anyonewill take me one cell for us one onethree property to the school so then Iwould basically consider thiscombination five seven times three timesright add that to our s again with evenpresence even though there are variableshere when you're computing the value ofthe policy you just basically get youalready have three 0s so you get me onea static this is the way we compute thepolicies that this is the way we convertthe value of AIDS policy value of aparticular way of living a policy in aparticular thing because it's tellingyou in this state if you have so manyyears to go do this so for example apolicy is still young and you knowyou're on this but you know you have somany more then it's basically equal totell you the policy value that you getif you hang around outside what ispossible you get you some of the bus myassumption is that when you reallyreally really order your brain isokay[Music][Music]that's the value function I would saythis is a holiday value vector is avector in this particular case is wellsteps long it's a vector of length walkright so if the finite horizon is