okay so those are the kinds of things wedo this by the way is for model sleepand this is called model gaze okayim3 will like one of the things I'm Ihope we'll get in this class by the endof these two process for this personnext class is to understandI think deep level the differencebetween models being versus part of thebase if I would have needed in differentproblems okay and we will give exampleswhere we actually have ourselves newmodel free and what will be nice tolearn real life just so that you get youknow that distinctionokay so this is basically the mainthings that we are going to do in thenext few classes especially these eventsokay but in doing so we consider andagain this is basically one of baseversus model this I will actually let melook at that firstwhich is the piece or model base youwill learn the action models basicallytransition probabilities a model of anaction in an atomic sense is if I wantto do this action in this case what willhappen okay if you have the model youcan simulate the world in your head ifyou have the model you can simulate theword in your headthe nicest thing about simulating theworld in your hand is nobody dies whensimulating stuff in their head exceptmaybe in Inception which have seennobody dies during dreams your dyingdreams you can still make up right andsimilarly if you're thinking about backin two days and I gotta go and then Idie or I will go thatthis is what sort of child is all aboutsearch is microsimulationbeing done in a clean algorithmicfashion so if you have a model of theworldthen you can simulate the world that'svery importantyou can mentally simulate the world butI'm not of course from that you can alsolearn the optimal policy and figure outwhat's the interaction but if all youwanted to do the optimal action then youcan also do model free which is in somesense a cheaper technique because itcuts the little thing out the modeler isnot learned it goes directly from theproblem specification so I will do thisif I'm in this place it's kind of areflux agent the number from the agentpipes it's like there's a valid thing itknows that I've been in this state Ishould do this at a to learningbasically is you are just way out backrolling with next agents you can go andwith reflex agents you know again it'sabout computing value except you areremembering the two values rather thanjust a single value for the state so thecrucial difference you want to keep inmind as I said is the ability tomentally simile the future if you everneed to mentally simulate the future youneed model based learning the futurethen you can get by model okay anexample why I said Iraq invasion thismust have been detailed using the timeand they were invading Iraq but you cansee that we are about to invadesleeping what's a stagette is that wewant to invade ok you try to invade andsee what happensalthough we seebut we actually plan we have models maynot be greatest models but we havemodels you use that one is to simulatethis is what we do this is the kind ofstrategy we will do you know what fearis the ultimate game theory you dorealize that that game theory develop inranch operation out in Santa Monica andit was basically bankrolled by DARPAwhich is the progenitor of DARPA whichis you know US military what it is theultimate game team you guys for exampleknow that nominee division there washuge huge such a future they did youmight amount of stuff to make the otherguys think that the invasion will besomewhere elseso they got all their people out thereand then you in here you also knew Alexyou don't you know I'm going now thatenigma code was broken before and thedeliberate choice not to let Germanyknow during basically broken makingEnigma code you know that it will choicenot to let do anything that will letGermany you know that they have brokenthe code which in war for exampleknowing that this particular shipcontaining some of you is going and it'sgoing to be tougher the word it says funit's for the greater goodyou guys will be gone but the rest ofother way aroundseriously this is what basically youknow what fair is you essentially let afew people go just so that axis won'tknow that we grow anymore and they'remuch later of course you're waiting forthe big payoff is what I'm saying so nowwhat is we do simulations mentalsimulations your mama's mentalsimulations for things like wow what'sus swimming swimming you just do it it'sa Mikey if I by the way Mikey is themodel three capitalright just do it don't just do it likesomewhat swimming this is my mind youthink it so suppose I want to see my infact one of my students told me that Icould see with my swimming by lying onthe floor I see but secondly that stillsimulating an external world it's a fullsimulated fundamental simulator we areextremely bad at mentally stimulatingswim because there is no modelsremember mom but I said I think it lookslong bad but when this course started Isaid and Coursera started I was worriedthat I won't have a job and that I wasgoing to become a swimming instructorwhy because we we can't be run fromvideos and reading books except I guessthe one article people who everyday cando that but for most of us somebody hasto hold a sort of show and then in factyou need to figure out what is happeningwhen you arrange it in fact in a worldwe have simulators for flights we don'thave swimming simulators swimmingsimulators are consuming pools you don'twant to get that like my student justhanging around on the lab floor is notgoing to helpit's a very poor simulator so it will bethankfully you may have a lead levelplanning as to okay I will go from hereto here soon from here to here that fromthere to there to there until the actualparts of what I am going to do atvarious points I'm not going to wrapthis off I'll just do it my image andwhat I have learned and you can call itmuscle memory or whatever the heck youwant to value itbut that's model free learning what isit we'll we'll just do it you who neverswam but justice just do itthe swimming champion neither of youcannotsimulate very well on even reasonablywhat happens when you do various thingsbut you have much more experiencebecause of which you have a good model 3- well nice especially so at that pointeven exact words that I'd actuallyreflexing ok keep that in mind so I doone other thing is people tend to one ofthe crazy things that people tend tot-touch is opening ie people as well asthe health of all people is that somehowit's all being done with no knowledge sohow many people believe that our girllearned how to play chess with noknowledge about chess how many peoplethink that it played without any notneither them are trueit had a simulator somebody built thesimulatorrather than tell you know the Ergopeople these are the actions in therecord-setter trap they give a piece ofcode somebody else wrote it tell me whenyou define writes that code also itactually comes up with interesting gamesand just learn how to play it and alsoseduces you guys to play it so that youwill be very impressed that it won howmany of you for example know that weachieved path of supremacy do you careabout the problem that was solved toeach quantum supremacy should have goread that it's a made-up problem just toprove a point nobody really ever wantedto solve that problem people he doanother problem existing but it's aproblem that they were able to solve toshow that quantum computers in thisparticular case definitely do betterthan classical computers is the samething that will happen if you want tomake games by yourself and start playingin a computer sense because nobody elsemight be interestingthe ones that really play we haveessentially given the simulator that'sknowledge about the game so anytimesomebody says I don't need any knowledgewhatsoever and they say I just need asimulator it's a star the simulator youdon't want to hear me you want just thecode somebody has to write the codeokay so simulator is a way of actuallytelling about the world if I'm everentirety back in the world the worldthat we are living and then you foundout on and you get up etc they are muchgreateryou know reinforcement learning is ifyou hear that reinforcement learning isbeing used in a particular in thefollowing application we shall run forthe hillswhat is this application it's a verypopular application if you hear thatreinforcement running being used forhalf of your have run for the hillsand if ago okay it's a new theoryenforcement running for some otherapplication like yeah think about weshould run for the hills which one soshould be who came truesounded kind of good and they were goingabout the car people don't like itthat's what it was already means to readfor students with no knowledge okay soin fact people I think um they couldn'thave this video that you've shown methat if you are one if you're learningby video now I driving by is are drivingby we disagree essentially I wanted toput that animated give it start turningover a steep road following us that's abook on a steep road again salsa wouldyou want to be knocked up even Googledoesn't have that kind of a money youhave due to a number of cars dying andsuddenly you don't have any money to dieeven though it did allow somebody to dieas you guys know with stupid Googleself-driving car which is worse than anormal guy but anyway so so keep allthis stuff in mind is usually asimulator you're using somebody'sknowledge and in fact if you don't usethe simulator then you are really whatin the world and that's very parsleythat's very very fastly many interestingdomains like seven tiny trying it out inthe airport is no good I mean they cansay look you know unlike human stage youbecame a good driver your friend doesn'tmake a good driverwhereas is one Google car becomes a gooddriver all Google cars mechanical driveso you know Larry and Sergey and comeand say you know come on guys it'd beenough people a few could die it's opengreater good it's like that enigma thingyou know some some people going on theships after you beat up a door but wekept a lot sure we can do this you knowoutside of all but you know maybe themost accommodating these days and sothat way you could argue but obviouslyyou like her at all because our stafffailure is extremely high andwhat we have in mind whether it'sactually it's kind of useful to keepthis thing which has nothing to do withthe actions but it gives an interestingdistinction between okay knocking beforegoing here type for that to say this onethat's it what's the tactic that's itwhat's effective really activereinforcement means the only thing thatyou're interested which is the agenttrying to figure out what action itshould make when finds itself in anystate which is what MVP optimal policieshowever under wait to eatwe will just like me in the case of MVPswe wanted only how to compute optimalpolicy but we went to the imperious stepup if somebody were to be Mia policy howdo i compute its value and I use that tokind of understand how to do the optimalpolicy you know 1k just a set of linearequations are I guess they relax themore the gunmen are cateredokay so enforcement when we do wedistinguish between passive versusactive learning in the case of passivelearning the policy has already beendecidedand you are just blindly following thatpolicy you can't change the policy butyou can learn you to have this it's likethat you building do much if we needgood experience you can see that in aminuteit takes up we will get that I reallyneed somebody better be good becauseI've tried this policy multiple timesand encouragement so an example of thatI don't think I have it here and in thenext slide it's also you know then youwanna kick it you sat in the back seatright to the seat and your parents aretaking you they are improving the policyand even now for your thoughts that'swhat happens the dog is in the carnot only the policy unless your dog diesbut as you were killedas you turn that you back towards aplace where you have seen the last 15times this ice-cream power and then youhave this memories of nice sweet icefield that you get really you startjumping up and down you realize thatthis would happen your dog does not forice creamit basically starts doing it for petsbaths have essentially the ability thesixth sense to figure out that they'd betaken to the vet's office is there butthat one will be wet gene or somethingno they have learnt it after a while butnobody ever comes when you go to thebare surface is it to poke you and sothe moment you turn oh my gosh that'slearning it's unfortunately sad learningit's learning that doesn't changeanything but at least you know you seewhat I'm saying that's passive learningit's useful somewhat it's good tounderstand that you what's going tohappen to you in future even though youcan't change your futurethat's passive learning in activelearning you are essentially trying tofigure out what's the actions I shoulddo such that I get the best future I amhere okay that's the distinction andwe'll start with passive learningbecause that sort of connected to policylearning policy evaluation and then goto acting okay actually that we skipover this for now and then actually dothe passenger so here's the simplifiedtask so I have my MVP picture here okayand again I don't know the transitions Idon't know the rewards I'm just given apolicy PI it's actually fasting so someparticular concepts be given so this isthe policy of India those hours tellingme which action to do in what state okayso this is the policy that I know isgoing to have so my question then iscan I learn the values of these dips Iturn I know the rewards because themoment I go to that state I know thereward and next time I go to the stateI'll get the same reward the question isdo I can - can I get the value so forexample has ever seen when you indicatedyour policy was most indicated by yourparents if it's 6 a.m. they talk or goto school if you have a word did learnthe test Mondays and look power toFridays you did control your life butyou learn it but ok this is when you abad day for me it's Mondayok that's what's passing by ok so oneway of doing passive learning is thefollowing which is in this particularcase I will spark from let's say anyplace take the action that equality isasking me figure out where I go take theaction what I'm supposed to do there etcuntil I die in this case actually I'dstop each episode stops with thetermination state ok that this thesquare ones are termination States itturns out actually that this thing canalso work for non terminal States andwith discounted rewards but we justunderstand the simpler case whereterminal States and undiscounted reworksok so here is a possible episode justmake sure that you understand that I amin 1:1 I went out because I'm in 1:1 Igot a minus 1 rewarded because theexpectation is that rewards is minus 1everywhere except here and here this isplus 1 red and minus hundred so it'stough minus 0.04 it's minus 1 here forevery state so then I did this up andit's the same kind of actions when I tryto go up and go up with point eightpoverty and sideways it point to powerok so then I went up this time when Irobbed and basically went up here thoseare probabilitiesremember that okay I just told you aboutprobabilities but you don't know thisthis is the hole and made I amcontrolling this one I regard your onlything in this one in fact not evenacting in the world you're seeing yourparents act in this one okay so allyou're doing is you did one one andyou've been given one one we did up yougot two one two and in what we aresupposed to in work you did that it sohappen that you just wanted eventhough we try to go up because youdidn't know why but I'm telling you onewe know as to you and made these actionsfantastic and when you try to go one wayyou don't want point two probability youtry to Buddha by deductions and in thisparticular case a both sides that wallsand so you wind up staying in the samestate this is the God's moving what'shappening Lee but you don't know how youjust get in the experience okay thenwhat I myself want to either subscribeyou know any questions on how anexperience like this can be made andthere are other experience if I havethese two experiences the question iscan I just compute the value of a statewhat is the value of a state value isthe expected cumulative reward from thatstate according to this episode okay sowhat happens is in the U of 1 1 if youare 1 1 then basically you add up allthese rewards you get 92the same way you add up all theserewards you get 100 minus emergency soone time I got a 92 cumulatively 1 minus1/6 you ready to goso what's my best guess as to what isthe cumulative are expected utilitywater you get average of thoseokay so that's it you are fun when isminus plus 92 plus minus I and C by 2which is minus F similarly one of thethings you can do is you can also seewhat happens plus 3 3 D 3 occurs here aswell as here so you can use the sameexperience to compute the values ofother states you don't have to startfrom there separated so from 3 3 I haveminus 1 minus 1 minus 1 so minus 3 plus100 okaythat is mine the plus 9 is 7 okactually there are two easierdid you guys notice right so because thepitch 1 3 3 is 99 other key 3 is 97until you die and the other 3 here isminus 1 enter I am out of them subtract2/3 that 1.8 success you learned areinforcement learning algorithmdepressedthis is reinforcement learning againthere you had numbers divided it doesn'tget any betteryou sometimes also multiply by alpha thequestion is not that we force runningout of games then you do kill theoutreach with them it's veryunderwhelmingit doesn't look like this what happensmy car I expected something like a B andrebalancing you know at least it is ofwhat happens is G possibility isessentially you are trying to look ateach experience and we are quicklytrying to improve your estimates it'sjust estimating proving what'sinteresting is there are better andworse ways of estimating proving andmuch of a distinction between thereinforcement learning algorithms is tounderstand that this is like thestupidest way of doing itthis is for the Montecarlo estimationand what's called the direct estimationit will work okayand it's like a dust mop is of core andthey keep doing me a number of thingsand as you behave behavior be here aftera while you will get the actual valuesand the values would be as if the guardcomputed policy evaluations using thepermanent equations why because the Godthat is I am who are controlling themthe word and essentially have the actualstochastic distribution for each ofthese actions but the point ofprobability see is that long term theratio of the number of times an outcomeoccurs with respect to another outcomehe is proportional to theirprobabilities the more probable outcomesare come more often the less probableoutcomes occur all that's happenedthat's a lot large numbersif I tell you that this is going to kindof point nine hands point one type tailsand I'm about to toss can you guesswhere it comes heads or tails perfectlyno but if I were to tell you that thiswill become point nine hands point onedays I'm going to now pass this amillion times can you start puttingbombs you have much better idea therenine hundred thousand approximately wasa hundred thousand approximately betweenthe number of heads are not rotatesthat's the law of large numbersthat's what probabilities are fine sothe world becomes its own simulatoressentially and the probabilities willcome in these summations document and soin essence you are solving the back wallpolicy evaluation by just simulationokay it looks almost as we why did youmake us go through the other stuff if Ican do this well because I want tounderstand and in fact the only way thatfailure song is try to take somethinglike this right and try to compute thevalue of this policy knowing the modelsee how much time we took try to computethe same value this way and see how manyiterations you have to simulate and howmuch time we do and remember there aretwo kinds of complexities one is howmuch time did you have to wait beforethe Act the Daniel Sturridgesecondly is it and we actually act inthe world you have taken a chance for byokay and the expectation is that if youhave the model you have you get thepolicy value much faster than this wayokay so somewhere here is like in termsof the task let's say with the model thetask for cons evaluation with expects itwith this approach it be somewhere hereand the next approach are not good canyou will get you some very big thingsokay and in fact all the enforcementthat is in between you see in essenceokay so now let's look at this directestimation and see you understoodeverybody understood this way I'm justbasically doing the assimilation and Ijust basically for each state I'mlooking at how much sure and I'm lookingat the problem as if it is a solutionthe definition of the problem is valueis the expected community reward so Itook that directly and try to computethe word for each sequenceI put the expectation which is averageis what I'm saying this I would argue issort of like saying you took back theproblem specification of how to solve asequence of numbers a sorted sequencewill have the following propertythe increasing rather than every numberis smaller than the number to write afit okay and then you convert itdirectly into an algorithm which is Iwill generate n factorial permutationsand check if any of them are subjectthat is the solution that's actually afinite time solution but the whole pointof computer science is not to do it okayso my piccolo is sort of like that andyou have to understand that so what isit missing what is missing here exactlyas if each of the values in each ofthese states are independent variablesthat's not truein fact the values after spirits areconnected to each other by the Belmondequations intuitively you feel up nextto a neighbor who is very rich you can'tbe very poor where neighborhood isdefined in terms of the actions he'scertain say but this game would realizethat you would essentially act as ifthere are as number of unconnectedvariables and it's estimating each ofthem separatedthat's a huge huge waste of time okay solet's say oh they're a bunch of thingsthat happen hereso so we're basically doing empiricalquality evaluation but we know this willbe wasteful because it misses thecorrelation between the values of theneighboring states what do we do werealize that they connected by Darwinequations let's just use the normalequation yeah use the bellman equation Ineed transition probabilitiesyou see what I'm saying so what we do isfirst of all the first thing is to justthis is an example to show that when yourun on this particular little examplewhen I run the Montecarlo at some pointof time you might find half you mighthave things like this where for examplethis one is classy this is minus 2 thisis plus 4 this is plus 1 that's globallyinconsistent because how till this guygoes next 2 plus 4 because he whereasthis guy was next to class for the -room even the same actions that we allhave that's what is not understandingthe correlation between the values ofthe states so to do that what I'll do isI do Batman equations and the simplestcase the simplest case is I take thesame example as before instead ofestimating the values directly Iestimate transition probabilities I cando that even the same thing right forexample if I went 3-3 and I do writewhat is the probability that I'll go 2for 3okay that's one of the properties I needso I am in 3 3 I did right hereI mean 3 328 ami Gigi Hadid idea