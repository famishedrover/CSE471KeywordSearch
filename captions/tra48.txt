so today I will be talking of course mylearning eye but we begin buzzwordbecause most of us are probably movingafterward thanks to alphago so this is abuzzword part of the course that willstart now okay so the best way to thinkabout so basically the word learning iscoming for the first time in the classtoday okay and interestingly actually wewill still be within atomic modelsatomic models essentially means that thestates of black boxes and when willactions we get to another state set ofstates with their poverty coming to theforethere is no internal structure for thestates there no state variables nothingyetokay it's kind of quite interesting tohave that in the back of your headbecause actually you know other kind oflearning is possible in atomic modelsnormally to do learning we talked aboutthis notion of generalization whichstarts with problems that the modelsthat are above atomic so factoringfeature based relational and so onokay here it's not obvious how togeneralize anything but instead whatyou're learning here is essentially theparameters of the Markov decisionprocess okay so in fact it's what we'rethinking that we've been from a StarSearchwe match to 10 VP in all these cases youwill be told what the photon or themodel is okay so in the case of a starsearch what is the angle the model isthe child generator function and thecost of all that you know actions thatare getting to those children if youhave that model then the only thing thatis left is what start inference which isgiven this model what's the best what'sthe optimal solution okay imagine I'masking you to solve an a-star searchproblem and I do not either first of allI might give you a charge emulator whenI won't tell you the cost okay but thenI will tell you that when you get to theoptimum when you get to the board in theoptimal path you'll get a candy youunderstand I'm saying can you learnessentially what was given to you beforefor free which is this village andgenerate a function plus costs of theactions because if I give you the customaction then it says normal a star so inthe case of a star search actually youknow that if you do an action it takesyou boom you know there aredeterministic actions they take you to aspecific stage okay so in fact one ofthe other things in is I might not alsotell you that you do an action whichstates it takes movie so you know thenames of actions you fight them out andsee where you are going and we keepdoing it until you reach a still it'svery same in which goal at that point ifyou get a candy how big the candy isbased on that you know Adama you havedone something like based on thisnebulous clear enough number of timescould you have essentially essentiallygot the model that you haveas they important before please say whatI'm saying we could do this integral tothe case of a star such people do thesame kind of an idea in the case of minmax we just choose to do it in the caseof emptiness because in some sense MVPsare more general than all of these so ifyou understand in the case of MVPs Iwill employ an MVPI won't tell you what the transitionfunction is so I just drop it can you befact in most cases will tell you theaction names in real real wordsactually you don't even know whatactions are ok but imagine I give youthe actual names but you don't know whathappens when you do those actions okayand so when you do the actionessentially something will happen youwill find yourself in a new state andbased on that you lateroh that means if I do this action I getto this step now because this was a starsearch you did this action in s1 and yougot to s2 you can always remember thataction a will always take me from s1 tos2 if it is empty please you can't dothat because these actions can bestochastic so this one time it took usto do it again you take it as one andafter doing that in infinitely often okthen you can take the ratio after numberof times you visited a particular statedivided by the total number of times inthis action if you do this you go to thetransition probability that this actionfrom this state will take you to thisother state okay so I gave you basicallyI'm asking I'm telling you that you havean angry Pete but I tell you why I won'tgive you the transition function I alsowon't give you the report functionexcept that is you are in the state andthe state has a rewardwashes over you so you'd be here I'msorry I'm sayingso in fact when this is think ofyourself as an indication you give me tothis world in fact you don't even knowwhat your actions are all you have ourfriendship effectives okay I have a handI have a mouth the Big Easyeverything looks like put it in yourmouth and see you can have a thing andsee some of the lectures we still dothey're very useful actions eatingsomething that's not working is always agood idea it was it was right someothers like putting everything in themouth we start doing ugly okay but itwas like a very popular action when wewere kids and then I mean also ifsomething is bright you try to kind ofgo towards it okay and then there arespecific sensations you get essentiallymake the sensation of being courts insession of the WOM traditions are beinghappy these are biologically wiredsensations like for example if you wantuh putting something in your mouth andit all was it was sugar essentially it'slike taking drug for a kid who basicallywas brought up on mothers will you guysprobably don't remember this but itturns out that the human milk is notsweet okayand so kids who are proud of hermother's milk the first time this muchpure sweet the essentially oh my and I'mstuck with this stuff with lotion whenwe contain okay so essentially they didknow what sugar was but we are basicallywiredevolutionarily you get it high if we webasically have sugar on our so then theyeat the sugar II think they get thereward and the tortoise washes over itif in fact you're one of those kids whodoes not have that sensation that meansyour tongue doesn't work then you haveto be told you know youcan be you're feeling sugary sensationand that's not fun right because that'swhat you're saying you should enjoy thisclass I'm telling you this that's theonly word you should just enjoy by youknow the enjoyment for the watching overyouthat's the one okay if you think aboutthis actually these things for examplewhen we can reenact that but the kidactually kind of walks over a a sharpobject they will feel pain that's thereward washes over them imagine kids whodon't have a proper sensation in theirfeet in fact diabetics all the diabeticsdon't have sensation in their feet andin their activities and one you knowmany diabetics bind up dying of ganglygangrene has nothing to do with diabetesgangrene is basically what happens whenyou got hurt and they the moon and themoon sexy sex it not only as soon as weyou know basically have a wound we knowbecause we have the pain sensation thereward just washes over us the negativereward washes over us when we don't feelthe pain sensation we don't notice itand then that can lead to gangrene thatcan lead to sepsis that can it alsosucks resumes the point I'm trying tomake is you are not given either rewardare the transition function but ifyou're acting in the world you will feelthe reward in the state okay and to someextent is not really rewarding statesright I mean even though I gave you acandy we are even though you said youknow you just one and five milliondollars and if you don't feel anyhappiness then much because essentiallyhave been compromised and then of coursethat we need tips or problems so we disassume that the reward can be sensedwhen it is opportune we can assume stillback in MVP so if you are in aparticular state you know what thatstate is so my question then is can Isolve this MVP optimally okay rememberthat if I am given via hard we spent allthe last three classes almost trying tofigure out how to do value iteration howto do false iteration to compute theoptimal policy and the class before thatwhich is the end of the game please wetalked about how to do online search tocompute the policy as needed at thatplace where you are it oh this is soonall of this assume that you have themodel the super models that MVPs requireare T and I he is a transition functionR is the reward function now this is notjust an empty people it's also astarting today I start the transitionfunction is if I do this action fromthis state they will I gothe reward is the cost of the action iswhat I'm saying so my question is if Ihave a problem solving setup and I canfind it out I can try this problem outin the real world can I slowly learn themodel so that I can become optimal inchoosing my actions over a period oftime and the fact that you are hereessentially means probably that sort of- although we tend to think they do youthink optimally obviously we are not butat least we can do better than the veryfirst time you see you know experimentedin the world okay so this is basicallyour reinforcement learning actually theboard as I mentioned the wordreinforcement learningpsychology literature this is thepavlova or attendance cone and bammobasically we'll do this Pavlovian dogexperiments and of course a stir jokethat the dog is actually thinking thatit's controlling tableaux without alwaysthinking that he's controlling the dogbut essentially you're trying to givethe enforcements like positive rewardsfor there are many does the right kindof behavior this is the whole damn treeyou still do the same thing the problemdots are not unison they have no good ifone full generation of barks and learnedsomething then the new generation ofbarks have to learn it from scratchagain under satty evolutionary scaletheir genome changed there is no way upthe odds kids to capitalize down theirparents or their grandparents we havethis nice way of writing stuff down forthat previous people move whatever theylearn and we just take for granted youknow we start from there so even nowthat work is done we haven't approvedit's still the same okay you give thereinforcements and then try to get itdefinitely new stuff and it's verypainfulhow many of you have dogs are traineddogs yeah it's pretty painful like manypeople think maybe endearingly that mindis a dumb dog as if there are someingenious parts mucking aroundthey are done compared to our etiquetteand I am really been a long arrogantsense just because they are humans wehave a bunch of additional capabilitiesand if we are at the same level as dogsthat's basically losing very bad for usokay anyway so reinforcement learning iswhat we're going to be looking at so asI said we still have an MDP and as Isaid when I say MVP it could be astarter it could be main facts youshould think of all of them just to bespecific and think in terms of degreesof course which has a set of sails setof actions a transition model of ourculture this is what MVPs if I ever giveyou this you can use the value iterationquality once iteration are TDP remembera TDP you know their online messageonline decision-making parenting pleaseany of those algorithms to figure outwhat's the optimal tension to make abetter decision to make my question is Idon't give you any of these essentiallyI put you in a state you know anyone youhave a so you know which state you arein currently okay and then I will sayyou know you're not given either tiaracan you learn something about this MVPespecially the DNR functions such thatonce you do it you can then do policyevaluationokay that's the enforcement and again Ikeep repeating that this could have beenthought of in terms of a star search -or in terms of in ice cream essentiallyyou are trying to learn the model ratherthan just do the search okay learningthe model in this context is no means interms of this is key SAS - which will bec s in the case of a star search andthen I'm sorry no ESS - is childrenelevator functionfor this research and then Alice Alice -is the cost for this research I'mpushing this because I wanted tounderstand that it's like again I wantyou to stop saying I'm doingreinforcement learning or MVPs withdeterministic actions and it'll involveenforcement earning Forrester search andin fact if you ever find in other thanfor one of these it should be aneconomic level for something else - likefor example I mentioned activity in theclass in the slides there is a versionfor a la fiesta which is the version ofart in GP which just does no expectationjust max because if you're doing insertit's just you don't actually get asingle state think it charged it okayand basically the ideas are party a starconnected to various a party is what youneed it's a safe there are no wellconnected yes the big difference is inthe case of learning we are trying tolearn these models okay now of course asI said they studied this and they try tofigure out that animals try to learn andin terms of reports there are someinteresting things are for example theselearn near optimal foraging plant in afield of artificial flowers withcontrolled nectar supplies they justfigure this out they go in a new fieldand they still figure out which is thebest way to how much time you spend onwhich plot okay so be brained is not asbadessentially it's learning something andthen of course here this bees have adirect neural connections from nectar wetake measurements to motor planning areaso you do the thing that gives you morereward raise your hand say and thereward has to be biological ability forbees at any rate it's not enough formeans to be told in a way notice that inthe librarylook in the catalog which says these aresupposed to have fun when havingselected those kinds of is done becausethey they country and then of course wealso they are not finding any happinessin life there's a bit deeper squeezeokay okay so animal origin is like thisso the question of course is in thebeginnings of psychology one of thebiggest questions is how different ishuman learning from this kind of animallearning and in addition to that onethere is a protective schema BF Skinnerwhose name I mentioned and he was goingto behaviorism and basically essentiallysaid human intelligence can also beunderstood just by looking at theirbehaviors and also by setting rewardsand resource also to some extent we donew reinforcement learning but we alsotend to have a lot more stuff obviouslyI mean you know we also have knowledgeabout the world a lot more knowledgeabout the world that either has beenbuilt in are we can really even if wehave actually experienced you understandwhat I'm saying that makes a huge bigdifference you would have to tryeverything to understand that it'sneither good or bad for you you couldactually learn from other people'sexperience and those are those thingsfind that making a big difference forhumans but you know people did think interms of using reinforcement learningindicator of you know I mean you must inhere in reinforcement learning of coursewas oftentimes used in the context ofgames and this is not surprising becauseof crowd the most reinforcement learningalgorithms work even now for games rightand all of you know that this talk aboutalcohol whatever etcetera why is that itturns out that if you do not if you'retrying to learn the world by behavein that world you can die imagine tryingto learn how to climb Everest by justgoing after Alice no reading books nosharpest just go and try it out and youdie and hopefully the guy behind you cantake notesdon't do what that's typically thesecond guy can do slightly better youunderstand what I'm saying butunfortunately as much as patriotism allthat stuff is facts I won't we let ourlife we don't want to die so that otherpeople can learn because about that okayso in general trying to learn by doingis the last result if they are learningeverything actually by doing we won't bewhere we are civilization on it okaythere are kisses we're doing desertkingdom way for example doing is playinga game right you know what I'm sayingand so this was really a great thingespecially if it's a computer game it'seven greater because this is a simulatorfor the game and you can try out allsorts of strategies many many many timesokay and this is the fact we talk aboutis again but this issue the biggestproblem with reinforcement learning isactually having to act in the world andyou want to learn as much as possiblefrom many few times you act in the worldbecause if you not too long then youcannot plus also it's very very wastefulokay so keep that in mindbut anyway so that one of the firstsuccess stories is backgammon whichessentially by itwho still alive who's actually hello andhe's at IBM and he basically did thisreinforcement learning based approachessentially you know for learning toplay that game okayto some extent it's a give yeah givesour opponents it's not a cure maybe butas I said there are connected by a starsearch we max and the TV they areconnected so it's not too surprisingthat you should be able to use reinforcelearning techniques for games althoughwe won't go into it right away okay sothat gamma they actually called it II Dgamma and TD stands for temporaldifference we will talk about what thatis until later and you know basically hewas able to show that exactly one glasslayer very fast this was 13 2530 yearsback alreadyI'm actually a lot of it is without selflearning through simulator that's toaccelerate plates so you could use selflearning up climbing by trying to climbthe Everest but oftentimes most peoplewill just die after the first few timesand so and then whatever they learn justmostly them but if you have a simulatorfor time you know going up the arrestmany of you probably tried to play GrandTheft Auto kinds of games and perhapsforget to learn driving which is part ofthe reason we have so many accidentsyou know if you buy then you get upagain imagine if that would happenoutside of Grand Theft Auto okay butessentially if you have a simulator thenone of the greatest things in simulatoris remember this word we talked aboutand got itnumber wars are not I've got ityou can't I that means you may reach astay from which there is no return whichis not Malik on that okay but you willsee you laterthen that happens is we stop thesimulator if you why you can't okayhaving simulators essentially allowsspeeding up means you know trial trialand error methods now when we thinkabout simulators for things likecomputer games there are computer gamesthey have their own simulators from theday one they were simulators mostbeautiful things don't have simulatorsget a security that you have to spendthousands of monies we construct asimulator okaythe very first flight simulators arevery costly to build because basicallythey necessarily get you into this thingwhich will also have the exact kind ofmotion sensor plus so that you will feelwhat happens in the GS and everythingand then you try to figure out how tofly the plane and still it won't becompletely activated nASA has huge windtunnels how many have you have seen BigB tunnels NASA have some of the biggestreturners victims are too soon it whathappens to trees like rockets and theymight apply in the in in the beat youcan have some camera you know equationsetc but really they're the models arenot good enough that the simulator isthe best way and the thing one of thoserelated extremely costly computersimulators on the other hand there theyare already so you just find thatkeep that in mind that the generalproblem with reinforcement learning isthat when you try to act in the worldthat can be very costlyyou might learn but you might learnafter large number of wounds and may bedead okaybut if you have a simulator that you canavoid some of that it's a computersimulator you can avoid a whole lot ofitso somebody for example put thisnonsense but something about these openeye stick cigarettes don't get mestarted on them I try to learn I don'tsee it's it's interesting in as much ashow many of you know the game of lifeokay stick figures learning operation inRL is as interesting as give up flightsgetting into interesting considerationsof survival it's kind of interesting butwhat's the application of it what do youdo is in the world with that kind of youknow if only how tell Vivian are 10trillion I think in this particular isfive hundred million iterations wereneeded for them to figure out a few ofthe strategies okay that actually showsyou how costly it is to learn with youknow just by acting in the world andthis was the world where they don't haveany life and we they don't back in thatI they restarting it you don't have thatadvantage okay if you live in writing abook saying this is the stuff thathappened to him and then we try to avoidit thing that's about it okayso keep that in mind so remember the theproblem itself is trying to learnELR okay and the things that we try todo is objectives of reinforcer learningare given your effectors as percept usand the warden you know the most generalis the world in there are tooth and clawbasically you are actually working inthe world if you are working in asimulatorit's like a made-up world that's niceokayand then sometimes a simulator so youget a Garissa T and 10 repeats everysource if you have that then youbasically are using that to learn toperform well by doing the rightpositions at the right steps and youknow that if I'm giving you PL are youcan figure out what are the optimaldecisions that's what the last fewclasses last 1/2 process while nowyou're trying to learn T and R and youknow the simplest way to learn bastianaand then learn how to perform that butone trick that will find out is actuallyif all you want to do is to learn toperform well you may not necessarilyneed to know TLR if it just directlylearn the action that's something thatwe can do and you see that happeninghereso this basically the way you can dothis might involve the most obvious casewhich is learn the MVP model we learntin our directly and once you learn theTNR then so learning the transitionmodel learning the rewards and once youlearn them and that worked basically youcan avoid that state you sent everyoneand you write in your needle book sayingI've had this reward okay so if you havelearned the rewards in transition then asolved problem thank you guys once I notthe entire exam so I did you strictly aproblem already sound that by the way sothere is this this is kind of thing thatwe do in our subjects like we basicallybook a new problem and do something sothat it gets reduced to an old problemso figure on tiara then this becomesreduced to MVP some HP are very job okayso that's the most of this thing and wewill look at this but there is alsoother way which is if otheryou used to figure out what is theoptimal action to do then you can justeither love directly the state valuejust learn which states are good whichtapes are bad okay and you can learn thestate review either by learning the VRSah by learning QRS a remember that youtalked about in the class if you if youdo not you of SA like say if you learnQoS a for all actions and all states sthen I would argue that you know what todo in any state in any given state youpick the action a and so we can actuallythat means Q of s a the highest for thatstate suppose that state has actions a1to a4 they're the two values of a1 to a4you take the max of it and whichever isthat much the action you do it's justone single packsno thank you just maxing okay so I couldlearn that and that's basically the ideaof Q learning this is what you have tolearning and a connected idea basicallyis he called PDin both these cases you'd never actuallytake the intermediate step you don't getthe DM out you just go directly to theanswer it is actually possible in somecases that pretty much the right thingto dookay if you and then the final thing ofcourse is you can try to search in thespace of policies themselves and pickthe best policy and this is not policyiteration this is kind of starting witha policy and really improving it andthis is basically we won't talk muchabout it but this is also a very usefultechnique it's found on the policygradient search and typically in fact itonly makes sense in the cases where thepolicy makersin a non-atomic a factored form afeature based representation of forcingbecause that's how you can generalize tomeet the boxes