but if you don't understand this you seethis happening everywhere around you nowkeep this in their mind and start seeingpeople are able to predict x-rays fromx-rays before I have to predict ratesfrom faces people are able to predictterrorism from faces what exactly willit be is very passive theory connectingthe input data to the output no can youlearn even will there is no causaltheory yes that the society want that tohappenfind for Facebook predictions not findfor gender prediction not find forrecidivism prediction which is anotherplace where basically it already gotinto bad repute back in in furtherpeople would use this actually featurebased learning system not even ahigh-dimensional one to predict whethermark somebody is likely to go back to alife of crime if they were to be leftout and the judge is basically have todecide whether to be you are whether tobasically put you back again dutifulusing this particular piece of softwarefor not point software not point of oursystem and if you are courageous we arenot killing it just using that as one ofthe many many inputs because often theyjust want to keep their jobs if theyjust say we are using a program they'llsay instead of you the program sits inyour chair so they'll say no no we'renot is no problembut when you actually look at theirdecisions the decisions are basicallyhighly correlated with water program setand this program essentially has nopuzzle theory of what actually causesthis reason and it's been shown to havea high amount of race bias so forexample drawn some rocksgood old by boy it's not that somebodyDP some purpose it is it just a learnerthat learn the foundations tested and inthe training laterand then just hop right into thedesperate okay so you have to keep allof these things in mind so in thehigher-dimensional past learn featuresmay not have any meanings to the humansin the loop typically veryhigh-dimensional256 by 256 image will be having 65,000pixel intensities so 65,000 diamondsmall input is that and I wanna write 2xon so you write in big letters in yourthing did you know networks Moodle it'sokay please understand the differencesthis is reasons nobody uses them theyhave only used for play emojis if youwant to generate hollywood-stylepictures we please for K not even foraviary but at least for K and the village will have 26 point infact whether okay because the intensityis typically splitting to RGB componentsfor our images so it's 3 times 65,000okay and for the full image it is 4096by 2560 x 3 that's twenty six point sixmillion inputs you need a learningtheory that works for 10 inputs as wellas pretty 6 million inputs we have sucha theory as we figure out for the restof the class you know what we have ismostly a theory that sort of works forthis and we're trying to figure out whatactually makes sense for highdimensional energy to the fact that somepeople often times because this highdimensional tasks are nobody's goodquestion whether or not you're doinganything reasonable because this isworks it sound next you must be reallysomething right so people could take itfor example you here is a way you canconvert boolean gate classificationproblem into a high dimensionalclassification problem take the pictureso input this iPod equal to zero I didit over there fiction if you take it forpizza a dimensional picture that is 365layers at that point of time you havedeluded yourself magically to thinkgreat stuff is happeningand if you think this is a joke I canshow youpeople's being written in the state ofthe heart conferences where people takea picture of a set of logical formulasand fill it to the neural netwhy because nobody can question thatyou're not state-of-the-art some of themsay okay I'm hoping that at least youwon't do that just because we have apicture doesn't mean that you shouldtake pictures and feel that becauseoftentimes the concepts that we haveactually are low dimensional and it'smuch better to do the learning in thatspace when you have a theory for a spaceuse the theory instead of asking thelearner to figure out if you also knowhow to figure out learner will figureout calculations and then you complainand so this basically is a fear way inwhich remember the beginning of theclass we talked about Kalani's revengeand art acid versus explicit knowledgethe connection here is these typicallyare used for domains where there isexplicit knowledge these fantasticthings that work for that sentimentswill also work for explicit but theywould do things that you have no ideawhy that what they're doing other thansaying yeah it looks like it's workingso if in fact I was able to give yougreat and not too many of you complainbecause I didn't tell you that I trainedme Holocaust glass mug shots to figureout their grades and you start to giveyour grades you will get used to itespecially because I find up repair theground I said range can be anything musthave taught me some divine inspirationand this must be migrated easier I'msaying but then if I tell you I took apicture then you will yeah especiallyyou would say I'm back there I didn'thave good hair okay so any of you whowill need to leave please leave I willcontinue five thirty more minutes sothat we must have one[Applause]okay[Music]so so this is a 70% of the class whatleft is the 30% Bath's what's a4-dimensional tasks so now given that wehave talked about how to evaluatelearning algorithms now we can talkabout the whole stuff aboutbias-variance tradeoff which we talkedabout earlier which is this compressioncar we do the compression already okayand one of the things we talked about atthat point of time is as you drive downthe training error in the typically thevalidation error will reduce but youstart raising other test level of thevalidation error introduced and willstart racingokay and when that happens you areactually overfitting to the noise in thetraining data this is what we talkedabout this is the traditional learningtheory with all fit okay so the questionis if you assume so if one of the funnythings about your networks is you canuse them for this as well as this butfor this pretty much the only game intown is deep learning right now very thevery high parameter intensive neuralnetworks those are the only things thateven work for these nothing else seemsto work right now it is as of now butfor this there is a whole bunch ofnormal classical machine learningalgorithms honest accreditationthe Gentry's statistical learning etcexcept our government works so andpeople think our networks they basicallyuse they would say deep learning wasused to learn boolean functions becausedeep learning is what gives you jobs butthe problems that we talk about thelearning here are different from theproblems we talk about it here here thedeep learning are the material recoverywhat's exactly the same way any of therest of the machine learning works whichis essentially it will have thisbias-variance tradeoff and in particularthe v2 actually the way to actuallydecide how to you know one of thequestions you have in half which youthen I could be a mean idiots how manynotes how many connections this is theusual stuff people tend to startthinking of when they think of in ournetwork learning right so if you aredoing it in the low dimensional spacethe usual idea is keep track of thevalidation set ever and if thevalidation set error is first comingdown and it's increasing as you keepgiving more examples that means yournetwork is probably overfitting if thevaluation is not coming down and it'sjust sort of fluttering in a certainlevel that means your network doesn'thave the capacity to learn this functionwhich normally doesn't happen in neuralnetworks if you essentially have onehidden layer which is wide enough thenyou can learn any function but if it'snot wide enough that it may actually notlearn some functions with a given numberof examples so the engineering advice indoing this would be if the training lossis too high then maybe the network sizeneeds to be increased so you are unableto even drive down the training setnoise then maybe you need moreparameters that can be tuned such thatyou canI have a closer to zero if you increasethe size of the network you increase thenumber of tunable parameters which insome sense increases the number offeatures okay and so then you canessentially this is what you had in mindwhen you when you said if I try to fitthe gender I mean if that the grade tothe face of a person I would be able tofigure you do that this is possiblebecause they're just enough trainingrooms you will be able to find a way offitting it okay and of course in thosecases basically the question isespecially in my things are featurebased learning if the training loss doesgo to zero but the foundation last fastreduces and that starts increasing thenyour network is overfitting R ismemorizing the data so it's not going tobe doing well on the test data yes okayso loss is basically the ever okay andaccuracy is the opposite of it okayclaiming loss is the error on thetraining set you see back then I'mbasically I'm using the training errorto actually change the waves but anygiven point of time I know how much everI am how many training data samples aremisclassified that's the training lasterror Athlon improvesogresses if you have enough capacity youcan have training class to become zerocapacity means think of having higherorder polynomials okay then you will beable to fit any random set of pointssuch as a polynomial goes through allthose points so the training they talkand eventually to be made zero if on theother hand if I'm only looking for linesthe training and my data is basicallynot a straight line data then trainingclass will never be zero you guys get upokay now validation is essentially thesame thing exceptthe validation set okay so undervalidation loss we fix faster using themsignificant that the network isoverfitting I memorizing in fact is youneed to reduce the size of the networkin you select a cell network keep thenetwork size the same and use theregularization such as dot method whichI will hopefully mentioned before thatpasses over okay but this is far sort ofreducing overfittingokay yes there is a question yeah Julielaughs to memorize the validation sendit take a little longer to memorize thevalidation set is memorizing thetraining data in fact one of theinteresting things that you know if youhave sufficient capacity then thenetwork can learn anything in the sensethat it can drive down training dataever to zero here is an example of thatif you took image net data okay whichhas two hundred classes the pictures arefor no dogs cats human being they'retrying their 200 classes no it hasn'tclassified 10,000 process two hundred ofjust talks which is a problem with theimage name that's why it mostly imaginesdogs you know it's going on just like wedream a few months imagine it lots ofgaps okay how do we not dream so manydogs and cats in your mind because youmostly focus on humans anyway so in thecasewhat for the same one person yeahimagine a thousand classes right and youknow they've got the actual things inthe imager and you know that terminatesour water which we'll be talking aboutare able to learn them and they're ableto get up to 97 99 person accuracyimagine the following Chris imagenetcompetition where I take the data eachimage and give it a random label Sambathousand labels so I take flop say yourcat and I take this person and say youare a fish and you said somebody elseand say you are a novice can husky yousomehow don't you think that the stuffthat we are learning or learner willbecause there is some kind ofirregularity in the labels for exampleit is unlikely you think that a labelingscheme which says I'd say I'm human butthis person is a fish is probably notlikely Dimitri because how humans willkind of be closer you give this ramdomlylabel data to the neural network it willlearning it will completely learn it inthe sense it will drive off the trainingdata error to zero that means at thatpoint of time your network is a nice wayof remembering mnemonics essentially soI start telling you acts used I starttelling you yzw etcetera etceteranow I forgotten what I count the firstguy well they're not worth a moment'sthat has nothing to do with learningthat's just rememberingthat's just memorization and because ithas high enough capacity it can runpretty much any arbitrary labels okay soso this is some of what you will findout doing in the case of capacityfitting and if you're using copper thenyou can always start with a largenetwork and have less worried aboutoverfitting typically before drop out bythe way drop rod is one of these thingsthat I should mention as much becauseit's protective now we sold out peoplewhat they will do is they will dooptimal brain damage which is a verylarge network start removing someconnections and retrain it and if you dothat you can reduce the overfittingyou're essentially reducing the capacityand seeing if the network is able tolearn the concept with less capacitythat reduces that has a better chance ofstopping overseas but roppa basicallydoes that for you behind the scenes okayso that is something that we can do interms of ensuring also whether the othername for optimum brain damage this isold name this is before 90s now youshould say a destination after 2015 adistillation essentially is taking a bignetwork training a big network of yourtiny data and then use this trainnetwork itself to produce levels forother data and then give this traininglater to bring a smaller network whichhas fewer waves such condition that someof you reduce the number of parametersand you're seeing if you can reduce theparameters and and you know get the sameaccuracy if you could have reduced theor fatigue and B it will also work onyoudinky little cellphone because thesenetworks have to work on yourself andthe smaller it is the better off you areso the guy who just got hired inchallenge on last week last year he justgot hired this year he basically doesneural network compression algorithmsand which essentially I cannot get thedestination yes nobody with nobody woulddo that in general that's if you work inyour back it shows up hard in the backyou see what I'm sayingyou don't improve your exam performanceby not looking at the entire textbook sothe next bit is to break let me justfocus on the first three pages I want towork it but you will over under theextreme bigger wheel here like one pointor something okay so that's now nobodyever if you ever think I have a brightidea of not using training data makeyourself up okayyour job is use the training data itshould change something else data ishard to get why would you not use itit's like textbooks are hard to writesay why could you not have written asmaller network I could have made thebigger money for a smaller textbookmechanisms okay what's a good questionokay finally gets us to the interestingpart of today's lecture which is yesbasically the way to talk about is itlearning something or not is these thesecurves touching there is no additionalreach uncle who can tell you that youcan rest easy genitals have beenclassified if it does not make test datamice go down to zero that basicallymight have been because it doesn't havethe capacity are because you chose notto go out then it down for whateverreason okaythere is no simple answer tragic learnthe other families that's the point I'mtrying to get you to understand there isno better answer to have you learnedthan your source under test however badthe test is because LaMotta me is notstill allowed he used to be allowed ordays but not I know what to be doesn'ttell you a thing you know I try to openyour brain I won't know what the heckhappened if I opening a watch 350 layernetwork I have no idea what it's likethis is the only currency that you haveso that leads us to the last part oftoday's lecture which is omega R whichis a that this stuff that I just talkedabout so the stuff I talked about aboutcapacity versus all-city essentially isusing the theory of learning that's allat once in the traditional learningtasks the typically interesting for thehigh dimensional learning tasks which isthe teammate recognitions and so on isin the traditional learning theory saysthat if you drivenever baffles you know you will be worthvideo so you should stop before thetraining error becomes zero but if youactually use these things on highdimensional data typically with calmnerves which have this particular typeof network next class but an image datafor example which is a very highdimensional data nobody in the rightmind stops before that they read itbecomes zero in fact that is not onlyshould you make the training it at zerobut continue keep on training even onthe training it has become zerook train together to become observingand I'm sorry yeah so so water basicallymeans is that that's it so in the wholeidea was to stop overfitting try fittinga order polynomial instead of the 350thorder one so keep the capacity of thenetwork low so that it won't or now whatyou do you start with very largenetworks oftentimes networks which havemore tunable waves than they restrainingit so you could have like 1.3 billionwaves and it will be a training data thething it when I trained it has big butthey're 1.3 billion waves you can learnany random thing any random noisepattern we can learn in that case so thequestion then is we are starting withvery high capacity networks and they infact we reach zero training error veryeasilyshould they be overfitting they must beperfectly am I am usingthat's where we arelisten to what that's the part ofengineering okay a hand yes that's whatit isthey may well be more fitting so thishere be the dragon slide is from a talkin like a couple of months back so as Isaid we once we start getting into thisstuff you know basically much of thistheory is just not there people aretrying to figure out so one candidatewhich caught my attention are you knowwhatever contributing it might actuallyyou know survive it is really this workby Misha welcomehe actually had a paper in ICML 2019 andin fact you can get in that particularpart I thought that he gave thatparticular link you know anything theslides you can click on that theinteresting point that he makes is thatin a sense what actually happens in manyof these high dimensional dataempirically speaking is instead ofhaving a single you bias-variance carthat means as the training error becomeszero pass the test and reduce and thenstart simply see if you continuechanging waves beyond this point thenthis is basically called theinterpolation point at this point wehave sweetie ourselves every trainingdata point so interpolation would havehappened for example if I have pointslike this with some points are rightthere not go that interpolation and it'sa perfectly fitting that indicator theyare continuing beyond that point most ofthe time no network training for thislarge-scale networks continuously atthis point when it continues we at thatpoint what you will findshould be the training the best everwhich have increase in okay and this isin the very bleeding edge of ourunderstanding of what they talk aboutactually what's happening doesn't makeany sense because most of the networkshould be already they are in some weirdsense over fatigue but this particularway what is happening is you did startwith some test data from the samevillage netting the business you keepsome number of pictures and youpredictor they kept we kept them faraway and you train your network withjust a training data and it kept shakingits accuracy on the test data that's faraway and it kept reducing what happenswhen you do that is then you do thiskind of interpolationoh these points are getting labels thatdon't make any sense in somethingthey're not on the flight they're notsupposed to memorize that noise youshould not have learned them but youlearn everything but if you learn themand so it's very if in fact this youwould expect is because of the noiseindicator noise in the labels that meansthese points are not supposed to havebeen there but you wanna put in thereunderneath those points to what thatmeans is that you are essentiallylearning the noise in the data becauseof which you will start misclassifyingnot just these points a whole bunch ofother points for example a point thatwas one in this area now who'd get somerandom flavor like that okay but thatdoesn't change the test ever because thetest data was selected beforehand withthe same distribution what it doeschange is this so when you actually gofar we arethere's some interesting things that Iwon't have time to get into but whatwines are happening is that there are somany features to reduce the trainingdata has seven that you will startpicking better and better features suchthat the beliefs associated with thosefeatures that Muhammad of those waves isminimized they become smaller as beforeso this becomes a smootherinterpretation interpolation than in thebeginning so as you go forwardessentially you are doing better andmore smooth interpretation but the thingthat this leads to is this interestingpart which is so how many of you haveseen this stuff I may have mentioned itagain this is completely connected itallows for dance set of dance set ofadversarial examples I never told youthat there can be a presenter examplesmachine owners are supposed to justassume that from the trainee intersectwhat weak from the same distribution butif I give you trick questions and makeyou fall flat on your face and a jump upand down in green that's possiblemiss Aronson except if you werepositions find up being life-and-deathdecisions such as I basically give you astop sign which looks exactly like astop sign with a little noise and yourTesla says this is squad 65 miles perhour continue at 65 miles per hour soyou go because you're gonna stop andthen you'll be under the track livelater these are called adversarialexamples so what is happening in thisparticular business is here is robustI added some nice to this bus just afraction of this noise such that I getthis which is nice which you alreadyknow rightsimilarly there are some toys aretreating it right now the interestingthing of course is this possible thatthe world is full of ostriches but Imention this to you