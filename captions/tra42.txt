[Music]okay so let's let's get started so againa reminder that there is no cosobviously on Tuesday and today would bea big day on on Thursday and then wewill send you more information regardingvictims such as the like an example asample paper acceptor reporter anyquestion before I get started okay so wewere the other is a quick reminder ofwhere we were we were basically a classclass so in the last month we werebasically changing we started to createusing the lifted representations that isrepresenting states in terms of featuresand that got us to start talking aboutgeneralization we are a voluntary longdiscussion about downand its discontents and we talked aboutmultiple reasons why you want togeneralize the value function if interms of features or in terms of notenough training data and in terms oflearning and of course the most obviousone is memory but that's the leastimportant thing actually the reason yougeneralize in reinforcement learning isjust because you can generalize it muchless amount of data and so you can learnfaster and then we talked about featurebased representations approximate ideaof how they will apply and then wetalked about this exactly if like that Iadded after the class to summarizeeverything that was said without eslintin the class in particular that i spenta bunch of time trying to explain to youthat generalization is impossible if youdo not have if you only have atomicrepresentation the whole entire thinkabout if you guys are represented interms of your social security numberthen I can't generalize anything fromthere okay I have to represent you interms of some features and and then Ialso pointed out that if you havefeature basedlifted factor representations then wecan start generalizing but it alsoactually means there is a for entirequestion of who decided how manyfeatures of suppose we put four statesand in fact if I bombed our for decidingthat the only thing that I care aboutyou is just you know the marks in thetest in the various marks in a test thenin essence nothing I could conclude withtwo individuals with two very differentpersecute numbers who have the exactsame marks in the test so this is calledstate aliasing so what what are twocompletely differentin the real world became the same interms of feature representation okayand this is something that will happenin general then you have diversitylimitations now there is a discussionabout moon does come up with featuresfactors for the stagemostly in the beginning we assume thatsome designer comes up with the featuresfor the state and in some particularcases such as animated technician weeklylearning you may well essentially startfrom really completely raw data such asthe pixels and then try to learnfeatures from there but that is actuallyonly true for some specific uses such asimages are toys for example if I have todecide something about your credit scorethen I need to look at your entire lifenot just a single picture okay so otherthan feature based because addiction theonly other way I could really start fromscratch is to have a spacetime cube ofyour life essentially images taken inyour life every millisecond and then usethat to figure out what should be thereal features and that can be done intheory what is pretty much impossible inpractice so most people who basicallysay they use for example things likedeep learning windup using hand codedfeatures that is some designer gives thefeatures which does bring up thatquestion I think somebody from the askedlast time as together then we have thespecific features you use themselvesmight be introducing biases that isindeed very much true okay it's aparticular way you decided to look atthe state so if I decide to representall of you in terms of just the set ofmarks and the radius test then that's mybias and that might be reasonable biasfor giving a grade in this course butcertainly that's not a good bias forsome other task okay so we looked atthat too and then we looked athow basically function approximationworks we look at the difference betweencompression and learning both of whichare extremely close but in the case ofcompression what you are interested inis lossless Ness on the training data inthe context of reinforcement learningthe training data vastness lovelessnessmeans I have currently a value functionI want to somehow come up with a featurebased representation and a particularyou know a particular function thattakes the features and exactly predictsthe values such that the predictions ofthe values agree we pretty much everystate so there is no error whatsoever inthe existing value function so ingeneral bachata moves for example twoover and you can work an image into aJPEGin a sense you're trying to reduce theerror with respect to the original pHdata in fact JPEG is lossless or lossythe people no last CV okay so in factone of a nice thing you can do is if youhave very large text files jpg the heckout of them right and that would beextremely noisy but treats like zip andyou know basically our losslesscompression schemes and if you try touse things like zip on images in essenceyou don't get much of a compression howmany of you have tried to zip a I had animage and found that it increases thesize essentially because there's youknow the way in general compressionworks in things like text and so onessentially it considers the fact thatthey're minimallythat are being used with highfrequencies and the idea of is to put abig cold table up front say the mostfrequently used word gets the sharpestcoat so in English the will get probablylike a single bit code okay and theleast frequently use once they getlonger pores and if you do this then inessence wherever they suppose with the Iwill just put this one duty and thenduring the compression you go back tothis core table and say oh one wins thatthat's how you do compression okayin general for things like language etcthere is this RFID petition so returnsare in the space of words you canactually do compression but in theimages that doesn't work becauseessentially the pixel values there is noobvious way in which they repeat so infact you mind up to the clicks like JPEGwhich you won't get into here but anycompression of them can be seen as somesort of a do all of the learning of themand it's doing to put a compression thenit is actually a bad learning algorithmbecause learning is about future notabout past okay so other theirconnections and that's why we actuallyspend a bunch of time about thistraining it hours ever on the trainingdata versus error on the test data andas you drive down the level of thetraining data in many cases the error onthe test data starts increasing and thatis because that's when you say you areoverfitting to the noise in the trainingdata that's what we talked about theoverfitting and in fact this particularidea is called this phenomenon is calledbias-variance tradeofffrom statistics okay - and then ofcourse we started talking about doingthe actualfunction approximation with value is alinear weighted sum of features with theweights of the Thetas you know that weuse what we wanted to cut them Thetasand and then we want to essentially dothe TT update instead of the we've usedto before is we had a 17 in the valuefunction of the state and you know youwind up doing a transition find thatyou're next to a neighbor who has 95value so based on the TD update you wantto update this guy's value from 17 to 19in the atomic representation you just gointo that cell and cut out 17 put myname now we are not allowed to do thatinstead you have to change these Thetassuch that for this particular you knowthis particular cell value is much tooclose to towards the neighbor so in factyou want to make this bigger size alittle close behind you now the problemof course is in doing so you wound upchanging everybody else's value do thisis where we have this very longdescription which I thought wasamazingly connected by probably you needas to if I don't like one specificperson in this class and I want em toget a bad gradeI can stop changing the weights given tothe different you know assignments untilthey terminate you falls below some cashon it and then say look you have nogreat right so I'm going to do thatunfortunately everybody else's gradesalso change and the question of courseis you can see this as a baccaratfeature if I care about is justbasically doing this person making thisperson get back to it then this is a if on the other hand you know I'mthinking of when I'm making the valuechange for this state somehowthat should change the values of all theother states that have similar featureswhich is basically what we were hopingfor when we were looking at you knowthis miss pac-man thing remember whenyoudespite one thing you wanted to be ableto say that this state and this stateand mistake while the completelydifferent from anatomic perspective areessentially very similar in terms of youknow they're the back - and so so that'sbasically what should you can then thinkof that day in that case if you view itthat way you will see as a featureessentially okay so that's what we weresaying here that when you change thedata then other guys values will alsochange and that's the generalization nowin fact again this is what I like Iwould like to pick up generalization ingeneral as its it can be seen as afeature that one when like I'm gonnageneralize I could actually do the wrongthings okay and this is you can't avoidthat problem in general but you know sothis is what we're trying to do and youknow if you're trying to make thatknowledge of course that we pointed outthat what happened in what we did therewas we may need to measure rewards onewas to go from atomic to literally takestate the presentations other is to gofrom discrete search in a star searchetc we do search in the continuous spacebecause you're trying to find the thetavalues how we change the theta value sothose are continuous vectors and so youhave to somehow to search overcontinuous space and that normally iswhere you get your calculus and then weended it we talked about gradientdescent and we talked about the factthat you know normally you take thepartial derivatives at a point and thenmove in the direction opposite to thepartial derivatives or to minimize theerror and if you want to maximizesomething so in fact it turns out thatone of the other things we'll see is welook at machine learning from multipleperspectives during this course in thispart of the lecture we will be talkingabout loss functions and error functionsand you want to minimize the last youwant to minimize the error likeobviously what part of error don't youand when you think of it should beminimized or maximized nobody wants tomaximize their right but on the otherhand you can also look at all thesealgorithms but from a statisticalperspective and then you start thinkingin terms of likelihoods and thelikelihoods have been maximizedbut the theory is the same you know ifyou have gradient you know gradientascent you get to maximize you dogradient descent you get to minimizeokay so you have some method which issort of the most obvious place to startfromand if you use newton-raphson methodthen you won't need this alpha factorwhich is the hyper parameter about howmuch of a move to make in the directionis what I'm saying that's the learningrate because newton-raphson methodactually uses the second derivativedirectly except it turns out that thesecond derivatives for multi variablefunctions of the one which are the onlyones we're interested inwe're multi variable so the thing that'sdifferent about basically machinelearning data science blah blah blah andthe old calculus that you did is when Iwhat you said multivariable you meanttwo or three variables okay it wasn'tone it's two or three that's it okay butreally most of the time here we will betalking about multi variable where thevariables can be Millionsokay are can be hundreds it can bethousands in your life and it can bemillions - okay terms of number offeatures that you have okay and so thatwines are actually becoming quite aninteresting question in fact evenunderstanding how the intuitions thatyou have about lower dimensions whetherthey even carry out are not too highdimensions is a very interestingquestion okay but suffice it to say thatin this particular case even for twodimensions already f double dashthe second derivative it's not a singlenumberit is a matrix of numbers okayand it's the matrix is the Hessian wouldbe N squared where n is the number ofdimensions and so if you have you knowone dimension it's actually matrix ofsize one one by one otherwise in me youknow - body - and if you have 100dimensions 100 100 matrix okay againinstead of in general whenever you havea division by linear mappers if you havea matrix you take the inverse of thematrix how many have you done in linearalgebra okay so all of that stuff willbe useful obviously so you would wind uptaking you know inverse of the Hessianand multiplied by the gradient thattells you the amount the size of thestep that you're supposed to do soinstead of an alpha you actually have adynamic step size because Hessianinverse will essentially be you knowdependent of the place you are under[Music]function okay so this is what we want todo but obviously it turns out that aretoo costlySdn inversion is very hard for a largenumber of dimensions and so instead wewill mostly play around with just asimple alpha alpha times the gradientokay and then we will consider forexample changing the Alpha once neverokay you use other things like momentumIDs etcetera which all can be seen as ifonly I could do this they were notneeded but this idea doesn't work forlarge large number of dimensions okaywhen we are here my a let me ask youfollowing question so you know threedimensional apples apples re-emerge nowright if you peel the Apple it's long Plet's give it up come on how much happenis left for happen hundred percent afteryou peel let's say how much Apple do youthink will be left like what10% 99% or anything99% unless you have one of those reallybad leaders right what is ittwo-dimensional Apple Apple's arespheres as far as mathematics isconcerned okay so today margin Apple isit circle so if you feel the suck you inessence you are taking its radius andit's some tech using by Epsilon okay soat the volume of the circle is basicallythe idea of the circuit okay so if youdo epsilon less damage to PI R squaredyou have PI R minus epsilon square ifyou peel an apple over three dimensionsin step 4 by 3 PI R cube every 4 by 3 PIR minus epsilon cube you see what I'msayingso if you consider an n-dimensionalapple and n-dimensional apple okay afterfeeding it will have R minus epsilon asitself in radius obviously and beforefilling it out targets radius so thequestion is you have a temper andthousand dimensional apple and youpeeled the Apple how much of the appleis a left one percent 10% 35% 99% waterwhat do you thinkyeah cause it's that possibility what doyou think how many of you have seen that10,000 a virtual Apple what life thatyou are now clear seeing afour-dimensional Apple I'm really hopingfor somebody yeah okay we should talk Iwant to take the same drugs you aretakinglet's see we are stuck in threedimensions let's talk in threedimensions citrix is easy becausephysics outputs factors basically giveme three dimensions maximum okaydata science people machine learningpeople who have to deal with any numberof dimensions but you can't visualize ita thousand dimensional Apple the math isthe same you see that if you were at thesub hill it would have been hot minusepsilon by R squared is the ratio of thearea of the circle after Pele was asbefore B if it is a cube it would bethis it would be four-dimensional itwill be that it will be M dimensional itwill be n if n is 10,000 what do youthink will happen to thisso this is a real number minus epsilonby the same real number so this ratio isless than one if you take a numberthat's less than one and exponentiate it10,000 times what do you getthank you so if you take it andthousands an Amazon Apple did you everget one you know don't ever try to bebecause after the field is no Apple leftdid you understand what I just saidso you think that you have intuitionsabout higher dimensions because I knowabout one and two and three how muchworse can it be that other dimensionsbut it turns out none of us have goodintuitions about high dimensions highdimensional apples are all P and no forlet's say okay this is true for you twoto three dimensions if you're onlycutting the epsilon percentage you knowif you take a splitter off and afterwhich a circular and to paint upsalonpas on a job you'll have more Appleleft there than when you do a threedimensional Apple and forget it to learnyou know the reason I'm saying this isanytime you understand data science weunderstand machinery it's just highdimension calculus teachers up on theback of your head you have no ideaat least not on the GPU sense okay thereare other things so for example hydrogenapples are have been known and also thatin high dimensions the most piece ofvectors are perpendicular to each otherit turns out that is a random repeat twovectors in high dimensions and you taketheir dot product that will be close tozero very high probably again I canprove this mathematically easier whatI'm saying one more thing one more thingof course is as you go to higherdimensions in two dimensions and reallymonkeylike this right until I mentioned thatminimum is like this as well as likethis basically looks like a nice deepditch for dimensions only that kind ofsome features one of the interestingquestions that with respect to thecontinuous search that people havewondered is that I told you there is noguarantee that you will find the bestminimum many functions will havemultiple minima and the innocent justgets you to the one of those minimum somore likely than not you will get alocal minimum not equal minimum so thereare two ways in which you can make surethat gradient descent based search willactually find a global minima which ismake the world B so making the won't beso essentially says I shall require thatthe function I am trying to optimize iscontext what convex means is it only itwill have one minimum it won't have anyother meaning and if there's only oneminimum you will find that only minimumit's the what I'm sayingso many people in machine learning wouldfor a long time essentially focused onconvex learning because that's the placewhere gradient based search we findwhatever finds did necessarily be theonly minimum so that's a global minimaif on the other hand you take a normalfunction as you don't put this convexstate requirement then you are likely toget stuck in some local minimum there noguarantees that you won't get stuckanother way so one of the biggestworries people have in trying to gohouse of convex learning learning overconvex functions is I'm pretty sure willget stuck in local minimaone of the things that you will find aswe go forward is you knowlearning at the multi-layer neuralnetworks essentially say to heck withall these guarantees we do what are thebest we will try to look for the minimumand this into work see to what is a badway of measuring anything butessentially they do seem not to givevery bad answers in general and thequestion then is if there are so manyminima how comemiraculously gradient descent is findingthe good minimum a possibility is thatit's not as easy as you think to havemore than one minimum if you are tryingto have it in one by one in twodimensions right you can basically thisis the way the minimum occurs the onlyway you can so in three dimensions so onthe other hand you have things like thiswhere you have a saddle point here it'snot a minimum but if you have a saddlepoint you know basically gradientdescent can slowly come out of it it's afall into it'll come out of it in threedimensions there are more saddle pointsin fact the word saddle is made for thatright now if you think of a standard somany of you have ridden horses okay soif you have a basically it has to be thesaddle has to be like this so that itwill be in shape with the bars and alsobe like this so that you are happysitting on it right so it turns outthat's in three dimensions the saddlepoint is not a minimum it's a maximumfrom the hostas point of view minimumfrom your point of view did you guys getthis it was kind of hard to actually getsecond it's kind of hard to actuallyconstruct a minimum because there aremore ways you canconstruct things that are mark minima inthree dimensions when you get to tenthousand dimensions - for it to be areal good one it has to be proposed inall dimensions you see what I'm sayingin the saddle point thing it was closedit basically it wasn't closed in thisdirection if you're thinking in terms ofmeaningit was in this direction but not in thisdirection actually constructed minimum the gods who make this function itwould be very hard for them because theyhave to close this in ten thousand waysnow that a thousand dimensions so thatwill be minimum if it's open in onedimension out of that thousanddimensions that would be like Death Starwith that little hole will find its wayout you guys get my point okay so it'snot as just as you know there are theseEgyptians that we carry along and you goby dimensions thinking you know it'seasy to get lots of you know I caneasily make you know a two dimensionalfunctional with multiple uu minimas likethat so it should be just as easy inhigher dimensions in dimension if youtake a pair of vectors they are notgoing to be perpendicular with highprobabilitywhat is many high dimensions they arewith high probability going to beperpendicular in two dimensions you feelan apple in three dimension you peel anapple there is still a lot of Apple leftto eat if you make the mistake ofpeeling an apple in ten thousanddimensions you have no apple left it'sthese sacks of things that actuallymakes you know for one reason that sortof other reasons why hide a marshmallowgradient descent still seems to workokaythere is a question yesyeah if you find out later of course notof course not no okayokay unless you start putting extraconstraints on the function so forexample you can make that one shouldbecome making which kids will have a newone minimum okaybut then after class of functions in theworld convex functions are just a subsetthere are infinite number of convexfunctions what most functions are notconvinced okayyes no I'm just saying that we youexpected to do much worse that theystill seem to be doing okay so what aresome possible reasons these are nottheorems but these are some intuitionsas to why you don't expect in general ifin fact even if they were fusion is thatthe more the dimensions the more thenumber of local minima let's say thatwould be a wrong intuition in fact as anumber of dimensions increase havingminima is hard it takes work okay sothat's so having said that so that's allthat was our discussion about about highdimensional functions and also aboutgradient descent so now I go back towhat we were doing last class which isI'm trying to take you know the state SJstate SJ has some value currentlyaccording to what the the featurefunction stays where the featurefunction essentially is the state SJrepresented in terms of features f1 allthe way to escape and then you arehoping that their families theta 0 up totheta K we multiply these feature valuesthe feature values ofvalues you multiply these feature valuesby this rail and you see at the map thatwill give you a number so you're tryingto then find the best fit theta I thatwill essentially be you know you splitrepresent the value function now whatwould be the best fit it is somethingthat will minimize the error and so thisis where the version that were loststamps so suppose you represented thevalue of SJ using this feature we havetheta which is basically you know likethis and it's true when you attacking meyou should be BFS J then you want tomake the difference be smaller to makethe different smaller money you can justtake the absolute really what's theproblem with the absolute value problemwith the absolute value is that it's notdifferentiable because if you look atthe absolute value function it lookslike geez you can see that and so on atthe point where they meet that notdifferentiate because tell me from leftthere is one slope can be drawn likethis on the slope so differential is notwell-defined okay so mostly people tryto go with squared ever looking forminimize never it suggests me wheneveryou go for minimizing the squared errorbut whatever sweater will be like thisit's okay also you find that half mostof the time we'd like to be minus halfthe squared error what does that meanthat you're keeping the other half forrainy dayyou know if your