okay so today would be the second classimportant and we will start with VillaPark which was with respect to activelearning model based active learning andthe need for explanation that's when westarted that's why this part we willcontinue from there then talk about acouple of models of difference learningThank You learning and tilaka of a bunchof important dimensions refer to anotherand keep thinking about personallearning techniques and that's more orless what I am hoping to do today sobefore I get started any questions okayso this is by the way the currentsnapshot submitted they nearly like thatcan pass through is most places soclearly TS are not having a life andthen you know these are some of thepeople from the student side whoactually providing useful contributionslike thisbut I think some of them that we arealso providing distal contributions soyou want to be in this list okay that'show you get your a classic 157attributions the hanukkah will also getme but I think he already has one so itdoesn't matter so that's basically thecurrent statistics and that so you knowI keep repeating this in the sparse thatone of the most important things I hopeyou guys learned from this class is notto say that I'm using that publiclysolutionkeep running when what you are doing isa single agent from the observablescenario with what you can get by with astar search okaythere is no advantage in kind of takinga tractable problem and utility may betractable Angolaokay that is you get drops but but evenif you get the jobs doing that pleaseunderstand the intellectuals arechildren born is trying to makesomething simple look unnecessarilycomplex okay by all means keep yourprocess and get your bonuses but don'tcheat you ask thatdon't delude yourself and you know it'sreally who's doing this kind of nonsensewhy am I so worried about it I give yousome examples you might I might becoming at rather the beach party pooperyou know why can't we just get on withthe deep learning deep reinforcementlearning alreadythis is a paper in nature that actuallycame just of this bag and these two guysbasically saying we can buy the riceetcetera which also are published innature not a lot of publicity because wewere using deep learning for predictingaftershocks what's there not to beimpressed especially here in Californiayou know you want to predict after shotsfor earthquakes and if you're using deeplearning that hospital stuff right andnature of this accommodation is and thenessentially you know these are the partypoopers city that essentially first ofall they clarify that the same authorswho wrote that other paper wrote adifferent paper in 2017 with the shortthat I'd actually his problem can besolved with no hey I'd like whatsoeverbut then you solve the problem withoutthe Iowas interest right okay so theyfind out you know 2070 paper clearly didnot go to nature because nature and theywound up publishing it in somegeological only networks are somethingyou know nobody DNA that's importantreads that stuff right and then insteadof course if it is hoping that with deeplearning with only thirty thousand fourhundred fifty-one weeks it's not eventhe reality deep learning okay like amodern herothey got their AWS account they got youknow like gonna start chaos and let's goand then these party poopers essentiallypointed out that they could have done itwith logistic regression which is usingtwo parameters instead of fifty thousandfour hundred and fifty one parametersright the door they actually saythat modification you know is actuallyperforms better than these are the kindsof people who won't get papers intonature there are other causes alreadygot a paper in natureokay so kind of keep this in mind thatopposite is a height and the height isnot just in industries it's also inacademia nature is considered but thebest journal to publish and you know andin fact most of the time recentlynatira publishers started throwing someAI buzzwords and remember I already toldyou about that thing about the melanomareserves okay the keys they in fact youknow just basically pointing out thatthe system's the winery is solar systemsare confused by martyrs here thedelusion is self delusion because theresearchers deluded themselvesand of course nature the unit itselfokay so just an example but this ishappening like right now which is why Iwant you to kind of keep track of theproblem you're solving a rather try andjust say can I stop chaos right now okayraise your hand if you order okay yeahthe rest of you know if you are religionin college you should know - good boyokay good boy was this familybut earnest actually who make theseamazingly complex contraptions forsolving extremely simple tasks so forexample this simple contraption whichwould be publishable in nature of itstime I guess is a contraption that wouldtell from survives to remind him thathe's supposed to mail that letter hewrote for his wife who is right okaynotice how simple it is as you walk pastthe cobbler shop who strikes thesuspected would be causing it to peakfootball see through the goalposts befootball drops into the basket II andthe instructions we have pillssprinkling can causing water to soak orthree is at and the quartering stringsand opens the door J of cage allowingthe bird can't walk out on the porch Land get that one M which is thenattached to the string and this poolstuff that we crochet go on which iswritten you shall make that letter okaythere is a lot of Newport but they arebeing done in industry I hope you getsome of that money you know I meanyou're taking my class I should getthose avenues to but be aware of whetheror not you're solving the problem withthe right kind of technique this is agood boy and this of course sledgehammerto break eggs and this is the voltagehammer ai and you want to be aware ofthatokay that second uptick that we shouldkeep in class okayrecap from last class last class weessentially were doing what you knowthis model based when you conducted aneconomy and then we did it files forpassive so basically ADP estimates thetransition function T SAS - so Maticabout directly estimated the valuefunction okay here you are estimatingwith the transition quantityfrom the traces and uses this transitionfunction to evaluate the policyregarding the policy it of course usesthe bellman equation for the policyevaluation which just says that withrespect to policy by the value of thestate s is the reward of state s plusdiscount factor times the expected valueof doing the action a which is equal tofive years that means whatever is theaction that that policy said you shouldbe doing in that state you did that inthe state s you got to state s - theprobability of that happening is thisnumber times the value of the state s -with respect to this policy PI right andyou have this Sun basically when you dothis time if you have ten states s 1 tos 10 then you will wind up getting 10equations you do end up getting 10simultaneous linear equations and yousaw them you will get the values foreach of the states with respect to thisfalsified and then you do this inessence the problem that Montecarlosearch had which was not taking intoaccount the correlation between statesthe value combination between states etcis our taking care of right nowright ok so that much we did that weunderstood that but then we also put itout that if you're anyway gettingtransition function P ostensibly to usethese equation you put they shall haveused instead this equation which isusing the same transition rather thanjust do what the current policy isasking you to do in the state considerdoing every possible action and becausewe have the transition function you cancomputefor every action in this expression likewhich is the expected return of doingthat action it and you take the max overother actions and then have that 2 RSyou will then get a better value isbasically in some sense it's like westar except we'll call it V V V G thisis the green value corresponding to thetransition function you have and this iswhere I told you that essentially giventhis transition functionus are actually able to do this policywhich is at least as good as be fineokay and you got it free might as welluse it you see what I'm saying now ifyou are doing this then the essence it'sessentially not only are you once yougot this V G right you pay them at whatthe VG into Phi which is the policy V inthe PI G you can convert VG into PI Gright once I give you a value functiondepends on what the once a functionwhich is basically for each state whatis the action that is actually giving itthe maximum expected value back okay nowby G is going to be your policy you'regoing to act with this so we put youinto these rooms and only use Phi butthen now you have figured out a betterpolicy by G this is active now you canactually you know do this stuff you saywhat I'm saying now the question ofcourse is is by G the same as Phi staris VG the same as V Starit is true that VG is better than the PIand PI G is better than PI because ofthe way this is done in fact if in factthis is the best policy for thattransition functionthat right okay the question is is it bystar is it we stop and the answer is nounfortunately so there is the PI whichis less than or equal to VG which isstill less than or equal to V Star sothe question of course is why is thatdistend difference can be it's becausewhen you got the transition function youuse that you got the transition functionusing pipe so you only went around usingthe policy pipe it is possible that thispolicy saga intentionally stopped youfrom going into some other areas becauseof which you don't know the values thereokay and in fact me star might be quitedifferent because you haven't seen someof the state space at all okay you haveto explore you have to go up policywhatever policy you doing then you haveto go off policy during execution and dothings but you know what you know I doaction is that the policy is notelevated okay and that part is not anexploration problem okay and I showedyou this last time in fact if you have aboard like this and it just imagine thatyou are started in one of these stateswhere you know the policy okay then Ican guarantee you that you will never goto either of these games we will do notsome lots of things but you'll never goto these steps if this small problem youknow why because again these are actionswhich they will go in the intendeddirection with high probability and thetwo perpendicular directions with thelow probability but now we are into thebackward direction because they now gointo the backward directionthese two are essentially now we see soall I need to know is you are going tobe blind of these states so I go ahead Ithe nature will go ahead and put overthis effect must 1,000 reward here andplus 3000 devotee 8 then whatever theheck of policy you got in this placewill be much smaller than what you couldhave done if you've anything you seewhat I'm sayingso this is basically the need forexploration and this is not a selffundamental data to some extent you havegotten some amount of experience so infact I made it sound as if there was onepolicy that just gave you thetransaction function and then you haveto decide this means the transitionfunction but you really what mighthappen is you know you for some amountof time using some some policy okaylet's say and then after you computedthe transition function you can then getthe greedy policy right because Icomputed this transition function usingthe original policy that have a logicalform but then I turned around andactually computed a greedy policy withrespect to this transition function thatwould be different in general from thepolicy expanded from PI now my questionis do I just keep using this greedypolicy I also wore off policies from thegreedy policy so at any given point oftime in your life you have thisfundamental trade-off do i exploit whatI have learned up till now how do Ilearn some more and then startexploiting this is the explorationexploitation trader this is the point Iwas making when the end of the class wasclass that the bad students obviouslyhave a different explorationexploitation trade-off they tendto give higher value for explorationcompared to and the students who willstart like a premise and startexploiting and compare it to that youknow college students who might be doingit before that and so onokay general you want to prove that youhave the optimal policy then you reallyneed to consider exploring our parts ofthe state space so how would you getthat how would you get that essentiallythere are several streets severalschemes but the obvious ones essentiallyBo half policy which is what you knowkind of pirate analogy but I think ofMike our philosophy as that you know youyou have some goal you are just going onthe life side way but one thing ever youtake data up stake you know stop tosmell the rosesokay so explore you don't always justfollow the policy I know some of youwish I actually borrowed a policy forthe class that whatever it is that Itold you I will teach in that class Ijust keeps that and go of it and I keepmaking lots of lots and lots of detailsbecause I believe essentially that youhave to explore you can't get optimalvalue unless you are several meters evenis something called too much details andmaybe some of you are feeding that butit's important that you not follow thepolicy completely so he simplest way ofdoing itat any given point of time understandyou have a greedy policy by gee that'swhat I just showed you at any givenpoint of time we have will be theconsequent at your power withprobability epsilon you ignore thepolicy throw the plan out I just live init which is just to randomly pick anaction further state probability oneminus epsilon to follow your voice youfollow your current pulse is what I'msaying so if you I have actually donethis the summary at the end ofwas that if you are distinguishingbetween people who drop out after highschool and started doing jobswhat's that went up to like a masters orPhD jobs this is actually a spectrumthere could be people who are after highschool but are taking evening classesthat's a little exploration you see whatI'm saying you spend huge amount of timeto do that they may be doing some randomjob but then the evening you're doingclassify whatever classes you want totake it I think I should do the classbut any class you want to take and youwould essentially would be pleasedsingle exploration in sunshine so thatepsilon is the probability with whichthey're doing something off their policyyou know if you have a neck identifyingjob your policy is just stick to thatjob and you see how much happier you cansay you stick into the job in generalthat's like a mind-numbing boring thingthat you do from 9:00 to 5:00 andinstead of doing that once the valuethen something strange and doing thatoften times is basically the way ofcoming sort of going off policy okay ifyou touch the whole epsilon thenessentially then essentially you have ifepsilon is high that which are mostlyexploding you are not sticking toanything so if you you know that thepeople the the parents of PhD studentstend to think that why don't you have asmaller Epsilon what can't you say whichare already and get some job orsomething okaywhereas epsilon is too low then you justsay whatever is the policy I havecurrently I'm not going to change that'sall and this will follow our notice thatthat you're following your policy theycan be stochastic outcomes right so infact it's very important to understandthat then you are following these policythere are statistical terms when you trytooh yeah you might come here when you gofrom here you might come here I stayedhere right and when you try to go up inmy bar I'll go decide about the sidemaybe stochasticity even with thestochastic city there are parts of thefirst ways that might actually be maybeblindly that's possible all I needed todo is give you one counter example thatyou know that you not if you just followa single policy just because of thestochastic here the policy doesn't meanthat you will get the rest of the statespace you need more state space than youexpected because the stochastic city butit still doesn't countthe entire space whereas this idea willright because with epsilon probabilityat any state you are doing a randomlychosen action right so eventually youwill see the entire space anytime youare exploring you are losing the moneythat you could have made during thattime when you take evening classesinstead of doing evening classes youcould have made money standing inWalmart selling stuff being a vomitcashier instead of being a McDonaldmother flipper thank you for I got threemore money you lost that money byactually taking evening classes but whatyou actually have to pay tuition so youhave lots and lots of money last okaybut the expectation the point is thatexploration has cost you are livingmoney is so-called money on the tablebut to some extent without explorationyou might be leaving so much money onthe table without even knowing do yousee what I'm saying what you are notexploiting you can feel oh my god Icould have been making almost $3 an hourfor this one up our class but what doyou prepared after this class out of theDVD class later on you get to a pointwhere you make a lot more money estimatemoney is valuable okay so that'sbasically the simplest idea now ofcourse that I said problems with randomactionyou do explore the space but keepthrashing once the learning is done whatif in fact you're not learning so infact if you're doing this in thiscontext and virtually you started seeingthese states right then there is nopoint in exploring any further so youcan stop at smoothly once you have seenthat power states this but the problemunfortunately in real life is you don'tgo out with the state spaces okay so thebest you can do is essentially they useyour exploration probability as yourlife goes on in the PD week you knowit's like extremely silly to say I'mdone with grid one like Calvin of thecan be no hop sometimes with unit 1 I'mgoing to go to the job now that's likean exploitation policy that's much morelaughable ok there you should haveepsilon closer to 0.7 maybe by M s maybeit should be but in general as you goforward you might say good chance I'veseen more of the space so I will reducethe exploration okay which is the veryreasonable thing to do what thatincreases that that basically choose theinsurance the PMBOK now not only willyou be converging to the so as soon asyou do the exploration you areguaranteed to converge through theoptimal policy in the least if the Greekmeans if you wind up visiting everystateinfinitely often then at that point youtake that tea that you currently haveand you take the greedy policy withrespect to that that'll also be toughpolicy which is why actually we havethis GLI e3d in the limit of infiniteexploration if you take a greedy policyafter the limit of different expressionthat is going to be our purpose and whatwe're doing here is can we avoidspending that much time and still comesomewhat closer to our policy thetheorems are hard to come by here as tohow close you are to the optimal policybut clearly it makes sense to do thisyou know as you go you reduce the amountof exploration in some weird sense thisis sort of true even if you startexploring that you are my age they arenot necessarily as positive indescribing it they call it midlifecrisis it somehow doesn't have anypositive connotations but I do butapparently I can't because I live longenough or I should say to them okaywhereas if you don't explore when youare in fashion to be hated then yourPozos right because you're essentiallyassuming that I have done you have nomore love deeper meaning I'm closing myday okay so the other actually a moreinteresting idea is accumulationfunctions which is the next idea thatwe'll look at in here essentially it'sone size fits all so the epsilon is notdependent on the stateepsilon is not dependent on the stateokay but as you live there are certainstates that you've already visited closeto infinite number of times and someother states that you just happened tovisit new so an idea would bedifferentially many novel states thestates that are less visited look moreappealing than they are supposed to bejust by their current value do you seewhat I'm saying so you want to givenovelty points a state that you haven'tseen before whatever its currentvaluation I think that a state that youhaven't seen enough before whatever your3d value function stays its value is youadd a neglect XY to it so that itbecomes more appealing you guys get thisokay and in general actually this is thesame in the philosophy of I tryeverything once you know if I haven'tdone something yeah we just write likean exam some actual evolution making thesaga of novelty seeking into usespecially the young that's why thepeople they say I'm sure and do someBASE jumping you know once a try onceand I have been looking in the back Iput a time but you know people try itAmy okay so in general the teens thatyou haven't tried you try because itgive it higher novelty points there isno separate value for novelty you'rejust doing a pseudo thing you areincreasing you giving like extra coinsto a state because it hasn't been seenbefore but then after it has been seenenough number of times this bonus shouldgo away this is like the newcomers bonusyou see what I'm saying so what I do isI will use a function like this Spartanvalue and the number of times you havegone to that state before and UN wouldyou plus K by nokay and the idea being that if you areif you haven't gone to that before thenyou are getting a bigger plus okay so ifthe value estimated account and returnsan optimistic eternity it's making thevalue be more optimistic than it shouldbe because you have yet seen it enoughnumber of times okay and so thenbasically value iteration would now bedone with the F function thrown in themiddle so you are keeping track of thevisit counts to the state if you'revisited long enough then it's value isthe value that it actually got otherwiseit gets a bonus okayand the bonus waveform as we have seenthis stage enough number of timesbecause the count increases you see whatI'm saying because infinity that will bezero the bonus a be zerootherwise it's as many as K extra pointsthe other simple idea this is anexploration function this is better thanthe previous idea because it separatesescapes into the one that I've seen moreoften worse of the states that I've seenthe subject and have asked me need tofocus my exploration on the escapes thatI haven't seen now so the states thatI've seen before if I wind up goingthere I just do what my policy tells meto do states that I haven't seen enoughnumber of times that's where I wind upin a sense basically being in a bonusand look betterso in expect I'm going to be doing apolicy with respect to the gradingpolicy okay so that's the explorationfunction ideaso that's basically one of this learningwe talked about passive model-basedactive model ways and the distinction isreally small in fact this is just thesummary of whatever we discussed tillnow so you do get a instead of justgetting the pie and they VG which is thegreen eval you which is still hard Vistabut then if you do one of theseoperations then it will eventually bedone we start okay now we go back tobottle free learningremember I said model makes and modelfree okay so if you put what we startedwell as we started with Monte Carlo andthen we do what it's problems were andthen we went into a model case where youcompute T and you tried both passive andactive now you want to try model freemodel C as I said what did I say about