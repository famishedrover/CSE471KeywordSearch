another man so you've changed somesystem and then you applied it to forexample a deficit which involves all theother materials of the world not justAmerica and what do you think willhappen you can't even tell gendercorrectly and you guys know I don't knowhow many of you know this this wasactually a huge problem that happenedlast year that the commerciallyavailable gender classification systemsthat IBM was using that I was almostusing etc basically you are trading onone set of data the easily availabledata on the on the Google and you guyshave to use of course you keep puttingtoo many of your own pictures onto thecell phone and so essentially they werevery too many essentially Western facesand so the Machine basically just failedpretty miserably on the faces ofMalaysians and the faces of gang namesand the cases maybe some Indians toobecause not all idiots are here you knowa whole bunch of them in this classthey're far more left in Indiaso so what happened there what happeneddance is that the distributions aredifferent the dreaming distribution isdifferent from the test distribution andseeing how did you talkin yeah I do thisyeah is biased is crazyyeah is not biased data set is biaseddo young science a so you should havegiven a representative data set to trainin your system then it might have achance of doing that it might still notdo well because it have actually no ideahow to tell gender but at least untilthen it wasn't even a fair question toaskso right now right after that basicallyall of these people very quickly went topictures up lots of people in camera andMalaysia etc put them up into thetraining set and then retrain theirsystems now it's doing slightly betterokay so that's so if you wouldessentially without your knowledge youmight well have training and test sethave having different distributions andyou still have to deal with it and theseare the problem cases in fact one ofwhich we will talk about in today'sclass is called covariate distributionwhich is sort of like what happened inthe gender see near you to some extentthat so if you think in terms of aclassification problem it's taking someinput variables x1 and x2 x1 to XM andmapping into some class is variable byso picture pixels of your face beingmapped to male versus female if you'relooking binary gender classificationokay so the question thenthere are many ways in which the datathat you trained with and the data thatyou are testing with 10 differentdistributions the symbols are differentbut they could be for different reasonsone particular way would be that therelation between access and wise areunchanged but excess in the input dataare different from the excess in thetest data so the range is thedistribution of input variables X isdifferent in the training versus testedit and that's what is called covariatedistribution and what we call batch mobessentially winds up handling thisfamiliar distribution in a veryinteresting way because it winds uphaving to deal with it I think aboutnodes we talk about it ok but anywaylearning in non stationary environmentsis hard the particular form is covariantIFFT but most nama learning that we aretrying to know generalize assumes thatthe distribution of the training andtest data are the same assuming such afair test then you can talk about whatis a complexity of a learning problemremember in computer science we alwaystalked about complexity of a problemwhich is different problem that comeslike strictly an algorithm right ok soin general for example is you write youcan write an N factorial complexityunder them for sake but somebody hascan't write and handle again after theirfirst cycle an np-hard problem inparticular an easy problem where youexpect that nobody can write if all thenam-il are there do you like that's whatso problem complexity doesn't depend onwho is writing the order there we seethe spectrum or X are there then nobodycould have written everother than that we'll be cheaper thanexponentially so the question then iswhat is the complexity of machinelearning problems what is the complexityfor generalization problems and thetheory for complexity comes from is backprobably approximately ah the me she wasshe asked me but she was she butessentially I would argue that the onlyway you can talk about the complexity ofa learning algorithm is in terms of thiswhich is a a particular learning problemis back with epsilon Delta if in fact ifin fact it looks it can give you lessthan epsilon ever with more than oneminus Delta probability okay notice thatin describing this complexity have wetalked about how long the learningalgorithm will take no for complexityfor normal algorithms the coin of therealm is the amount of CPU time what isthe complexity measure for machinelearning generalization algorithmsnumber of samples so these are basicallysample complexity and you would say aparticular learning problem is hard ifit requires exponentially many samplesfor any other them to learn theirproblem exponentially many samples forany of the liberal order okay so thisfact theory is saga for machine learningwhat NP completeness please formalcomparisonsokay and the guy who came up with thisguard Chirag award Lesley variant as asmall historical footnote Lesley varianthand around in the same dog that Jeffhit the mouse walking around when he wasan undergrad so they were exchangingnotes about the architecture of the dogthat might have given them both duringawards okay so this is a valiant part ischeering about for pac learnability sokeep this in the back of your mind thisis really ultimately that theory oflearning now you have to use this tounderstand how do we evaluate learningof their lives and then see whether ornot those kinds of techniques will stillbe relevant for the kinds of problemsthat very large multi-layer networks arethe English which is what we will try todo yes touch this area that's the ideamaybe that made the distribution kindaflat if they added like everyone of thesame race but it's a very actuallyamazingly good point you're making whichis connected to Kosta missclassification okay so what he is sayingessentially means imagine that theentire world is really Indians which isactually true okayso that question is if I just dream allthe census data of India and China thenpretty much I should have very highaccuracy learners that is true okay butit is also true that when youmisclassify somebody's gender there is acost associated with it it's not just anumber of errors but is how costly isthere and if you want to reduce theerrors so it's like the commerciallyavailable systems it wasn't that theywere completely misclassifyingeverything with like as if it was like apasa it is that they were high enoughever they've had the lowest ever havelike that 99% accurate for white malesso you're fine okay but pretty mucheverybody else was no accuracy it didn'tmean they were 50 percent accuratewhether it what's the worst accuracy fora binary class learning fifty percentpeople understand this if it is campusit is a great learner just do oppositeabout it samedo you understand this okay the worstlearner are a binary classclassification is fifty percent accurateyou want it to be fifty plus epsilonthen it's actually better thanUniversity so anyway so the point beingis that it's not that they are TV 50%pasa kind of deserves they were dealingmore like seventy-nine percent orsomething I don't remember the exactnumber but that's considered prettyegregious for a particular problem doyou see what I'm sayingon the other hand the same types ofclassification technology is also beingused to fight Facebook to tell you heyyou would like this article hey you likethese friends update you do this you dothis all these nonsense in fact lambdacourt says we make 25 billionpredictions per day in booty claims whatand say it's like the cost of a badprediction basically think of the wholething is annoying I just don't even lookat itsee those cases essentially they can saywe're improving it doesn't really matterhere the errors actually have a veryhigh cost and that changes your interestin minimizing they're very very moreokay so it's a very important questionit is true that if you basically have aif I tell you you can only train yourdata either on nice data set off Islander some feed Landers on ice datacenter of Indians and Chinese and thetest dataset will be universal data'slike any any you know picture from theworld you much might have taken theIndians are Chinese register there aremore Indians and Chinese Finland andIceland I think they have or say aboutthemokay good persons so trading yourvalidation test and basically taking andvalidation sets and training andvalidation losses and actually find thethree types of tests training validationtest sex and then the losses that'swarning them last time whatever respectto those services okay so this basicallyis one of our not learning how do youevaluate a learning algorithm andthey're going to use these to alsoevaluate wishing know your networklearning other tips okay so supposebasically the way this goes is supposeyou have a thousand label samples okayyou've somehow figured out eleventhousand transactions for which you knowthe actual that are not they'refortunate or unfortunate are eleventhousand human faces on which you knowfor sure what the gender is okay andthen we did start by randomly removing aset of samples and keep them separate astest samples that is let's say you keepone k samples they may be far away fromwhere are you are put them in iron safelock it up and give the key to yourfriend that's fantastic you'll neverlook at the tested if you do whathappened to you in the project happenseither what you look at the test setslowly comes into the worldokay so you remove the test set put itsomewhere else and then there is 10 Ksamples left you split them into atraining set and a validation set okayso let's say 9 k as the training set andone case samples as the validation setnotice how the discussion is only interms of samples because we knowcomplexity of generalization is measuredin terms of samples in terms of time interms of samples okay soso you could do 90 up here and also theslightly more interesting version ofthis card k-fold cross-validation we youcan lick for example sweetie stepexamples you need to end batches of 1000each and then you can randomly pick minepockets on top that training set and theother set becomes the validation setwhat you then do is you clean yourlearning algorithm in your case yourneural network with a past only learningof the loop you know at the perceptronthat's only learning under them you knowokay on this 90-day defect see how wellis it performing and the validation setand that's where you try to decidewhether things like overfitting arehappening whether you should try tochange the number of layers in thenetwork etc after a while you think thatthe validation test error is low enoughat which point is say my done and isready for the real world and then youapply it to the tester this is how youdo the evaluation okay so keep this inmind the difference between validationand testing is subtle essentially fastthe learning after they're trained toswitch to the training data because it'strying to minimize the error in thetogether then every once in a while youare comparing it to the validation testsaid and and then feel they have tochange the learning algorithm itself soit slowly learns the validation set alsoit's not as fast but eventually thevalidation set patterns will leap intothe learning so just as you can over fitto the test that to the training centeryou can also all fit to the validationset which is why you don'tthat's it is manly she said do you seewhat I'm saying and then of course inthe end then you compare it to thedesert okaybut another thing just before I goforward I want to go back here andmention I guess yeah this one I don'twant to mention thisthere are speaking these things one ofthe words that people in machineryshould say whatever idea what they meanwhat it means is ing right iid the niceassumption okaywhat does IID mean it's that samples areindependently chosen and I was chosenwith the same distribution IIT meansindependent and identically distributedthat's basically what I'm doing if I'musing a property distribution P I'mpicking one sample at a time and againis the way we pick the samples in thechute to be with replacementokay the packet is basically it's reallynot a bucket if you're a probabilisticguy you will say it's an urn for mysticpeople don't have one hates theircomments where they can put their ashesthis is the urn model with replacementso you pick the thing I find the testbecause you look at this data the datagets consumed they are not eating itright okay so you doing this thing andthen so basically said is assumptionthis body just as requiring the trainingi test said to have the samedistribution is not always possible as Itold you requiring IIT is not actuallyobvious possible imagine the following Tyou pick somebody one part in atransaction and you realize instructor atransactionokay then you put down the othertransactions of thatall the transaction that happened onthat date that means now the examplesthat are being reached are notindependent they are dependent on eachotherit's nothing faster actually because init is really the case that fraudulenttransactions may tend to be clustered soif you find for example when we foundthat one cheating project we did saylet's just do independent identicallydistributed sampling we did say that wepick the ones who they cheated and thenwe hope that there are other projectsbingo I don't need to do iid understandand said I might learn much faster whocheated by doing that maha IT samplingbut the theory is much simpler if it isiid sampling so it's important toremember that people actually talk aboutmorality sampling but really emotionallyabout scenarios the way the trainingdata is pink is typically man IDokay keep that in mindokay finally we come to evaluatinglearning develops yes[Music]no assumptions are assumptions us youknow right so it's like simple in theirirony and most prop most learningalgorithms should be can be shown tohave good theoretical properties underany conditions but there are interestingtechnical challenges in coming up withlearning algorithms that can be havethat can be shown to have goodproperties in under non varietyconditions in particular a buzzword thatyou can use to impress your interviewersis our learners okay but I won't go moreinto that right now okay so finally weget to 30 cups okay finally we get tolearning which is basically that whenyou compare learning other words okay solearning curves essentially have on thex-axis the number of examples you arebeing given or what the size of thetraining set on the y-axis the accuracyon the test set the amount of data youuse to train yourself and however you doyou do on the test that's a learningcurve okay so here these forces this oneof them I would say imagine one of themis the learning curve for a perceptronand the other is for the multi-layernetwork which one is which what do youthink just from what I told you would bethe parcel America and which one wouldyou say the multi-layer networkthis one is what perceptronthis one is material in fact in manycases if you actually do the same thingit looks more like this that's how itlooksthe microwave network is typically lookslike light compared to versaclockbutt-fuckin get string really fast getsback at its height of expressivenessmulti-layer networks learned slowly butthey cannot do anymoreI knew a few parents had a dog when youwere a kid okay your parents would havecompared you to the top but let'simagine you and dog were born at thesame time okay and most reasonableparents something why did we have a dogdid you have a kidkids don't do anything they just hangaround very slow learners the oddswithin a couple of days they're runningaround a few months later they'recatching ball very fast learners theylike pacifiers you are like creepingaround like this the difference is thedog is still only good at catching thefrisbeeyou are Moran can fake coming to theinterpreter I plus under that one guythere is mostly spending its timelooking at his laptop and expects thatTommy knows this but even here webelieve that valo attention I'm going tofind out the name later butso what I don't think that people don'tunderstand is when you stand here myfriend what's your name you yeswhat okay can I talk to me after classokay coming up on the class we have goodfun okay so so the difference betweenthe dog and the person if the person wasa slow learner but the capacity forlearning is high so they can learn a lotmore concepts the dog was a fast learnerbut it stops after a while there's asmall grain do you see what I'm sayingyou can have learning algorithms justlike that but soccer is around and youprobably are aware later and you cancompare those types of things so you canactually when comparing machine learningalgorithms you consider the slope afterlearning curve which tells you whetheryou are a slow or a fast learner and theasymptotic accuracy which tells you thatcapacity of the network this is learning101 this is the stuff that you get badif you start taking any machine on yourcourse are you just using learning as achapter this is what the minimum thatyou need to know now we are going to usethis to talk about evaluating neuralnetworks values but before we go therethere is an interesting point we need tomake which is for the longest time inmachine learning did have been this sortof traditional learning tasks tended tobe low dimensional tasks Lord I must nowmeans the number of inputsis hundreds are houses maximum okay butfor example for the boolean gates thatwe looked it what is that a majoritybecause we said inputs are I 1 and I 2and output it is what is the gate valueand typically for the essential and itasks you basically assume either thatthe learning task is so simple thatreally doesn't have any more inputfeatures somebody to get much morecomplexsameer you converted into a whole asmall set of a small set of hand codedfeatures like for example you're allvery different learners each of you havevery difference of educated learningalgorithms but we have reduced you to alow dimensional vector form on score noone to score a more three score aboutfour score contact or score project onepenalty for cheating positive score forthe Copernicus cheated it and if youstill remain the goal then examine forexample score Exeter that's itso we are then taking this as the theseare manually generated features weextracted them I'm sure it was verypainful for you when we extracted thefeatures out of this out of you and thenwe use that for learning your graderight and much of the theory forlearning in both AI and statistics hasbeen developed for these this istypically low dimensional typically inhundreds to thousands of a new featureson the other hand many perceptual taskstasks such as given an image is it a catgiven a a speech signal what is the wordresponding to itgiven assist me graph did that makehappen these are all high dimensionaltasksthat input basically in fact even thesetasks in the past would be done in thetraditional way by essentially picking abunch of hand coded features out of thisinput and then use that to training thelearnerso for example in the current projectyou are doing feature basedreinforcement learning you picked upbunch of features but really you shouldbe able to just look at the picture anddecide what track man should be doing sosomebody basically came in between youand the problem and extracted thefeatures for you so that will become atraditional low dimensional do you seewhat I'm saying sometimes in fact thefeatures that people extract have reallythere has been you know a reasonableunderstanding and people saying that forexample that why can't I just take yourface for example which is a much highermarginal input and use that to besideyour grid it's a much bigger vectorgiving me enough compared to just yourscores which are only like you knowmaximum readybut high dimensional doesn't always meangood stuff in fact people are usingfaces to design your scores as romicyour emily's whether or not you shouldbe given a job whether or not you knowanything that's nonsensical because inessence when you do this people had sometheory of what part of your life isrelevant to you and read in this classhere there is no such theory you areexpecting the learner to pick it up andlearner doesn't learn theories it learnscomplete through the dinner weunderstand one side okay so in speechimages and signal such as these featuresare extracted by learner as part of thelearning if they extract them as part ofthe learning this is a different thatwould then be a waydifferent kind of learning from thislearning the learned features in thepenis may not have any meaning to thehumans in the loophow many of you agree with the followingstatement that if I were to takepictures of let's say this class and Ihappen to know that grades maybe I madeit about already okay well I said lastyear's class I took the pictures of thisclass that was Jay's class and I knowtheir dreams do you think I could traina neural network such that each trainingerror is 0 just from the faces how manypeople think that if I give the faces ofthe people from 2017 when we have thatwith the class and I know the gradesbecause on the ground truth cook it andI basically gave it to a neural networkand I trained ithow many of you think that there is noneural network that can get zerotraining error raise your hands how manythink that there will be a neuralnetwork that will have zero trainingerror families you see what is happeningin the world around do you understandwhat I'm saying if I did that and then Iapplied it to you guys because and thetraining said if I get that I can saywow I predicted some great correctly whoknows our that's right about in essencedid you understand what I just said ifyou have high enough number of inputdimensions and high enough number ofparameters you can choose any 20 errorcan be driven down to zero that doesn'tmean that you learnt a team that justmeans that you memorized how to compressthe data