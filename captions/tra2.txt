this is nothing but t xq l th the Xcubed D and H is nothing much because Xis right okay so instead of taking thesum I want to take the element in thissum that is the maximum yeah sobasically I can then consider thehypothesis H I that means P X given witha child I am skiing a chai the highestthe same no actually no so PD getenergized x bhi diets and to do thisessentially I canokay yes your maximise is this is Godnot maximum a-posteriori hypothesis thathypothesis and has the highest posteriorprobablyI'm still not not like you the next newsis I'm poor Ajay that maximises log ofprobability of the data given itemswhich also maximizes the like that theyhave you know that scale maximizinglikelihood capsule is what I'm sayingafter two steps of huge simplificationsyour father through the usual thing thatyou now the interesting question is canhe is it of coursethis itself is a huge problem you shouldnever have done this and I've been goingfrom here to here is not a huge problemif you are thinking in terms of why isit feasible to go from here to here youcan ask the point that you can see thatwe give nhi is nothing but activity 1etc I am given a child which because it[Music]was stillnow when you do the log of D isn't therewill be lot of which saga tells you thathas the amount of data in business inthis sum here this factor dominates sothe for the fully large data ismaximizing likelihood it's as good asmaximizing a posteriori problem whichthe baby is as good as B so that'sbasically Bayes networks so the otherthing that for minutes I want to mentionsomething[Music]but the ECM some people just don't getoff remember patience basically say butthis is very frequently say that I meanfirst of all basis a probabilities arewe made upyou can't decide what your powerrelations are essentially okay I'll beC&C remember that's what I asked you inthe Bayes Network and you put CPD's insuch a way that the network will blow upno it will never put any numbers youwant the only thing that they ask is ifyou believe in some probabilities makesure that you live your life in thosepower be assisting liquid this safeprobabilities are objective they don'tdepend on wishy-washy I feel likeso in fact the classic example is what'sthe probability that would apply onMarch 1907 Einstein had a large propertyI mean I feel ready to kiss on thefrequent estate this question doesn'tmake sense you cannot set up anexperiment unless you write you knowunderstand has to keep going through theparticular day at a particular fiveo'clock about thousand times and you seehow many are crossed and she had a bigTV and then you say oh it is the power[Music]button to pick them the Bayesian steamrandom may I put this is a random afrequent testing hypothesis not a randomday but you both agree on one specifictechnical method which is maximizing thefrequent you say[Music]because that's also a huge religiousmother used today in the StatisticsDepartment and they keep it this one isrecovering[Music][Music][Music][Music][Music][Music]some involves age and thepublic image itself so one of theinteresting things is actuallyuseful to understand whatever powerremember I said the addition isunderstandable from the Bayesianlearning point of viewif you're basically starting from thisside you didn't know how you fell fromheaven and your honor and things aregoing bad you're trying to reimaginewhat could have been betterokay the Fulham of course win verysimilarly but other is at least tryingto see which hypotheses are more versusless likely in doing the optimization Iwant to not just be careful this is thatmaximizes the likelihood but also becareful this is the likelihood why I'meven making some assumptions about ourhypotheses are more likely what ourLeslie right and so if you remember thatw deceived and I said they do wreckrealization so the declaration they doplease you would be too big becauseevery any seeming to be in any largemakes the network has such littlegrandmother neurons right so youessentially by putting theregularization constraint you areequalent lee makingoh right did you remove it again givethis way you don't watch you to have itand so you can oh it's not even thatplus okay let's go and then I justbasically want to get you started in themaximum likelihood estimation for Bayesnetworks and then I will give you thevideo with the slides that you can watch[Music]so think about the covering sequel thecity area okay so I'm basicallyconsidering again something like thiscandy scenario where I have a bank froma new manufacturer has eitapercent of charitiesso there's a bag and this might containcharities okay so then the base networkthat I have in mind is Deepa isprobability so then I pick up the candythen I get up to the train yard lineso essentially I have flavor is therandom variable and I pick a candy it'slevel be the Julia line andbasically right so except now noticepreviously I did the same thing except Iconsidered if I go to my job now acontinuous video so any problem now if Iget a whole number of candiesessentially then I just want to maximizethe likely I'm going to go with theclass better maximize the likelihood sofor example if I have seen em candiesokay after age L line candies till nowand see our cherry candies you know okaythen the probability of seeing thesecandies l11 city- this is okay I just need to maximizethis expression with respect to thetathis is a continuous expression in termsof theta I just need to maximize itso to make my life simple I will decideto take the log of this and maximizethat instantly if the function if I'mtrying to find out the fraction is afraction for take the log of thefunction find the place where the Maximaoccurs so simplify them recognizingactual value would be obviously t powerright so I want to do the log so there[Music]right at this point I need to maximizethis prospective right now it turns outin this particular case essentially thisis one item one variable theta it's notmy typical variation Thanksso I just essentially you know I'm sorryso I need to be ready to maximize thevalue I need to find the value of thisfunction so then I take derivative ofthis and equate it to 0 remember nom ofgradient descent would be used whenactually to do this finding the placethen immediately ten equal to zero itturns out for this particular case youcan analytically sound you cananalytically solve it and if you do thisit will be C by D theta plus L byand then there's a - - and youmind or something the calculation whereyou move another way but you do allbunch of things in your life all thetime without like my next one this isthe reason you're right in assuming thathalf hundred candies xx shitty candiesassuming that the probability of jimmiesis twenty five hundred twenty percent iscoming directly from maximizing okaythis is the definition of the next thingthat I want to show you is this looks asif this is too easy rightthe next thing I want to show you isconsidered so I want to take this outagain all of these are slides areavailable consider a more interestingcase that we draw the case whether itwould stop now I have which then causesso previously our candy foxes came inthe same plane Brahman okay now thebasically the the angle of the rapperswould be the brief are ready to sort ofindicate whether the stock has linecandies are Syriansthis is what I'm saying okay so thequestion this is my base at work now Iplay I basically leading to the rapokay so that green wrapper dependsprobablistic al become clever it's notdeterministic it's not necessarily thatall jelly bags will only have so thequestion then is is that so then I havehow do i specify this properly so howmany customer group is not here I givein this case okay so what kind ofabilities liningI need funny probability parameter and Ineed a conditional probabilityit's basically says if the flavor ischerry what is the probability that thewrapper equal to then given them is whatI'm sayingthat's our promise T dependence is theflavor is cherry let us assume that it'seither one probability that happen todelay and if the flavor is liabilityso imagine that I have seen Emma candiesthen which which are CR cherries L ourlives okay I'll see Jenny candies itwherever L seegenican is include refugees can disagreerefer and so onokay these are the numbers that I canjust see how they keep getting how manyyou know I thought actually examining abitch see our Chinese wives after seriesRC of them are actually having likerapper RL GC of them are having dinnernow GC is C - these are the numbers thisis the amount of the diaper now thequestion then is I need to write inemployee based network parametercalculation future of statisticallearning how you need to is set up thelikelihood equation likelihoodestimation that's it I need to set upthe right orientation and then of coursethen I do so let's get the likelyearlier provision humor for the am notwhich other men see was Jenny's and sothen it up our CI mahamasina paolellothat was it now basically it's three tofour different casesokay so I will write that expression oftheir try to make you understandso in there are the cherries they're thetheta power seats so consider and aspecific a specific date upon which isactually in the back I just saw a changein agreementwhat is its problem Jennyso it's together in that no chilly it'schillyand then the real bad is because itseems is theta 1 so 1 minus T Dow thepeople see this for each Jenny implementI have multiplied that now if I see aCheney in red backso the overall expression I like thisand you can seeso this is the part but seeing seecherries and links and then after seecherriesI found a sea of them in red and that Ifound GC out there in the green backs ofthe limesI found them in the net banks and thiswould be the final expression but eachdata point I will get like the thetatimes theta one theta maximum is theta 1minus theta times theta 2 etceteraif I multiply them all then I will getthis big expression I need to find inthe minimum of this expression now thisis a three dimensional function it is athree argument function it's likebasically basically it has threeparameters what I mean - so the threeinputs and then why that's what I meanso that input dimensionality is thingokay so it's y equal this is basically[Music]that's your and now I'm going to findnow what happens is this is essentiallygoing to be gradedactually but you guys watch you can putthis funky same but it turns out that isalways a cheap easy yeah you can set thegradient expression equated to zero andwe'll be able to solve it in particularyou need and now before I can getequations equations in the worst casethe three equations would besimultaneous nonlinear equations if theyare not linear simultaneous equationsthis magnetic solution then youessentially basically to solve it youmight as well use newton-raphson methodwhich is like this gradient descent tobegin ok what is interesting is forvarious networks you have continuousdata with your complete leader that isyou always see the data and all itsattributes so for example thiswhat has completely done it if I said Ihave a data point what are the questionsyou're going to ask me about it what'sthe flavorwhat's that I if I can answer both thenI have a complicated sometimes forexample I am NOT allowed to open theback so I don't need no the wrapper Idon't know what I'm saying then thefavor becomes hidden variable if youhave hidden variables doing Bayesnetwork learning the hidden variablesleads to an idea Carl's expectationmaximization because negative variablesyou need to sum like you